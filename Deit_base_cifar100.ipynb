{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70Vxlv9lMRZ",
        "outputId": "edfb9fba-0ae9-4169-c90c-52defaa36461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ViT-pytorch'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 170 (delta 32), reused 27 (delta 27), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (170/170), 21.31 MiB | 15.35 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "/content/ViT-pytorch\n",
            "Collecting ml-collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from ml-collections) (1.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ml-collections) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ml-collections) (1.15.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from ml-collections) (0.5.5)\n",
            "Building wheels for collected packages: ml-collections\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94524 sha256=149fd91b9d9f961a1a03161900839107d1ed11831afd84dcb6fbb4225635a2fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/da/64/33c926a1b10ff19791081b705879561b715a8341a856a3bbd2\n",
            "Successfully built ml-collections\n",
            "Installing collected packages: ml-collections\n",
            "Successfully installed ml-collections-0.1.1\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n",
            "--2022-03-14 13:43:57--  https://docs.google.com/uc?export=download&confirm=t&id=1-AxL45qSt354FadCyK375_60ehXZfCJs\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.126.113, 108.177.126.100, 108.177.126.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.126.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d4co63bc3rumhj7s9qcjb2bslif6ub9c/1647265425000/17691537378993098219/*/1-AxL45qSt354FadCyK375_60ehXZfCJs?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-03-14 13:43:57--  https://doc-08-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d4co63bc3rumhj7s9qcjb2bslif6ub9c/1647265425000/17691537378993098219/*/1-AxL45qSt354FadCyK375_60ehXZfCJs?e=download\n",
            "Resolving doc-08-5g-docs.googleusercontent.com (doc-08-5g-docs.googleusercontent.com)... 142.250.153.132, 2a00:1450:4013:c16::84\n",
            "Connecting to doc-08-5g-docs.googleusercontent.com (doc-08-5g-docs.googleusercontent.com)|142.250.153.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 343302577 (327M) [application/x-zip]\n",
            "Saving to: ‘cifar10-100_500_checkpoint.bin’\n",
            "\n",
            "cifar10-100_500_che 100%[===================>] 327.40M  61.3MB/s    in 5.4s    \n",
            "\n",
            "2022-03-14 13:44:03 (60.1 MB/s) - ‘cifar10-100_500_checkpoint.bin’ saved [343302577/343302577]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jeonsworld/ViT-pytorch.git\n",
        "%cd ViT-pytorch/\n",
        "!pip install ml-collections\n",
        "!pip install einops\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-AxL45qSt354FadCyK375_60ehXZfCJs' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-AxL45qSt354FadCyK375_60ehXZfCJs\" -O cifar10-100_500_checkpoint.bin && rm -rf /tmp/cookies.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hGKear4S3Q_p"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from datetime import timedelta\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from models.modeling import VisionTransformer, CONFIGS\n",
        "from utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule\n",
        "from utils.data_utils import get_loader\n",
        "from utils.dist_util import get_world_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "# Required parameters\n",
        "parser.add_argument(\"--name\", default=\"cifar10\",\n",
        "                    help=\"Name of this run. Used for monitoring.\")\n",
        "parser.add_argument(\"--dataset\", choices=[\"cifar10\", \"cifar100\"], default=\"cifar10\",\n",
        "                    help=\"Which downstream task.\")\n",
        "parser.add_argument(\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\",\n",
        "                                              \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"],\n",
        "                    default=\"ViT-B_16\",\n",
        "                    help=\"Which variant to use.\")\n",
        "parser.add_argument(\"--pretrained_dir\", type=str, default=\"checkpoint/ViT-B_16.npz\",\n",
        "                    help=\"Where to search for pretrained ViT models.\")\n",
        "parser.add_argument(\"--pretrained_model\", type=str, default=\"cifar10-100_500_checkpoint.bin\")\n",
        "parser.add_argument(\"--output_dir\", default=\"output\", type=str,\n",
        "                    help=\"The output directory where checkpoints will be written.\")\n",
        "\n",
        "parser.add_argument(\"--img_size\", default=224, type=int,\n",
        "                    help=\"Resolution size\")\n",
        "parser.add_argument(\"--train_batch_size\", default=512, type=int,\n",
        "                    help=\"Total batch size for training.\")\n",
        "parser.add_argument(\"--eval_batch_size\", default=64, type=int,\n",
        "                    help=\"Total batch size for eval.\")\n",
        "parser.add_argument(\"--eval_every\", default=100, type=int,\n",
        "                    help=\"Run prediction on validation set every so many steps.\"\n",
        "                          \"Will always run one evaluation at the end of training.\")\n",
        "\n",
        "parser.add_argument(\"--learning_rate\", default=3e-2, type=float,\n",
        "                    help=\"The initial learning rate for SGD.\")\n",
        "parser.add_argument(\"--weight_decay\", default=0, type=float,\n",
        "                    help=\"Weight deay if we apply some.\")\n",
        "parser.add_argument(\"--num_steps\", default=10000, type=int,\n",
        "                    help=\"Total number of training epochs to perform.\")\n",
        "parser.add_argument(\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"cosine\",\n",
        "                    help=\"How to decay the learning rate.\")\n",
        "parser.add_argument(\"--warmup_steps\", default=500, type=int,\n",
        "                    help=\"Step of training to perform learning rate warmup for.\")\n",
        "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                    help=\"Max gradient norm.\")\n",
        "\n",
        "parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                    help=\"local_rank for distributed training on gpus\")\n",
        "parser.add_argument('--seed', type=int, default=42,\n",
        "                    help=\"random seed for initialization\")\n",
        "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "parser.add_argument('--fp16', action='store_true',\n",
        "                    help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
        "parser.add_argument('--fp16_opt_level', type=str, default='O2',\n",
        "                    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                          \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "parser.add_argument('--loss_scale', type=float, default=0,\n",
        "                    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
        "                          \"0 (default value): dynamic loss scaling.\\n\"\n",
        "                          \"Positive power of 2: static loss scaling value.\\n\")\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl',\n",
        "                                          timeout=timedelta(minutes=60))\n",
        "    args.n_gpu = 1\n",
        "\n",
        "args.name = 'cifar10'\n",
        "args.device = device"
      ],
      "metadata": {
        "id": "2t5UUX-qR_lO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = CONFIGS[args.model_type]\n",
        "num_classes = 10 if args.dataset == \"cifar10\" else 100\n",
        "model = VisionTransformer(config, args.img_size, zero_head=True, num_classes=num_classes)\n",
        "model.load_state_dict(torch.load(args.pretrained_model))\n",
        "model.to(args.device)\n",
        "train_loader, test_loader = get_loader(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "481fe63d433f4bb68353dbc217360d41",
            "52008afc76af46e187642e835270340d",
            "9c3eb7787e7c42a1bf3f16e737a0b485",
            "81c712ba88dd4a1a9352f5dd61178ce1",
            "d98b6aba2831489eba00563e9820e590",
            "6e01b15ed4bb46529aac6bd41d208987",
            "d6efbae9e2e8453da58fd2e043fed52c",
            "37a73608bfa54c47aacc5f66a4fae6ec",
            "acfd700929a34bb79df79cc8f354db60",
            "89a49a6b969b4e59912f1f788310a5df",
            "7f131ac7fc854ee7b7f8390741d527da"
          ]
        },
        "id": "w9chZMfcSYaW",
        "outputId": "576b87b2-1faf-4c08-cb94-f29ef6efc2db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "481fe63d433f4bb68353dbc217360d41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def test(model, test_loader):\n",
        "  model.eval()\n",
        "  all_preds, all_label = [], []\n",
        "  epoch_iterator = tqdm(test_loader,\n",
        "                        desc=\"Validating... (loss=X.X)\",\n",
        "                        bar_format=\"{l_bar}{r_bar}\",\n",
        "                        dynamic_ncols=True,\n",
        "                        disable=args.local_rank not in [-1, 0])\n",
        "  loss_fct = torch.nn.CrossEntropyLoss()\n",
        "  for step, batch in enumerate(epoch_iterator):\n",
        "      batch = tuple(t.to(args.device) for t in batch)\n",
        "      x, y = batch\n",
        "      with torch.no_grad():\n",
        "          logits = model(x)[0]\n",
        "\n",
        "          eval_loss = loss_fct(logits, y)\n",
        "          # eval_losses.update(eval_loss.item())\n",
        "\n",
        "          preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "      if len(all_preds) == 0:\n",
        "          all_preds.append(preds.detach().cpu().numpy())\n",
        "          all_label.append(y.detach().cpu().numpy())\n",
        "      else:\n",
        "          all_preds[0] = np.append(\n",
        "              all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
        "          )\n",
        "          all_label[0] = np.append(\n",
        "              all_label[0], y.detach().cpu().numpy(), axis=0\n",
        "          )\n",
        "      # epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
        "\n",
        "  all_preds, all_label = all_preds[0], all_label[0]\n",
        "  accuracy = simple_accuracy(all_preds, all_label)\n",
        "  print(\"Valid Accuracy: %2.5f\" % accuracy)"
      ],
      "metadata": {
        "id": "3208CvASUeR3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbUXkRNxVjls",
        "outputId": "6cd54636-58ed-4dc1-cd1e-cdd2e8c02d2b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating... (loss=X.X): 100%|| 157/157 [00:33<00:00,  4.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.98940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Eurus-Holmes/DeiT-CIFAR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubDg6H_ocUXT",
        "outputId": "b372dd02-d434-4ca3-fba5-fc16c72d3214"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeiT-CIFAR'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 28 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DeiT-CIFAR/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cXR17cTcca7",
        "outputId": "c46e5001-e24f-4c13-8d33-37edbe9186ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-pytorch/DeiT-CIFAR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm==0.4.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJk7qe7Kcfzq",
        "outputId": "e4edd5a1-45ee-43ad-e50c-78703b4119a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 40.5 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 46.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 71 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 215 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 225 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 266 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 276 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 286 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 317 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 327 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 337 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 368 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 376 kB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.4.12) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.4.12) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm==0.4.12) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.12) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.12) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --finetune https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth --data-set CIFAR --model deit_base_patch16_224 --batch-size 64 --epochs 13 --output_dir output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-17nSmwb-Wq",
        "outputId": "6456eb32-1914-4a99-8322-151e3505c7eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(aa='rand-m9-mstd0.5-inc1', batch_size=64, clip_grad=None, color_jitter=0.4, cooldown_epochs=10, cutmix=1.0, cutmix_minmax=None, data_path='/datasets01/imagenet_full_size/061417/', data_set='CIFAR', decay_epochs=30, decay_rate=0.1, device='cuda', dist_eval=False, dist_url='env://', distillation_alpha=0.5, distillation_tau=1.0, distillation_type='none', distributed=False, drop=0.0, drop_path=0.1, epochs=13, eval=False, finetune='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth', inat_category='name', input_size=224, lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, min_lr=1e-05, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='deit_base_patch16_224', model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, momentum=0.9, num_workers=10, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='output', patience_epochs=10, pin_mem=True, recount=1, remode='pixel', repeated_aug=True, reprob=0.25, resplit=False, resume='', sched='cosine', seed=0, smoothing=0.1, start_epoch=0, teacher_model='regnety_160', teacher_path='', train_interpolation='bicubic', warmup_epochs=5, warmup_lr=1e-06, weight_decay=0.05, world_size=1)\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /datasets01/imagenet_full_size/061417/cifar-100-python.tar.gz\n",
            "169001984it [00:05, 29191631.22it/s]                   \n",
            "Extracting /datasets01/imagenet_full_size/061417/cifar-100-python.tar.gz to /datasets01/imagenet_full_size/061417/\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Creating model: deit_base_patch16_224\n",
            "Removing key head.weight from pretrained checkpoint\n",
            "Removing key head.bias from pretrained checkpoint\n",
            "number of params: 85875556\n",
            "Start training for 13 epochs\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [0]  [  0/780]  eta: 0:26:44  lr: 0.000001  loss: 4.7225 (4.7225)  time: 2.0565  data: 1.2532  max mem: 7744\n",
            "Epoch: [0]  [ 10/780]  eta: 0:05:40  lr: 0.000001  loss: 4.6788 (4.6924)  time: 0.4416  data: 0.1141  max mem: 8734\n",
            "Epoch: [0]  [ 20/780]  eta: 0:04:36  lr: 0.000001  loss: 4.6570 (4.6668)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [ 30/780]  eta: 0:04:12  lr: 0.000001  loss: 4.6447 (4.6653)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [ 40/780]  eta: 0:03:58  lr: 0.000001  loss: 4.6463 (4.6618)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [ 50/780]  eta: 0:03:49  lr: 0.000001  loss: 4.6502 (4.6611)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [ 60/780]  eta: 0:03:42  lr: 0.000001  loss: 4.6553 (4.6624)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [ 70/780]  eta: 0:03:36  lr: 0.000001  loss: 4.6676 (4.6642)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [ 80/780]  eta: 0:03:30  lr: 0.000001  loss: 4.6795 (4.6655)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [ 90/780]  eta: 0:03:26  lr: 0.000001  loss: 4.6358 (4.6617)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [100/780]  eta: 0:03:21  lr: 0.000001  loss: 4.6420 (4.6627)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [110/780]  eta: 0:03:17  lr: 0.000001  loss: 4.6379 (4.6598)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [120/780]  eta: 0:03:13  lr: 0.000001  loss: 4.6310 (4.6591)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [130/780]  eta: 0:03:10  lr: 0.000001  loss: 4.6086 (4.6554)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [140/780]  eta: 0:03:06  lr: 0.000001  loss: 4.6050 (4.6524)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [150/780]  eta: 0:03:03  lr: 0.000001  loss: 4.6158 (4.6512)  time: 0.2783  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [160/780]  eta: 0:02:59  lr: 0.000001  loss: 4.6337 (4.6504)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [170/780]  eta: 0:02:56  lr: 0.000001  loss: 4.6322 (4.6492)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [180/780]  eta: 0:02:53  lr: 0.000001  loss: 4.6331 (4.6483)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [190/780]  eta: 0:02:50  lr: 0.000001  loss: 4.6331 (4.6486)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [200/780]  eta: 0:02:47  lr: 0.000001  loss: 4.6189 (4.6479)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [210/780]  eta: 0:02:43  lr: 0.000001  loss: 4.5956 (4.6459)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [220/780]  eta: 0:02:40  lr: 0.000001  loss: 4.6041 (4.6452)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [230/780]  eta: 0:02:37  lr: 0.000001  loss: 4.6089 (4.6436)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [240/780]  eta: 0:02:34  lr: 0.000001  loss: 4.6039 (4.6425)  time: 0.2780  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [250/780]  eta: 0:02:31  lr: 0.000001  loss: 4.6203 (4.6429)  time: 0.2783  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [260/780]  eta: 0:02:28  lr: 0.000001  loss: 4.6221 (4.6414)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [270/780]  eta: 0:02:25  lr: 0.000001  loss: 4.5960 (4.6405)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [280/780]  eta: 0:02:22  lr: 0.000001  loss: 4.6302 (4.6400)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [290/780]  eta: 0:02:19  lr: 0.000001  loss: 4.6302 (4.6394)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [300/780]  eta: 0:02:16  lr: 0.000001  loss: 4.6058 (4.6381)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [310/780]  eta: 0:02:13  lr: 0.000001  loss: 4.5970 (4.6375)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [320/780]  eta: 0:02:10  lr: 0.000001  loss: 4.6046 (4.6366)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [330/780]  eta: 0:02:08  lr: 0.000001  loss: 4.6028 (4.6351)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [340/780]  eta: 0:02:05  lr: 0.000001  loss: 4.5916 (4.6343)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [350/780]  eta: 0:02:02  lr: 0.000001  loss: 4.5967 (4.6337)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [360/780]  eta: 0:01:59  lr: 0.000001  loss: 4.5967 (4.6327)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [370/780]  eta: 0:01:56  lr: 0.000001  loss: 4.5838 (4.6314)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [380/780]  eta: 0:01:53  lr: 0.000001  loss: 4.5834 (4.6300)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [390/780]  eta: 0:01:50  lr: 0.000001  loss: 4.6026 (4.6292)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [400/780]  eta: 0:01:47  lr: 0.000001  loss: 4.6042 (4.6286)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [410/780]  eta: 0:01:44  lr: 0.000001  loss: 4.5930 (4.6276)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [420/780]  eta: 0:01:42  lr: 0.000001  loss: 4.5799 (4.6265)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [430/780]  eta: 0:01:39  lr: 0.000001  loss: 4.5908 (4.6259)  time: 0.2785  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [440/780]  eta: 0:01:36  lr: 0.000001  loss: 4.6064 (4.6257)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [450/780]  eta: 0:01:33  lr: 0.000001  loss: 4.5857 (4.6245)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [460/780]  eta: 0:01:30  lr: 0.000001  loss: 4.5770 (4.6239)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [470/780]  eta: 0:01:27  lr: 0.000001  loss: 4.5832 (4.6230)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [480/780]  eta: 0:01:24  lr: 0.000001  loss: 4.6089 (4.6231)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [490/780]  eta: 0:01:22  lr: 0.000001  loss: 4.6207 (4.6233)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [500/780]  eta: 0:01:19  lr: 0.000001  loss: 4.6020 (4.6225)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [510/780]  eta: 0:01:16  lr: 0.000001  loss: 4.6002 (4.6222)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [520/780]  eta: 0:01:13  lr: 0.000001  loss: 4.6106 (4.6220)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [530/780]  eta: 0:01:10  lr: 0.000001  loss: 4.5872 (4.6211)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [540/780]  eta: 0:01:07  lr: 0.000001  loss: 4.5723 (4.6205)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [550/780]  eta: 0:01:04  lr: 0.000001  loss: 4.5804 (4.6201)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [560/780]  eta: 0:01:02  lr: 0.000001  loss: 4.5946 (4.6197)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [570/780]  eta: 0:00:59  lr: 0.000001  loss: 4.5953 (4.6193)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [580/780]  eta: 0:00:56  lr: 0.000001  loss: 4.6058 (4.6190)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [590/780]  eta: 0:00:53  lr: 0.000001  loss: 4.6034 (4.6184)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [600/780]  eta: 0:00:50  lr: 0.000001  loss: 4.5827 (4.6178)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [610/780]  eta: 0:00:47  lr: 0.000001  loss: 4.5843 (4.6172)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [620/780]  eta: 0:00:45  lr: 0.000001  loss: 4.5699 (4.6164)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [630/780]  eta: 0:00:42  lr: 0.000001  loss: 4.5644 (4.6159)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [640/780]  eta: 0:00:39  lr: 0.000001  loss: 4.5808 (4.6157)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [650/780]  eta: 0:00:36  lr: 0.000001  loss: 4.5873 (4.6151)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [660/780]  eta: 0:00:33  lr: 0.000001  loss: 4.5944 (4.6147)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [670/780]  eta: 0:00:31  lr: 0.000001  loss: 4.5790 (4.6140)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [680/780]  eta: 0:00:28  lr: 0.000001  loss: 4.5603 (4.6129)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [690/780]  eta: 0:00:25  lr: 0.000001  loss: 4.5489 (4.6122)  time: 0.2810  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [700/780]  eta: 0:00:22  lr: 0.000001  loss: 4.5523 (4.6112)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [710/780]  eta: 0:00:19  lr: 0.000001  loss: 4.5633 (4.6105)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [720/780]  eta: 0:00:16  lr: 0.000001  loss: 4.5710 (4.6100)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [730/780]  eta: 0:00:14  lr: 0.000001  loss: 4.5573 (4.6094)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [740/780]  eta: 0:00:11  lr: 0.000001  loss: 4.5817 (4.6091)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [750/780]  eta: 0:00:08  lr: 0.000001  loss: 4.5908 (4.6086)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [760/780]  eta: 0:00:05  lr: 0.000001  loss: 4.5855 (4.6085)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [0]  [770/780]  eta: 0:00:02  lr: 0.000001  loss: 4.5724 (4.6077)  time: 0.2780  data: 0.0001  max mem: 8734\n",
            "Epoch: [0]  [779/780]  eta: 0:00:00  lr: 0.000001  loss: 4.5571 (4.6071)  time: 0.2769  data: 0.0001  max mem: 8734\n",
            "Epoch: [0] Total time: 0:03:39 (0.2818 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 4.5571 (4.6071)\n",
            "Test:  [  0/105]  eta: 0:02:45  loss: 4.4725 (4.4725)  acc1: 3.1250 (3.1250)  acc5: 17.7083 (17.7083)  time: 1.5754  data: 0.9391  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:27  loss: 4.4582 (4.4666)  acc1: 4.1667 (4.4508)  acc5: 16.6667 (16.0985)  time: 0.2854  data: 0.0899  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:18  loss: 4.4585 (4.4685)  acc1: 4.1667 (4.2163)  acc5: 15.6250 (15.8234)  time: 0.1442  data: 0.0026  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:13  loss: 4.4703 (4.4710)  acc1: 3.1250 (4.0323)  acc5: 14.5833 (15.7258)  time: 0.1322  data: 0.0002  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:11  loss: 4.4768 (4.4750)  acc1: 3.1250 (3.7856)  acc5: 14.5833 (15.2947)  time: 0.1320  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:09  loss: 4.4754 (4.4754)  acc1: 3.1250 (3.6560)  acc5: 13.5417 (15.1961)  time: 0.1318  data: 0.0006  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:07  loss: 4.4666 (4.4767)  acc1: 3.1250 (3.6885)  acc5: 13.5417 (14.9590)  time: 0.1324  data: 0.0006  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 4.4754 (4.4767)  acc1: 4.1667 (3.6972)  acc5: 12.5000 (14.8181)  time: 0.1330  data: 0.0002  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 4.4754 (4.4775)  acc1: 2.0833 (3.5365)  acc5: 12.5000 (14.6219)  time: 0.1332  data: 0.0002  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 4.4819 (4.4790)  acc1: 2.0833 (3.5600)  acc5: 13.5417 (14.4689)  time: 0.1322  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 4.4765 (4.4789)  acc1: 3.1250 (3.5272)  acc5: 13.5417 (14.5111)  time: 0.1309  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 4.4713 (4.4776)  acc1: 3.1250 (3.5500)  acc5: 13.5417 (14.6400)  time: 0.1264  data: 0.0001  max mem: 8734\n",
            "Test: Total time: 0:00:15 (0.1485 s / it)\n",
            "* Acc@1 3.550 Acc@5 14.640 loss 4.478\n",
            "Accuracy of the network on the 10000 test images: 3.6%\n",
            "Max accuracy: 3.55%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [1]  [  0/780]  eta: 0:16:56  lr: 0.000001  loss: 4.6137 (4.6137)  time: 1.3032  data: 0.9363  max mem: 8734\n",
            "Epoch: [1]  [ 10/780]  eta: 0:04:46  lr: 0.000001  loss: 4.5839 (4.5888)  time: 0.3726  data: 0.0853  max mem: 8734\n",
            "Epoch: [1]  [ 20/780]  eta: 0:04:09  lr: 0.000001  loss: 4.5656 (4.5701)  time: 0.2802  data: 0.0004  max mem: 8734\n",
            "Epoch: [1]  [ 30/780]  eta: 0:03:55  lr: 0.000001  loss: 4.5572 (4.5628)  time: 0.2808  data: 0.0004  max mem: 8734\n",
            "Epoch: [1]  [ 40/780]  eta: 0:03:45  lr: 0.000001  loss: 4.5550 (4.5577)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [ 50/780]  eta: 0:03:39  lr: 0.000001  loss: 4.5550 (4.5552)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [ 60/780]  eta: 0:03:33  lr: 0.000001  loss: 4.5694 (4.5602)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [ 70/780]  eta: 0:03:29  lr: 0.000001  loss: 4.5733 (4.5605)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [ 80/780]  eta: 0:03:24  lr: 0.000001  loss: 4.5624 (4.5624)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [ 90/780]  eta: 0:03:20  lr: 0.000001  loss: 4.5536 (4.5609)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [100/780]  eta: 0:03:17  lr: 0.000001  loss: 4.5491 (4.5602)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [110/780]  eta: 0:03:13  lr: 0.000001  loss: 4.5422 (4.5570)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [120/780]  eta: 0:03:10  lr: 0.000001  loss: 4.5313 (4.5547)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [130/780]  eta: 0:03:07  lr: 0.000001  loss: 4.5313 (4.5519)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [140/780]  eta: 0:03:03  lr: 0.000001  loss: 4.5287 (4.5496)  time: 0.2785  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [150/780]  eta: 0:03:00  lr: 0.000001  loss: 4.5195 (4.5488)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [160/780]  eta: 0:02:57  lr: 0.000001  loss: 4.5505 (4.5498)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [170/780]  eta: 0:02:54  lr: 0.000001  loss: 4.5538 (4.5486)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [180/780]  eta: 0:02:51  lr: 0.000001  loss: 4.5290 (4.5478)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [190/780]  eta: 0:02:48  lr: 0.000001  loss: 4.5503 (4.5482)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [200/780]  eta: 0:02:45  lr: 0.000001  loss: 4.5501 (4.5478)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [210/780]  eta: 0:02:42  lr: 0.000001  loss: 4.5527 (4.5476)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [220/780]  eta: 0:02:39  lr: 0.000001  loss: 4.5527 (4.5474)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [230/780]  eta: 0:02:36  lr: 0.000001  loss: 4.5414 (4.5469)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [240/780]  eta: 0:02:33  lr: 0.000001  loss: 4.5338 (4.5457)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [250/780]  eta: 0:02:30  lr: 0.000001  loss: 4.5338 (4.5455)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [260/780]  eta: 0:02:27  lr: 0.000001  loss: 4.5407 (4.5450)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [270/780]  eta: 0:02:24  lr: 0.000001  loss: 4.5306 (4.5444)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [280/780]  eta: 0:02:21  lr: 0.000001  loss: 4.5334 (4.5442)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [290/780]  eta: 0:02:18  lr: 0.000001  loss: 4.5465 (4.5441)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [300/780]  eta: 0:02:15  lr: 0.000001  loss: 4.5398 (4.5438)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [310/780]  eta: 0:02:12  lr: 0.000001  loss: 4.5358 (4.5436)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [320/780]  eta: 0:02:10  lr: 0.000001  loss: 4.5381 (4.5429)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [330/780]  eta: 0:02:07  lr: 0.000001  loss: 4.5293 (4.5423)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [340/780]  eta: 0:02:04  lr: 0.000001  loss: 4.5110 (4.5415)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [350/780]  eta: 0:02:01  lr: 0.000001  loss: 4.5264 (4.5417)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [360/780]  eta: 0:01:58  lr: 0.000001  loss: 4.5349 (4.5410)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [370/780]  eta: 0:01:55  lr: 0.000001  loss: 4.5108 (4.5400)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [380/780]  eta: 0:01:52  lr: 0.000001  loss: 4.5120 (4.5390)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [390/780]  eta: 0:01:50  lr: 0.000001  loss: 4.5120 (4.5384)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [400/780]  eta: 0:01:47  lr: 0.000001  loss: 4.5465 (4.5385)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [410/780]  eta: 0:01:44  lr: 0.000001  loss: 4.5382 (4.5381)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [420/780]  eta: 0:01:41  lr: 0.000001  loss: 4.4988 (4.5375)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [430/780]  eta: 0:01:38  lr: 0.000001  loss: 4.4979 (4.5367)  time: 0.2831  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [440/780]  eta: 0:01:35  lr: 0.000001  loss: 4.5169 (4.5364)  time: 0.2835  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [450/780]  eta: 0:01:33  lr: 0.000001  loss: 4.5209 (4.5357)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [460/780]  eta: 0:01:30  lr: 0.000001  loss: 4.5145 (4.5353)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [470/780]  eta: 0:01:27  lr: 0.000001  loss: 4.5137 (4.5343)  time: 0.2815  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [480/780]  eta: 0:01:24  lr: 0.000001  loss: 4.5181 (4.5344)  time: 0.2818  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [490/780]  eta: 0:01:21  lr: 0.000001  loss: 4.5335 (4.5345)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [500/780]  eta: 0:01:18  lr: 0.000001  loss: 4.5335 (4.5342)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [510/780]  eta: 0:01:16  lr: 0.000001  loss: 4.5019 (4.5336)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [520/780]  eta: 0:01:13  lr: 0.000001  loss: 4.5108 (4.5335)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [530/780]  eta: 0:01:10  lr: 0.000001  loss: 4.5108 (4.5329)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [540/780]  eta: 0:01:07  lr: 0.000001  loss: 4.4766 (4.5318)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [550/780]  eta: 0:01:04  lr: 0.000001  loss: 4.4750 (4.5314)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [560/780]  eta: 0:01:01  lr: 0.000001  loss: 4.5075 (4.5310)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [570/780]  eta: 0:00:59  lr: 0.000001  loss: 4.4993 (4.5305)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [580/780]  eta: 0:00:56  lr: 0.000001  loss: 4.5062 (4.5303)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [590/780]  eta: 0:00:53  lr: 0.000001  loss: 4.5098 (4.5300)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [600/780]  eta: 0:00:50  lr: 0.000001  loss: 4.5036 (4.5297)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [610/780]  eta: 0:00:47  lr: 0.000001  loss: 4.5005 (4.5292)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [620/780]  eta: 0:00:45  lr: 0.000001  loss: 4.5005 (4.5286)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [630/780]  eta: 0:00:42  lr: 0.000001  loss: 4.4693 (4.5282)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [640/780]  eta: 0:00:39  lr: 0.000001  loss: 4.4903 (4.5276)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [650/780]  eta: 0:00:36  lr: 0.000001  loss: 4.4903 (4.5269)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [660/780]  eta: 0:00:33  lr: 0.000001  loss: 4.4723 (4.5264)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [670/780]  eta: 0:00:30  lr: 0.000001  loss: 4.4918 (4.5259)  time: 0.2782  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [680/780]  eta: 0:00:28  lr: 0.000001  loss: 4.4918 (4.5255)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [690/780]  eta: 0:00:25  lr: 0.000001  loss: 4.4839 (4.5249)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [700/780]  eta: 0:00:22  lr: 0.000001  loss: 4.4548 (4.5236)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [710/780]  eta: 0:00:19  lr: 0.000001  loss: 4.4417 (4.5225)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [720/780]  eta: 0:00:16  lr: 0.000001  loss: 4.4565 (4.5219)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [730/780]  eta: 0:00:14  lr: 0.000001  loss: 4.4863 (4.5214)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [740/780]  eta: 0:00:11  lr: 0.000001  loss: 4.5194 (4.5213)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [750/780]  eta: 0:00:08  lr: 0.000001  loss: 4.4852 (4.5205)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [760/780]  eta: 0:00:05  lr: 0.000001  loss: 4.4607 (4.5203)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [770/780]  eta: 0:00:02  lr: 0.000001  loss: 4.4791 (4.5193)  time: 0.2784  data: 0.0002  max mem: 8734\n",
            "Epoch: [1]  [779/780]  eta: 0:00:00  lr: 0.000001  loss: 4.4578 (4.5188)  time: 0.2770  data: 0.0001  max mem: 8734\n",
            "Epoch: [1] Total time: 0:03:39 (0.2813 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 4.4578 (4.5188)\n",
            "Test:  [  0/105]  eta: 0:02:04  loss: 4.2858 (4.2858)  acc1: 14.5833 (14.5833)  acc5: 28.1250 (28.1250)  time: 1.1819  data: 1.0251  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:21  loss: 4.2662 (4.2620)  acc1: 11.4583 (10.7955)  acc5: 30.2083 (30.3030)  time: 0.2294  data: 0.0935  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:15  loss: 4.2553 (4.2632)  acc1: 11.4583 (11.5575)  acc5: 30.2083 (30.2579)  time: 0.1335  data: 0.0012  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 4.2643 (4.2687)  acc1: 11.4583 (11.0551)  acc5: 31.2500 (30.0739)  time: 0.1331  data: 0.0012  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 4.2782 (4.2739)  acc1: 9.3750 (10.4675)  acc5: 28.1250 (29.5986)  time: 0.1337  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 4.2745 (4.2747)  acc1: 8.3333 (10.2533)  acc5: 28.1250 (29.0850)  time: 0.1331  data: 0.0002  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 4.2668 (4.2754)  acc1: 9.3750 (10.3313)  acc5: 28.1250 (29.1496)  time: 0.1325  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 4.2732 (4.2759)  acc1: 9.3750 (10.1819)  acc5: 27.0833 (29.0493)  time: 0.1323  data: 0.0006  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 4.2680 (4.2768)  acc1: 9.3750 (10.0694)  acc5: 27.0833 (28.8966)  time: 0.1324  data: 0.0006  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 4.2733 (4.2773)  acc1: 8.3333 (9.9931)  acc5: 28.1250 (28.7775)  time: 0.1319  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 4.2745 (4.2779)  acc1: 9.3750 (9.9732)  acc5: 27.0833 (28.7232)  time: 0.1310  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 4.2656 (4.2759)  acc1: 9.3750 (10.1200)  acc5: 28.1250 (28.8200)  time: 0.1257  data: 0.0002  max mem: 8734\n",
            "Test: Total time: 0:00:14 (0.1427 s / it)\n",
            "* Acc@1 10.120 Acc@5 28.820 loss 4.276\n",
            "Accuracy of the network on the 10000 test images: 10.1%\n",
            "Max accuracy: 10.12%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [2]  [  0/780]  eta: 0:16:37  lr: 0.000013  loss: 4.3715 (4.3715)  time: 1.2789  data: 0.9283  max mem: 8734\n",
            "Epoch: [2]  [ 10/780]  eta: 0:04:47  lr: 0.000013  loss: 4.4077 (4.4017)  time: 0.3734  data: 0.0860  max mem: 8734\n",
            "Epoch: [2]  [ 20/780]  eta: 0:04:09  lr: 0.000013  loss: 4.4106 (4.4136)  time: 0.2812  data: 0.0010  max mem: 8734\n",
            "Epoch: [2]  [ 30/780]  eta: 0:03:54  lr: 0.000013  loss: 4.4388 (4.4297)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [ 40/780]  eta: 0:03:45  lr: 0.000013  loss: 4.4518 (4.4246)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [ 50/780]  eta: 0:03:38  lr: 0.000013  loss: 4.3780 (4.4147)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [ 60/780]  eta: 0:03:33  lr: 0.000013  loss: 4.3488 (4.3998)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [ 70/780]  eta: 0:03:29  lr: 0.000013  loss: 4.3488 (4.3971)  time: 0.2809  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [ 80/780]  eta: 0:03:24  lr: 0.000013  loss: 4.3842 (4.3895)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [ 90/780]  eta: 0:03:21  lr: 0.000013  loss: 4.2160 (4.3632)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [100/780]  eta: 0:03:17  lr: 0.000013  loss: 4.1570 (4.3429)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [110/780]  eta: 0:03:13  lr: 0.000013  loss: 4.1256 (4.3197)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [120/780]  eta: 0:03:10  lr: 0.000013  loss: 4.1342 (4.3032)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [130/780]  eta: 0:03:06  lr: 0.000013  loss: 4.1181 (4.2832)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [140/780]  eta: 0:03:03  lr: 0.000013  loss: 4.1318 (4.2730)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [150/780]  eta: 0:03:00  lr: 0.000013  loss: 4.0834 (4.2572)  time: 0.2809  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [160/780]  eta: 0:02:57  lr: 0.000013  loss: 4.0077 (4.2420)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [170/780]  eta: 0:02:54  lr: 0.000013  loss: 4.0591 (4.2286)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [180/780]  eta: 0:02:51  lr: 0.000013  loss: 4.0577 (4.2134)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [190/780]  eta: 0:02:48  lr: 0.000013  loss: 3.9781 (4.1995)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [200/780]  eta: 0:02:45  lr: 0.000013  loss: 3.8614 (4.1816)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [210/780]  eta: 0:02:42  lr: 0.000013  loss: 3.8622 (4.1698)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [220/780]  eta: 0:02:39  lr: 0.000013  loss: 3.9263 (4.1601)  time: 0.2809  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [230/780]  eta: 0:02:36  lr: 0.000013  loss: 3.8903 (4.1418)  time: 0.2811  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [240/780]  eta: 0:02:33  lr: 0.000013  loss: 3.8181 (4.1270)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [250/780]  eta: 0:02:30  lr: 0.000013  loss: 3.8128 (4.1096)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [260/780]  eta: 0:02:27  lr: 0.000013  loss: 3.7774 (4.0971)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [270/780]  eta: 0:02:24  lr: 0.000013  loss: 3.7774 (4.0844)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [280/780]  eta: 0:02:21  lr: 0.000013  loss: 3.7020 (4.0688)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [290/780]  eta: 0:02:18  lr: 0.000013  loss: 3.6260 (4.0524)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [300/780]  eta: 0:02:16  lr: 0.000013  loss: 3.5137 (4.0381)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [310/780]  eta: 0:02:13  lr: 0.000013  loss: 3.4818 (4.0227)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [320/780]  eta: 0:02:10  lr: 0.000013  loss: 3.6933 (4.0116)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [330/780]  eta: 0:02:07  lr: 0.000013  loss: 3.5437 (3.9954)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [340/780]  eta: 0:02:04  lr: 0.000013  loss: 3.4035 (3.9787)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [350/780]  eta: 0:02:01  lr: 0.000013  loss: 3.5233 (3.9677)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [360/780]  eta: 0:01:58  lr: 0.000013  loss: 3.4266 (3.9503)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [370/780]  eta: 0:01:55  lr: 0.000013  loss: 3.4057 (3.9353)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [380/780]  eta: 0:01:53  lr: 0.000013  loss: 3.2972 (3.9187)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [390/780]  eta: 0:01:50  lr: 0.000013  loss: 3.2709 (3.9030)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [400/780]  eta: 0:01:47  lr: 0.000013  loss: 3.3248 (3.8906)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [410/780]  eta: 0:01:44  lr: 0.000013  loss: 3.4200 (3.8772)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [420/780]  eta: 0:01:41  lr: 0.000013  loss: 3.4200 (3.8656)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [430/780]  eta: 0:01:38  lr: 0.000013  loss: 3.3669 (3.8548)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [440/780]  eta: 0:01:35  lr: 0.000013  loss: 3.3704 (3.8451)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [450/780]  eta: 0:01:33  lr: 0.000013  loss: 3.5957 (3.8399)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [460/780]  eta: 0:01:30  lr: 0.000013  loss: 3.4746 (3.8283)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [470/780]  eta: 0:01:27  lr: 0.000013  loss: 3.4540 (3.8213)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [480/780]  eta: 0:01:24  lr: 0.000013  loss: 3.6029 (3.8146)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [490/780]  eta: 0:01:21  lr: 0.000013  loss: 3.2865 (3.8025)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [500/780]  eta: 0:01:18  lr: 0.000013  loss: 3.2145 (3.7902)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [510/780]  eta: 0:01:16  lr: 0.000013  loss: 3.2922 (3.7821)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [520/780]  eta: 0:01:13  lr: 0.000013  loss: 3.3802 (3.7709)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [530/780]  eta: 0:01:10  lr: 0.000013  loss: 3.4306 (3.7661)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [540/780]  eta: 0:01:07  lr: 0.000013  loss: 3.4911 (3.7552)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [550/780]  eta: 0:01:04  lr: 0.000013  loss: 3.3435 (3.7482)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [560/780]  eta: 0:01:01  lr: 0.000013  loss: 3.3435 (3.7376)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [570/780]  eta: 0:00:59  lr: 0.000013  loss: 3.2895 (3.7298)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [580/780]  eta: 0:00:56  lr: 0.000013  loss: 3.3463 (3.7236)  time: 0.2784  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [590/780]  eta: 0:00:53  lr: 0.000013  loss: 3.3577 (3.7147)  time: 0.2783  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [600/780]  eta: 0:00:50  lr: 0.000013  loss: 3.3577 (3.7086)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [610/780]  eta: 0:00:47  lr: 0.000013  loss: 3.3738 (3.7024)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [620/780]  eta: 0:00:45  lr: 0.000013  loss: 3.2346 (3.6934)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [630/780]  eta: 0:00:42  lr: 0.000013  loss: 3.3856 (3.6888)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [640/780]  eta: 0:00:39  lr: 0.000013  loss: 3.2796 (3.6778)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [650/780]  eta: 0:00:36  lr: 0.000013  loss: 3.1241 (3.6704)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [660/780]  eta: 0:00:33  lr: 0.000013  loss: 3.1472 (3.6602)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [670/780]  eta: 0:00:30  lr: 0.000013  loss: 3.1267 (3.6506)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [680/780]  eta: 0:00:28  lr: 0.000013  loss: 3.0774 (3.6427)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [690/780]  eta: 0:00:25  lr: 0.000013  loss: 2.8659 (3.6324)  time: 0.2785  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [700/780]  eta: 0:00:22  lr: 0.000013  loss: 2.9361 (3.6244)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [710/780]  eta: 0:00:19  lr: 0.000013  loss: 3.0572 (3.6182)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [720/780]  eta: 0:00:16  lr: 0.000013  loss: 3.3056 (3.6135)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [730/780]  eta: 0:00:14  lr: 0.000013  loss: 3.2511 (3.6046)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [740/780]  eta: 0:00:11  lr: 0.000013  loss: 3.0632 (3.5966)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [750/780]  eta: 0:00:08  lr: 0.000013  loss: 3.0632 (3.5875)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [760/780]  eta: 0:00:05  lr: 0.000013  loss: 3.1563 (3.5833)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [770/780]  eta: 0:00:02  lr: 0.000013  loss: 3.3088 (3.5797)  time: 0.2781  data: 0.0002  max mem: 8734\n",
            "Epoch: [2]  [779/780]  eta: 0:00:00  lr: 0.000013  loss: 3.2210 (3.5727)  time: 0.2774  data: 0.0001  max mem: 8734\n",
            "Epoch: [2] Total time: 0:03:39 (0.2811 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 3.2210 (3.5727)\n",
            "Test:  [  0/105]  eta: 0:01:49  loss: 1.1935 (1.1935)  acc1: 80.2083 (80.2083)  acc5: 97.9167 (97.9167)  time: 1.0441  data: 0.8896  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:20  loss: 1.3732 (1.3695)  acc1: 76.0417 (73.4849)  acc5: 95.8333 (94.8864)  time: 0.2179  data: 0.0815  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:15  loss: 1.3616 (1.3591)  acc1: 73.9583 (73.8591)  acc5: 93.7500 (94.6429)  time: 0.1332  data: 0.0004  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 1.3616 (1.3616)  acc1: 72.9167 (73.3535)  acc5: 93.7500 (94.6237)  time: 0.1317  data: 0.0002  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 1.3307 (1.3471)  acc1: 73.9583 (73.8821)  acc5: 94.7917 (95.1728)  time: 0.1325  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 1.3173 (1.3521)  acc1: 73.9583 (73.5703)  acc5: 96.8750 (95.1593)  time: 0.1331  data: 0.0002  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 1.3436 (1.3499)  acc1: 73.9583 (73.7876)  acc5: 94.7917 (95.2527)  time: 0.1339  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 1.3253 (1.3518)  acc1: 73.9583 (73.6943)  acc5: 94.7917 (95.2905)  time: 0.1337  data: 0.0006  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 1.3253 (1.3512)  acc1: 75.0000 (73.9198)  acc5: 95.8333 (95.3961)  time: 0.1333  data: 0.0006  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 1.2984 (1.3501)  acc1: 75.0000 (74.0385)  acc5: 95.8333 (95.4670)  time: 0.1327  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 1.3601 (1.3516)  acc1: 73.9583 (73.9790)  acc5: 95.8333 (95.5033)  time: 0.1315  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 1.3113 (1.3495)  acc1: 73.9583 (74.0700)  acc5: 95.8333 (95.5100)  time: 0.1258  data: 0.0002  max mem: 8734\n",
            "Test: Total time: 0:00:14 (0.1416 s / it)\n",
            "* Acc@1 74.070 Acc@5 95.510 loss 1.350\n",
            "Accuracy of the network on the 10000 test images: 74.1%\n",
            "Max accuracy: 74.07%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [3]  [  0/780]  eta: 0:16:52  lr: 0.000026  loss: 2.9962 (2.9962)  time: 1.2980  data: 0.9362  max mem: 8734\n",
            "Epoch: [3]  [ 10/780]  eta: 0:04:49  lr: 0.000026  loss: 3.0534 (3.0272)  time: 0.3754  data: 0.0853  max mem: 8734\n",
            "Epoch: [3]  [ 20/780]  eta: 0:04:10  lr: 0.000026  loss: 3.0328 (3.0130)  time: 0.2813  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [ 30/780]  eta: 0:03:55  lr: 0.000026  loss: 3.0222 (3.0450)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [ 40/780]  eta: 0:03:46  lr: 0.000026  loss: 3.0794 (3.0650)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [ 50/780]  eta: 0:03:39  lr: 0.000026  loss: 3.0627 (3.0363)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [ 60/780]  eta: 0:03:33  lr: 0.000026  loss: 3.0005 (3.0259)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [ 70/780]  eta: 0:03:28  lr: 0.000026  loss: 3.1841 (3.0621)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [ 80/780]  eta: 0:03:24  lr: 0.000026  loss: 3.1372 (3.0499)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [ 90/780]  eta: 0:03:20  lr: 0.000026  loss: 3.0580 (3.0541)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [100/780]  eta: 0:03:17  lr: 0.000026  loss: 3.1326 (3.0640)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [110/780]  eta: 0:03:13  lr: 0.000026  loss: 3.1329 (3.0716)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [120/780]  eta: 0:03:10  lr: 0.000026  loss: 3.1994 (3.0714)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [130/780]  eta: 0:03:06  lr: 0.000026  loss: 3.1200 (3.0553)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [140/780]  eta: 0:03:03  lr: 0.000026  loss: 2.8203 (3.0441)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [150/780]  eta: 0:03:00  lr: 0.000026  loss: 3.0426 (3.0545)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [160/780]  eta: 0:02:57  lr: 0.000026  loss: 3.0426 (3.0495)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [170/780]  eta: 0:02:54  lr: 0.000026  loss: 2.7315 (3.0393)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [180/780]  eta: 0:02:51  lr: 0.000026  loss: 2.9191 (3.0424)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [190/780]  eta: 0:02:48  lr: 0.000026  loss: 3.2181 (3.0493)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [200/780]  eta: 0:02:45  lr: 0.000026  loss: 3.1261 (3.0457)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [210/780]  eta: 0:02:42  lr: 0.000026  loss: 3.1022 (3.0520)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [220/780]  eta: 0:02:39  lr: 0.000026  loss: 3.3934 (3.0677)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [230/780]  eta: 0:02:36  lr: 0.000026  loss: 3.2281 (3.0712)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [240/780]  eta: 0:02:33  lr: 0.000026  loss: 3.0134 (3.0629)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [250/780]  eta: 0:02:30  lr: 0.000026  loss: 2.8182 (3.0512)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [260/780]  eta: 0:02:27  lr: 0.000026  loss: 2.9118 (3.0504)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [270/780]  eta: 0:02:24  lr: 0.000026  loss: 2.9601 (3.0380)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [280/780]  eta: 0:02:21  lr: 0.000026  loss: 2.9136 (3.0372)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [290/780]  eta: 0:02:18  lr: 0.000026  loss: 3.1197 (3.0335)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [300/780]  eta: 0:02:15  lr: 0.000026  loss: 3.1197 (3.0362)  time: 0.2811  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [310/780]  eta: 0:02:13  lr: 0.000026  loss: 3.2379 (3.0340)  time: 0.2840  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [320/780]  eta: 0:02:10  lr: 0.000026  loss: 3.0732 (3.0287)  time: 0.2846  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [330/780]  eta: 0:02:07  lr: 0.000026  loss: 2.9382 (3.0280)  time: 0.2837  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [340/780]  eta: 0:02:04  lr: 0.000026  loss: 3.0152 (3.0275)  time: 0.2814  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [350/780]  eta: 0:02:01  lr: 0.000026  loss: 3.0308 (3.0258)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [360/780]  eta: 0:01:58  lr: 0.000026  loss: 3.0543 (3.0272)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [370/780]  eta: 0:01:55  lr: 0.000026  loss: 2.9843 (3.0258)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [380/780]  eta: 0:01:53  lr: 0.000026  loss: 2.8651 (3.0226)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [390/780]  eta: 0:01:50  lr: 0.000026  loss: 2.9343 (3.0218)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [400/780]  eta: 0:01:47  lr: 0.000026  loss: 2.7607 (3.0124)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [410/780]  eta: 0:01:44  lr: 0.000026  loss: 2.8200 (3.0109)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [420/780]  eta: 0:01:41  lr: 0.000026  loss: 2.8363 (3.0031)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [430/780]  eta: 0:01:38  lr: 0.000026  loss: 2.9316 (3.0087)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [440/780]  eta: 0:01:35  lr: 0.000026  loss: 3.1661 (3.0065)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [450/780]  eta: 0:01:33  lr: 0.000026  loss: 2.7586 (2.9999)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [460/780]  eta: 0:01:30  lr: 0.000026  loss: 2.7920 (2.9945)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [470/780]  eta: 0:01:27  lr: 0.000026  loss: 2.9327 (2.9935)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [480/780]  eta: 0:01:24  lr: 0.000026  loss: 3.0211 (2.9929)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [490/780]  eta: 0:01:21  lr: 0.000026  loss: 2.8224 (2.9875)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [500/780]  eta: 0:01:18  lr: 0.000026  loss: 2.7025 (2.9823)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [510/780]  eta: 0:01:16  lr: 0.000026  loss: 2.7025 (2.9769)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [520/780]  eta: 0:01:13  lr: 0.000026  loss: 2.5269 (2.9660)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [530/780]  eta: 0:01:10  lr: 0.000026  loss: 2.5161 (2.9639)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [540/780]  eta: 0:01:07  lr: 0.000026  loss: 2.9416 (2.9610)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [550/780]  eta: 0:01:04  lr: 0.000026  loss: 2.8366 (2.9543)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [560/780]  eta: 0:01:01  lr: 0.000026  loss: 2.9454 (2.9547)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [570/780]  eta: 0:00:59  lr: 0.000026  loss: 2.9662 (2.9509)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [580/780]  eta: 0:00:56  lr: 0.000026  loss: 2.7970 (2.9500)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [590/780]  eta: 0:00:53  lr: 0.000026  loss: 3.0179 (2.9519)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [600/780]  eta: 0:00:50  lr: 0.000026  loss: 3.0929 (2.9524)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [610/780]  eta: 0:00:47  lr: 0.000026  loss: 2.8649 (2.9478)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [620/780]  eta: 0:00:45  lr: 0.000026  loss: 2.7480 (2.9448)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [630/780]  eta: 0:00:42  lr: 0.000026  loss: 2.9073 (2.9443)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [640/780]  eta: 0:00:39  lr: 0.000026  loss: 2.9073 (2.9431)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [650/780]  eta: 0:00:36  lr: 0.000026  loss: 2.8920 (2.9422)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [660/780]  eta: 0:00:33  lr: 0.000026  loss: 2.8873 (2.9393)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [670/780]  eta: 0:00:30  lr: 0.000026  loss: 2.9025 (2.9376)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [680/780]  eta: 0:00:28  lr: 0.000026  loss: 2.9025 (2.9378)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [690/780]  eta: 0:00:25  lr: 0.000026  loss: 2.7586 (2.9330)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [700/780]  eta: 0:00:22  lr: 0.000026  loss: 2.7974 (2.9330)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [710/780]  eta: 0:00:19  lr: 0.000026  loss: 2.9401 (2.9327)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [720/780]  eta: 0:00:16  lr: 0.000026  loss: 2.8988 (2.9313)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [730/780]  eta: 0:00:14  lr: 0.000026  loss: 2.5842 (2.9263)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [740/780]  eta: 0:00:11  lr: 0.000026  loss: 2.5842 (2.9236)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [750/780]  eta: 0:00:08  lr: 0.000026  loss: 2.7689 (2.9215)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [760/780]  eta: 0:00:05  lr: 0.000026  loss: 2.9805 (2.9236)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [770/780]  eta: 0:00:02  lr: 0.000026  loss: 2.9805 (2.9203)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [3]  [779/780]  eta: 0:00:00  lr: 0.000026  loss: 2.6534 (2.9182)  time: 0.2771  data: 0.0001  max mem: 8734\n",
            "Epoch: [3] Total time: 0:03:39 (0.2812 s / it)\n",
            "Averaged stats: lr: 0.000026  loss: 2.6534 (2.9182)\n",
            "Test:  [  0/105]  eta: 0:01:41  loss: 0.6581 (0.6581)  acc1: 85.4167 (85.4167)  acc5: 100.0000 (100.0000)  time: 0.9673  data: 0.8209  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:20  loss: 0.8120 (0.8193)  acc1: 83.3333 (82.2917)  acc5: 97.9167 (97.9167)  time: 0.2132  data: 0.0754  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:14  loss: 0.8120 (0.8175)  acc1: 83.3333 (82.0437)  acc5: 97.9167 (97.7679)  time: 0.1349  data: 0.0009  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 0.8465 (0.8318)  acc1: 80.2083 (81.4516)  acc5: 97.9167 (97.4462)  time: 0.1328  data: 0.0006  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 0.8125 (0.8236)  acc1: 79.1667 (81.3262)  acc5: 97.9167 (97.6372)  time: 0.1332  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 0.8277 (0.8274)  acc1: 80.2083 (81.2500)  acc5: 97.9167 (97.6716)  time: 0.1323  data: 0.0002  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 0.7787 (0.8199)  acc1: 81.2500 (81.3866)  acc5: 97.9167 (97.7971)  time: 0.1320  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 0.7787 (0.8226)  acc1: 81.2500 (81.1473)  acc5: 97.9167 (97.7259)  time: 0.1327  data: 0.0006  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 0.7991 (0.8204)  acc1: 81.2500 (81.2757)  acc5: 96.8750 (97.6595)  time: 0.1333  data: 0.0006  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 0.7912 (0.8151)  acc1: 83.3333 (81.5591)  acc5: 97.9167 (97.6877)  time: 0.1323  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.8021 (0.8158)  acc1: 82.2917 (81.5388)  acc5: 97.9167 (97.6588)  time: 0.1310  data: 0.0001  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.7908 (0.8144)  acc1: 82.2917 (81.5500)  acc5: 97.9167 (97.6800)  time: 0.1257  data: 0.0001  max mem: 8734\n",
            "Test: Total time: 0:00:14 (0.1408 s / it)\n",
            "* Acc@1 81.550 Acc@5 97.680 loss 0.814\n",
            "Accuracy of the network on the 10000 test images: 81.6%\n",
            "Max accuracy: 81.55%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [4]  [  0/780]  eta: 0:16:22  lr: 0.000038  loss: 2.7645 (2.7645)  time: 1.2594  data: 0.9159  max mem: 8734\n",
            "Epoch: [4]  [ 10/780]  eta: 0:04:42  lr: 0.000038  loss: 2.8070 (2.8201)  time: 0.3671  data: 0.0834  max mem: 8734\n",
            "Epoch: [4]  [ 20/780]  eta: 0:04:07  lr: 0.000038  loss: 2.6297 (2.7390)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [ 30/780]  eta: 0:03:52  lr: 0.000038  loss: 2.8088 (2.8039)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [ 40/780]  eta: 0:03:44  lr: 0.000038  loss: 2.9118 (2.7814)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [ 50/780]  eta: 0:03:37  lr: 0.000038  loss: 2.9099 (2.7962)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [ 60/780]  eta: 0:03:32  lr: 0.000038  loss: 2.7670 (2.7948)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [ 70/780]  eta: 0:03:28  lr: 0.000038  loss: 2.6806 (2.7883)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [ 80/780]  eta: 0:03:23  lr: 0.000038  loss: 2.8380 (2.8088)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [ 90/780]  eta: 0:03:20  lr: 0.000038  loss: 2.9474 (2.7999)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [100/780]  eta: 0:03:16  lr: 0.000038  loss: 2.7465 (2.8023)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [110/780]  eta: 0:03:12  lr: 0.000038  loss: 2.7776 (2.8056)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [120/780]  eta: 0:03:09  lr: 0.000038  loss: 3.0432 (2.8311)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [130/780]  eta: 0:03:06  lr: 0.000038  loss: 3.0253 (2.8231)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [140/780]  eta: 0:03:03  lr: 0.000038  loss: 2.8208 (2.8229)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [150/780]  eta: 0:03:00  lr: 0.000038  loss: 2.7801 (2.8150)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [160/780]  eta: 0:02:56  lr: 0.000038  loss: 2.7142 (2.8110)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [170/780]  eta: 0:02:53  lr: 0.000038  loss: 2.7297 (2.8141)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [180/780]  eta: 0:02:50  lr: 0.000038  loss: 2.8187 (2.8123)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [190/780]  eta: 0:02:47  lr: 0.000038  loss: 2.9292 (2.8195)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [200/780]  eta: 0:02:44  lr: 0.000038  loss: 2.9778 (2.8204)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [210/780]  eta: 0:02:41  lr: 0.000038  loss: 2.7742 (2.8089)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [220/780]  eta: 0:02:38  lr: 0.000038  loss: 2.4900 (2.7973)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [230/780]  eta: 0:02:35  lr: 0.000038  loss: 2.7382 (2.8014)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [240/780]  eta: 0:02:32  lr: 0.000038  loss: 2.8384 (2.7969)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [250/780]  eta: 0:02:30  lr: 0.000038  loss: 2.7852 (2.7935)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [260/780]  eta: 0:02:27  lr: 0.000038  loss: 2.8337 (2.7940)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [270/780]  eta: 0:02:24  lr: 0.000038  loss: 2.8500 (2.7941)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [280/780]  eta: 0:02:21  lr: 0.000038  loss: 2.6920 (2.7851)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [290/780]  eta: 0:02:18  lr: 0.000038  loss: 2.6999 (2.7902)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [300/780]  eta: 0:02:15  lr: 0.000038  loss: 2.9432 (2.7891)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [310/780]  eta: 0:02:12  lr: 0.000038  loss: 2.6839 (2.7851)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [320/780]  eta: 0:02:09  lr: 0.000038  loss: 2.5721 (2.7778)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [330/780]  eta: 0:02:06  lr: 0.000038  loss: 2.5941 (2.7778)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [340/780]  eta: 0:02:04  lr: 0.000038  loss: 2.8636 (2.7783)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [350/780]  eta: 0:02:01  lr: 0.000038  loss: 2.6905 (2.7699)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [360/780]  eta: 0:01:58  lr: 0.000038  loss: 2.6905 (2.7728)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [370/780]  eta: 0:01:55  lr: 0.000038  loss: 2.8166 (2.7673)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [380/780]  eta: 0:01:52  lr: 0.000038  loss: 2.7107 (2.7648)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [390/780]  eta: 0:01:49  lr: 0.000038  loss: 2.9035 (2.7649)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [400/780]  eta: 0:01:47  lr: 0.000038  loss: 2.8721 (2.7617)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [410/780]  eta: 0:01:44  lr: 0.000038  loss: 2.8122 (2.7618)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [420/780]  eta: 0:01:41  lr: 0.000038  loss: 2.6503 (2.7571)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [430/780]  eta: 0:01:38  lr: 0.000038  loss: 2.5881 (2.7528)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [440/780]  eta: 0:01:35  lr: 0.000038  loss: 2.8186 (2.7591)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [450/780]  eta: 0:01:32  lr: 0.000038  loss: 2.8332 (2.7583)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [460/780]  eta: 0:01:30  lr: 0.000038  loss: 2.8332 (2.7604)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [470/780]  eta: 0:01:27  lr: 0.000038  loss: 2.8651 (2.7602)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [480/780]  eta: 0:01:24  lr: 0.000038  loss: 2.8403 (2.7559)  time: 0.2812  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [490/780]  eta: 0:01:21  lr: 0.000038  loss: 2.8624 (2.7544)  time: 0.2831  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [500/780]  eta: 0:01:18  lr: 0.000038  loss: 2.8624 (2.7567)  time: 0.2828  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [510/780]  eta: 0:01:15  lr: 0.000038  loss: 2.8896 (2.7581)  time: 0.2831  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [520/780]  eta: 0:01:13  lr: 0.000038  loss: 2.8030 (2.7547)  time: 0.2829  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [530/780]  eta: 0:01:10  lr: 0.000038  loss: 2.7730 (2.7576)  time: 0.2816  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [540/780]  eta: 0:01:07  lr: 0.000038  loss: 2.9504 (2.7583)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [550/780]  eta: 0:01:04  lr: 0.000038  loss: 2.8527 (2.7590)  time: 0.2827  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [560/780]  eta: 0:01:01  lr: 0.000038  loss: 2.7792 (2.7568)  time: 0.2841  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [570/780]  eta: 0:00:59  lr: 0.000038  loss: 2.7713 (2.7570)  time: 0.2824  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [580/780]  eta: 0:00:56  lr: 0.000038  loss: 2.6175 (2.7537)  time: 0.2829  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [590/780]  eta: 0:00:53  lr: 0.000038  loss: 2.6615 (2.7536)  time: 0.2821  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [600/780]  eta: 0:00:50  lr: 0.000038  loss: 2.7014 (2.7534)  time: 0.2827  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [610/780]  eta: 0:00:47  lr: 0.000038  loss: 2.8319 (2.7549)  time: 0.2834  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [620/780]  eta: 0:00:45  lr: 0.000038  loss: 2.9461 (2.7565)  time: 0.2835  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [630/780]  eta: 0:00:42  lr: 0.000038  loss: 2.9172 (2.7556)  time: 0.2845  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [640/780]  eta: 0:00:39  lr: 0.000038  loss: 2.6574 (2.7549)  time: 0.2834  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [650/780]  eta: 0:00:36  lr: 0.000038  loss: 2.7105 (2.7560)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [660/780]  eta: 0:00:33  lr: 0.000038  loss: 2.6494 (2.7526)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [670/780]  eta: 0:00:30  lr: 0.000038  loss: 2.7124 (2.7535)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [680/780]  eta: 0:00:28  lr: 0.000038  loss: 2.8438 (2.7561)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [690/780]  eta: 0:00:25  lr: 0.000038  loss: 2.7296 (2.7525)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [700/780]  eta: 0:00:22  lr: 0.000038  loss: 2.5510 (2.7495)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [710/780]  eta: 0:00:19  lr: 0.000038  loss: 2.7523 (2.7509)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [720/780]  eta: 0:00:16  lr: 0.000038  loss: 2.7523 (2.7487)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [730/780]  eta: 0:00:14  lr: 0.000038  loss: 2.7516 (2.7479)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [740/780]  eta: 0:00:11  lr: 0.000038  loss: 2.7513 (2.7461)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [750/780]  eta: 0:00:08  lr: 0.000038  loss: 2.6201 (2.7456)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [760/780]  eta: 0:00:05  lr: 0.000038  loss: 2.6201 (2.7437)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [4]  [770/780]  eta: 0:00:02  lr: 0.000038  loss: 2.6003 (2.7421)  time: 0.2782  data: 0.0001  max mem: 8734\n",
            "Epoch: [4]  [779/780]  eta: 0:00:00  lr: 0.000038  loss: 2.6003 (2.7381)  time: 0.2774  data: 0.0001  max mem: 8734\n",
            "Epoch: [4] Total time: 0:03:39 (0.2814 s / it)\n",
            "Averaged stats: lr: 0.000038  loss: 2.6003 (2.7381)\n",
            "Test:  [  0/105]  eta: 0:02:20  loss: 0.5758 (0.5758)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 1.3408  data: 1.1956  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:22  loss: 0.6987 (0.7250)  acc1: 83.3333 (82.8599)  acc5: 98.9583 (98.3902)  time: 0.2416  data: 0.1089  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:16  loss: 0.6987 (0.7029)  acc1: 83.3333 (83.7798)  acc5: 97.9167 (98.2143)  time: 0.1322  data: 0.0002  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 0.7055 (0.7289)  acc1: 82.2917 (82.6613)  acc5: 96.8750 (97.6479)  time: 0.1330  data: 0.0002  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 0.7258 (0.7186)  acc1: 82.2917 (83.1047)  acc5: 97.9167 (97.7388)  time: 0.1325  data: 0.0006  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 0.6917 (0.7210)  acc1: 83.3333 (83.0065)  acc5: 97.9167 (97.8145)  time: 0.1314  data: 0.0006  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 0.6576 (0.7101)  acc1: 83.3333 (83.2992)  acc5: 98.9583 (97.9167)  time: 0.1316  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 0.6941 (0.7128)  acc1: 83.3333 (83.1133)  acc5: 98.9583 (97.9167)  time: 0.1321  data: 0.0002  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 0.7212 (0.7132)  acc1: 81.2500 (83.1276)  acc5: 97.9167 (97.8652)  time: 0.1321  data: 0.0002  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 0.6807 (0.7085)  acc1: 83.3333 (83.3448)  acc5: 97.9167 (97.9396)  time: 0.1323  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.6843 (0.7074)  acc1: 83.3333 (83.3952)  acc5: 97.9167 (97.9682)  time: 0.1317  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.6843 (0.7093)  acc1: 83.3333 (83.3400)  acc5: 97.9167 (97.9700)  time: 0.1258  data: 0.0002  max mem: 8734\n",
            "Test: Total time: 0:00:15 (0.1436 s / it)\n",
            "* Acc@1 83.340 Acc@5 97.970 loss 0.709\n",
            "Accuracy of the network on the 10000 test images: 83.3%\n",
            "Max accuracy: 83.34%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [5]  [  0/780]  eta: 0:16:41  lr: 0.000050  loss: 2.6364 (2.6364)  time: 1.2842  data: 0.9506  max mem: 8734\n",
            "Epoch: [5]  [ 10/780]  eta: 0:04:46  lr: 0.000050  loss: 2.7387 (2.6775)  time: 0.3721  data: 0.0866  max mem: 8734\n",
            "Epoch: [5]  [ 20/780]  eta: 0:04:09  lr: 0.000050  loss: 2.8127 (2.7094)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [ 30/780]  eta: 0:03:54  lr: 0.000050  loss: 2.8134 (2.6766)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [ 40/780]  eta: 0:03:45  lr: 0.000050  loss: 2.5754 (2.6359)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [ 50/780]  eta: 0:03:38  lr: 0.000050  loss: 2.4892 (2.6123)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [ 60/780]  eta: 0:03:33  lr: 0.000050  loss: 2.5957 (2.6113)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [ 70/780]  eta: 0:03:28  lr: 0.000050  loss: 2.8418 (2.6617)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [ 80/780]  eta: 0:03:24  lr: 0.000050  loss: 2.9491 (2.6778)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [ 90/780]  eta: 0:03:20  lr: 0.000050  loss: 2.9128 (2.6763)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [100/780]  eta: 0:03:16  lr: 0.000050  loss: 2.6290 (2.6633)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [110/780]  eta: 0:03:13  lr: 0.000050  loss: 2.4883 (2.6576)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [120/780]  eta: 0:03:09  lr: 0.000050  loss: 2.6734 (2.6568)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [130/780]  eta: 0:03:06  lr: 0.000050  loss: 2.5204 (2.6530)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [140/780]  eta: 0:03:03  lr: 0.000050  loss: 2.6385 (2.6589)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [150/780]  eta: 0:03:00  lr: 0.000050  loss: 2.8335 (2.6752)  time: 0.2804  data: 0.0004  max mem: 8734\n",
            "Epoch: [5]  [160/780]  eta: 0:02:57  lr: 0.000050  loss: 2.8266 (2.6764)  time: 0.2800  data: 0.0004  max mem: 8734\n",
            "Epoch: [5]  [170/780]  eta: 0:02:54  lr: 0.000050  loss: 2.6354 (2.6650)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [180/780]  eta: 0:02:51  lr: 0.000050  loss: 2.3765 (2.6577)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [190/780]  eta: 0:02:48  lr: 0.000050  loss: 2.4834 (2.6476)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [200/780]  eta: 0:02:45  lr: 0.000050  loss: 2.5225 (2.6459)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [210/780]  eta: 0:02:42  lr: 0.000050  loss: 2.7131 (2.6457)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [220/780]  eta: 0:02:39  lr: 0.000050  loss: 2.6958 (2.6417)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [230/780]  eta: 0:02:36  lr: 0.000050  loss: 2.6958 (2.6474)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [240/780]  eta: 0:02:33  lr: 0.000050  loss: 2.6395 (2.6437)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [250/780]  eta: 0:02:30  lr: 0.000050  loss: 2.6094 (2.6382)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [260/780]  eta: 0:02:27  lr: 0.000050  loss: 2.5424 (2.6353)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [270/780]  eta: 0:02:24  lr: 0.000050  loss: 2.5626 (2.6379)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [280/780]  eta: 0:02:21  lr: 0.000050  loss: 2.7124 (2.6440)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [290/780]  eta: 0:02:18  lr: 0.000050  loss: 2.6077 (2.6411)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [300/780]  eta: 0:02:15  lr: 0.000050  loss: 2.5941 (2.6386)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [310/780]  eta: 0:02:12  lr: 0.000050  loss: 2.8432 (2.6459)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [320/780]  eta: 0:02:10  lr: 0.000050  loss: 2.8890 (2.6485)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [330/780]  eta: 0:02:07  lr: 0.000050  loss: 2.4966 (2.6444)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [340/780]  eta: 0:02:04  lr: 0.000050  loss: 2.6270 (2.6430)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [350/780]  eta: 0:02:01  lr: 0.000050  loss: 2.7657 (2.6445)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [360/780]  eta: 0:01:58  lr: 0.000050  loss: 2.7904 (2.6497)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [370/780]  eta: 0:01:55  lr: 0.000050  loss: 2.5900 (2.6408)  time: 0.2814  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [380/780]  eta: 0:01:52  lr: 0.000050  loss: 2.4413 (2.6406)  time: 0.2821  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [390/780]  eta: 0:01:50  lr: 0.000050  loss: 2.5921 (2.6387)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [400/780]  eta: 0:01:47  lr: 0.000050  loss: 2.5921 (2.6359)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [410/780]  eta: 0:01:44  lr: 0.000050  loss: 2.7105 (2.6386)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [420/780]  eta: 0:01:41  lr: 0.000050  loss: 2.7956 (2.6409)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [430/780]  eta: 0:01:38  lr: 0.000050  loss: 2.6653 (2.6392)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [440/780]  eta: 0:01:35  lr: 0.000050  loss: 2.5869 (2.6396)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [450/780]  eta: 0:01:33  lr: 0.000050  loss: 2.5719 (2.6349)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [460/780]  eta: 0:01:30  lr: 0.000050  loss: 2.6515 (2.6348)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [470/780]  eta: 0:01:27  lr: 0.000050  loss: 2.6694 (2.6342)  time: 0.2785  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [480/780]  eta: 0:01:24  lr: 0.000050  loss: 2.6131 (2.6333)  time: 0.2782  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [490/780]  eta: 0:01:21  lr: 0.000050  loss: 2.6116 (2.6329)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [500/780]  eta: 0:01:18  lr: 0.000050  loss: 2.6938 (2.6337)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [510/780]  eta: 0:01:16  lr: 0.000050  loss: 2.6695 (2.6349)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [520/780]  eta: 0:01:13  lr: 0.000050  loss: 2.6433 (2.6314)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [530/780]  eta: 0:01:10  lr: 0.000050  loss: 2.5949 (2.6313)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [540/780]  eta: 0:01:07  lr: 0.000050  loss: 2.7380 (2.6345)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [550/780]  eta: 0:01:04  lr: 0.000050  loss: 2.8385 (2.6354)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [560/780]  eta: 0:01:01  lr: 0.000050  loss: 2.6619 (2.6313)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [570/780]  eta: 0:00:59  lr: 0.000050  loss: 2.6626 (2.6359)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [580/780]  eta: 0:00:56  lr: 0.000050  loss: 2.6626 (2.6355)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [590/780]  eta: 0:00:53  lr: 0.000050  loss: 2.4683 (2.6305)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [600/780]  eta: 0:00:50  lr: 0.000050  loss: 2.6418 (2.6310)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [610/780]  eta: 0:00:47  lr: 0.000050  loss: 2.7139 (2.6345)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [620/780]  eta: 0:00:45  lr: 0.000050  loss: 2.7085 (2.6319)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [630/780]  eta: 0:00:42  lr: 0.000050  loss: 2.7017 (2.6334)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [640/780]  eta: 0:00:39  lr: 0.000050  loss: 2.7529 (2.6330)  time: 0.2785  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [650/780]  eta: 0:00:36  lr: 0.000050  loss: 2.7919 (2.6348)  time: 0.2784  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [660/780]  eta: 0:00:33  lr: 0.000050  loss: 2.6733 (2.6339)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [670/780]  eta: 0:00:30  lr: 0.000050  loss: 2.6296 (2.6333)  time: 0.2784  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [680/780]  eta: 0:00:28  lr: 0.000050  loss: 2.9232 (2.6358)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [690/780]  eta: 0:00:25  lr: 0.000050  loss: 2.9232 (2.6368)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [700/780]  eta: 0:00:22  lr: 0.000050  loss: 2.5699 (2.6359)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [710/780]  eta: 0:00:19  lr: 0.000050  loss: 2.5443 (2.6340)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [720/780]  eta: 0:00:16  lr: 0.000050  loss: 2.6099 (2.6356)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [730/780]  eta: 0:00:14  lr: 0.000050  loss: 2.5659 (2.6342)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [740/780]  eta: 0:00:11  lr: 0.000050  loss: 2.6744 (2.6371)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [750/780]  eta: 0:00:08  lr: 0.000050  loss: 2.7593 (2.6376)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [760/780]  eta: 0:00:05  lr: 0.000050  loss: 2.7275 (2.6407)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [770/780]  eta: 0:00:02  lr: 0.000050  loss: 2.7241 (2.6400)  time: 0.2784  data: 0.0002  max mem: 8734\n",
            "Epoch: [5]  [779/780]  eta: 0:00:00  lr: 0.000050  loss: 2.6164 (2.6384)  time: 0.2772  data: 0.0001  max mem: 8734\n",
            "Epoch: [5] Total time: 0:03:39 (0.2810 s / it)\n",
            "Averaged stats: lr: 0.000050  loss: 2.6164 (2.6384)\n",
            "Test:  [  0/105]  eta: 0:01:55  loss: 0.5476 (0.5476)  acc1: 86.4583 (86.4583)  acc5: 100.0000 (100.0000)  time: 1.0999  data: 0.9535  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:21  loss: 0.6459 (0.6401)  acc1: 85.4167 (85.6061)  acc5: 97.9167 (98.1061)  time: 0.2235  data: 0.0869  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:15  loss: 0.6459 (0.6340)  acc1: 85.4167 (85.6647)  acc5: 97.9167 (98.1151)  time: 0.1339  data: 0.0003  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 0.6496 (0.6591)  acc1: 84.3750 (84.6102)  acc5: 97.9167 (97.9839)  time: 0.1327  data: 0.0002  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 0.6766 (0.6531)  acc1: 84.3750 (84.5274)  acc5: 97.9167 (98.0183)  time: 0.1333  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 0.6262 (0.6581)  acc1: 84.3750 (84.3342)  acc5: 97.9167 (98.0392)  time: 0.1333  data: 0.0002  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 0.5962 (0.6499)  acc1: 84.3750 (84.5287)  acc5: 98.9583 (98.1557)  time: 0.1333  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 0.5954 (0.6496)  acc1: 83.3333 (84.3457)  acc5: 98.9583 (98.1367)  time: 0.1328  data: 0.0002  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 0.6340 (0.6470)  acc1: 82.2917 (84.3750)  acc5: 97.9167 (98.0839)  time: 0.1322  data: 0.0002  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 0.5811 (0.6424)  acc1: 85.4167 (84.5353)  acc5: 97.9167 (98.1571)  time: 0.1323  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.6070 (0.6401)  acc1: 85.4167 (84.7153)  acc5: 97.9167 (98.1642)  time: 0.1317  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.6070 (0.6392)  acc1: 85.4167 (84.6700)  acc5: 98.9583 (98.1900)  time: 0.1257  data: 0.0002  max mem: 8734\n",
            "Test: Total time: 0:00:14 (0.1422 s / it)\n",
            "* Acc@1 84.670 Acc@5 98.190 loss 0.639\n",
            "Accuracy of the network on the 10000 test images: 84.7%\n",
            "Max accuracy: 84.67%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [6]  [  0/780]  eta: 0:18:07  lr: 0.000046  loss: 2.3933 (2.3933)  time: 1.3949  data: 1.0764  max mem: 8734\n",
            "Epoch: [6]  [ 10/780]  eta: 0:04:52  lr: 0.000046  loss: 2.6748 (2.6795)  time: 0.3802  data: 0.0980  max mem: 8734\n",
            "Epoch: [6]  [ 20/780]  eta: 0:04:12  lr: 0.000046  loss: 2.7109 (2.7248)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [ 30/780]  eta: 0:03:56  lr: 0.000046  loss: 2.6759 (2.6638)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [ 40/780]  eta: 0:03:46  lr: 0.000046  loss: 2.7030 (2.6664)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [ 50/780]  eta: 0:03:40  lr: 0.000046  loss: 2.7030 (2.6232)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [ 60/780]  eta: 0:03:34  lr: 0.000046  loss: 2.6869 (2.6181)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [ 70/780]  eta: 0:03:29  lr: 0.000046  loss: 2.6869 (2.6195)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [ 80/780]  eta: 0:03:25  lr: 0.000046  loss: 2.6587 (2.6069)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [ 90/780]  eta: 0:03:21  lr: 0.000046  loss: 2.4499 (2.5875)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [100/780]  eta: 0:03:17  lr: 0.000046  loss: 2.5632 (2.5815)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [110/780]  eta: 0:03:13  lr: 0.000046  loss: 2.6678 (2.5823)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [120/780]  eta: 0:03:10  lr: 0.000046  loss: 2.6766 (2.5861)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [130/780]  eta: 0:03:07  lr: 0.000046  loss: 2.6310 (2.5815)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [140/780]  eta: 0:03:03  lr: 0.000046  loss: 2.5711 (2.5776)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [150/780]  eta: 0:03:00  lr: 0.000046  loss: 2.5711 (2.5737)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [160/780]  eta: 0:02:57  lr: 0.000046  loss: 2.7006 (2.5813)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [170/780]  eta: 0:02:54  lr: 0.000046  loss: 2.7006 (2.5810)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [180/780]  eta: 0:02:51  lr: 0.000046  loss: 2.4039 (2.5681)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [190/780]  eta: 0:02:48  lr: 0.000046  loss: 2.2814 (2.5588)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [200/780]  eta: 0:02:45  lr: 0.000046  loss: 2.3790 (2.5563)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [210/780]  eta: 0:02:42  lr: 0.000046  loss: 2.6725 (2.5583)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [220/780]  eta: 0:02:39  lr: 0.000046  loss: 2.6722 (2.5541)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [230/780]  eta: 0:02:36  lr: 0.000046  loss: 2.6287 (2.5549)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [240/780]  eta: 0:02:33  lr: 0.000046  loss: 2.6287 (2.5542)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [250/780]  eta: 0:02:30  lr: 0.000046  loss: 2.4461 (2.5500)  time: 0.2813  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [260/780]  eta: 0:02:27  lr: 0.000046  loss: 2.7072 (2.5579)  time: 0.2822  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [270/780]  eta: 0:02:24  lr: 0.000046  loss: 2.6814 (2.5585)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [280/780]  eta: 0:02:21  lr: 0.000046  loss: 2.6557 (2.5564)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [290/780]  eta: 0:02:18  lr: 0.000046  loss: 2.4697 (2.5490)  time: 0.2810  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [300/780]  eta: 0:02:16  lr: 0.000046  loss: 2.7707 (2.5594)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [310/780]  eta: 0:02:13  lr: 0.000046  loss: 2.7717 (2.5600)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [320/780]  eta: 0:02:10  lr: 0.000046  loss: 2.5757 (2.5616)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [330/780]  eta: 0:02:07  lr: 0.000046  loss: 2.4879 (2.5614)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [340/780]  eta: 0:02:04  lr: 0.000046  loss: 2.6264 (2.5598)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [350/780]  eta: 0:02:01  lr: 0.000046  loss: 2.6264 (2.5565)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [360/780]  eta: 0:01:58  lr: 0.000046  loss: 2.5871 (2.5554)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [370/780]  eta: 0:01:55  lr: 0.000046  loss: 2.5494 (2.5538)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [380/780]  eta: 0:01:53  lr: 0.000046  loss: 2.5644 (2.5566)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [390/780]  eta: 0:01:50  lr: 0.000046  loss: 2.4584 (2.5493)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [400/780]  eta: 0:01:47  lr: 0.000046  loss: 2.2508 (2.5485)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [410/780]  eta: 0:01:44  lr: 0.000046  loss: 2.6181 (2.5485)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [420/780]  eta: 0:01:41  lr: 0.000046  loss: 2.3039 (2.5397)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [430/780]  eta: 0:01:38  lr: 0.000046  loss: 2.4175 (2.5424)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [440/780]  eta: 0:01:35  lr: 0.000046  loss: 2.6089 (2.5415)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [450/780]  eta: 0:01:33  lr: 0.000046  loss: 2.6263 (2.5447)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [460/780]  eta: 0:01:30  lr: 0.000046  loss: 2.6543 (2.5444)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [470/780]  eta: 0:01:27  lr: 0.000046  loss: 2.5254 (2.5428)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [480/780]  eta: 0:01:24  lr: 0.000046  loss: 2.5585 (2.5447)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [490/780]  eta: 0:01:21  lr: 0.000046  loss: 2.6044 (2.5438)  time: 0.2832  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [500/780]  eta: 0:01:18  lr: 0.000046  loss: 2.5979 (2.5448)  time: 0.2841  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [510/780]  eta: 0:01:16  lr: 0.000046  loss: 2.4956 (2.5405)  time: 0.2819  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [520/780]  eta: 0:01:13  lr: 0.000046  loss: 2.4127 (2.5397)  time: 0.2827  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [530/780]  eta: 0:01:10  lr: 0.000046  loss: 2.6773 (2.5442)  time: 0.2832  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [540/780]  eta: 0:01:07  lr: 0.000046  loss: 2.6963 (2.5443)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [550/780]  eta: 0:01:04  lr: 0.000046  loss: 2.5523 (2.5423)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [560/780]  eta: 0:01:02  lr: 0.000046  loss: 2.7421 (2.5459)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [570/780]  eta: 0:00:59  lr: 0.000046  loss: 2.7506 (2.5456)  time: 0.2784  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [580/780]  eta: 0:00:56  lr: 0.000046  loss: 2.5475 (2.5446)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [590/780]  eta: 0:00:53  lr: 0.000046  loss: 2.5475 (2.5454)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [600/780]  eta: 0:00:50  lr: 0.000046  loss: 2.5791 (2.5450)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [610/780]  eta: 0:00:47  lr: 0.000046  loss: 2.5791 (2.5460)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [620/780]  eta: 0:00:45  lr: 0.000046  loss: 2.7786 (2.5474)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [630/780]  eta: 0:00:42  lr: 0.000046  loss: 2.7479 (2.5488)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [640/780]  eta: 0:00:39  lr: 0.000046  loss: 2.5855 (2.5439)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [650/780]  eta: 0:00:36  lr: 0.000046  loss: 2.3735 (2.5442)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [660/780]  eta: 0:00:33  lr: 0.000046  loss: 2.6972 (2.5441)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [670/780]  eta: 0:00:30  lr: 0.000046  loss: 2.6281 (2.5450)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [680/780]  eta: 0:00:28  lr: 0.000046  loss: 2.5785 (2.5424)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [690/780]  eta: 0:00:25  lr: 0.000046  loss: 2.4023 (2.5406)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [700/780]  eta: 0:00:22  lr: 0.000046  loss: 2.4023 (2.5392)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [710/780]  eta: 0:00:19  lr: 0.000046  loss: 2.4237 (2.5372)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [720/780]  eta: 0:00:16  lr: 0.000046  loss: 2.6668 (2.5403)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [730/780]  eta: 0:00:14  lr: 0.000046  loss: 2.5825 (2.5388)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [740/780]  eta: 0:00:11  lr: 0.000046  loss: 2.4654 (2.5385)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [750/780]  eta: 0:00:08  lr: 0.000046  loss: 2.4654 (2.5359)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [760/780]  eta: 0:00:05  lr: 0.000046  loss: 2.4579 (2.5359)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [770/780]  eta: 0:00:02  lr: 0.000046  loss: 2.4833 (2.5347)  time: 0.2782  data: 0.0002  max mem: 8734\n",
            "Epoch: [6]  [779/780]  eta: 0:00:00  lr: 0.000046  loss: 2.5908 (2.5325)  time: 0.2774  data: 0.0002  max mem: 8734\n",
            "Epoch: [6] Total time: 0:03:39 (0.2814 s / it)\n",
            "Averaged stats: lr: 0.000046  loss: 2.5908 (2.5325)\n",
            "Test:  [  0/105]  eta: 0:01:52  loss: 0.4934 (0.4934)  acc1: 89.5833 (89.5833)  acc5: 100.0000 (100.0000)  time: 1.0714  data: 0.9124  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:21  loss: 0.6190 (0.6316)  acc1: 86.4583 (86.7424)  acc5: 98.9583 (98.3902)  time: 0.2280  data: 0.0916  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:15  loss: 0.6130 (0.6289)  acc1: 85.4167 (86.0615)  acc5: 98.9583 (98.6111)  time: 0.1385  data: 0.0049  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 0.6842 (0.6550)  acc1: 83.3333 (84.9798)  acc5: 97.9167 (98.3535)  time: 0.1334  data: 0.0002  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 0.6963 (0.6526)  acc1: 82.2917 (84.8069)  acc5: 97.9167 (98.3232)  time: 0.1323  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 0.6590 (0.6583)  acc1: 83.3333 (84.5384)  acc5: 97.9167 (98.3252)  time: 0.1317  data: 0.0002  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 0.6147 (0.6515)  acc1: 84.3750 (84.7678)  acc5: 98.9583 (98.3948)  time: 0.1320  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 0.6306 (0.6548)  acc1: 84.3750 (84.7124)  acc5: 97.9167 (98.2688)  time: 0.1320  data: 0.0002  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 0.6337 (0.6526)  acc1: 84.3750 (84.8251)  acc5: 97.9167 (98.2125)  time: 0.1320  data: 0.0002  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 0.5822 (0.6469)  acc1: 86.4583 (84.9359)  acc5: 98.9583 (98.2944)  time: 0.1313  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.6026 (0.6454)  acc1: 85.4167 (84.9422)  acc5: 98.9583 (98.2673)  time: 0.1310  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.6026 (0.6458)  acc1: 85.4167 (84.9100)  acc5: 98.9583 (98.2900)  time: 0.1257  data: 0.0001  max mem: 8734\n",
            "Test: Total time: 0:00:14 (0.1421 s / it)\n",
            "* Acc@1 84.910 Acc@5 98.290 loss 0.646\n",
            "Accuracy of the network on the 10000 test images: 84.9%\n",
            "Max accuracy: 84.91%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [7]  [  0/780]  eta: 0:19:03  lr: 0.000039  loss: 2.7185 (2.7185)  time: 1.4664  data: 1.1349  max mem: 8734\n",
            "Epoch: [7]  [ 10/780]  eta: 0:04:57  lr: 0.000039  loss: 2.7185 (2.6858)  time: 0.3868  data: 0.1033  max mem: 8734\n",
            "Epoch: [7]  [ 20/780]  eta: 0:04:15  lr: 0.000039  loss: 2.6712 (2.6036)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [ 30/780]  eta: 0:03:58  lr: 0.000039  loss: 2.5476 (2.5846)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [ 40/780]  eta: 0:03:48  lr: 0.000039  loss: 2.4184 (2.5339)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [ 50/780]  eta: 0:03:40  lr: 0.000039  loss: 2.5940 (2.5382)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [ 60/780]  eta: 0:03:35  lr: 0.000039  loss: 2.6078 (2.5184)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [ 70/780]  eta: 0:03:30  lr: 0.000039  loss: 2.3916 (2.5069)  time: 0.2809  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [ 80/780]  eta: 0:03:26  lr: 0.000039  loss: 2.4148 (2.4958)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [ 90/780]  eta: 0:03:21  lr: 0.000039  loss: 2.3872 (2.4598)  time: 0.2793  data: 0.0004  max mem: 8734\n",
            "Epoch: [7]  [100/780]  eta: 0:03:18  lr: 0.000039  loss: 2.3872 (2.4531)  time: 0.2796  data: 0.0004  max mem: 8734\n",
            "Epoch: [7]  [110/780]  eta: 0:03:14  lr: 0.000039  loss: 2.5253 (2.4660)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [120/780]  eta: 0:03:11  lr: 0.000039  loss: 2.6076 (2.4598)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [130/780]  eta: 0:03:07  lr: 0.000039  loss: 2.5769 (2.4679)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [140/780]  eta: 0:03:04  lr: 0.000039  loss: 2.5594 (2.4624)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [150/780]  eta: 0:03:01  lr: 0.000039  loss: 2.5399 (2.4627)  time: 0.2830  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [160/780]  eta: 0:02:58  lr: 0.000039  loss: 2.4804 (2.4600)  time: 0.2827  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [170/780]  eta: 0:02:55  lr: 0.000039  loss: 2.5138 (2.4708)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [180/780]  eta: 0:02:51  lr: 0.000039  loss: 2.5918 (2.4747)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [190/780]  eta: 0:02:48  lr: 0.000039  loss: 2.4823 (2.4618)  time: 0.2783  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [200/780]  eta: 0:02:45  lr: 0.000039  loss: 2.4298 (2.4600)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [210/780]  eta: 0:02:42  lr: 0.000039  loss: 2.4545 (2.4600)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [220/780]  eta: 0:02:39  lr: 0.000039  loss: 2.5906 (2.4632)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [230/780]  eta: 0:02:36  lr: 0.000039  loss: 2.5939 (2.4679)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [240/780]  eta: 0:02:33  lr: 0.000039  loss: 2.6662 (2.4665)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [250/780]  eta: 0:02:30  lr: 0.000039  loss: 2.4464 (2.4591)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [260/780]  eta: 0:02:27  lr: 0.000039  loss: 2.2482 (2.4569)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [270/780]  eta: 0:02:24  lr: 0.000039  loss: 2.3184 (2.4485)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [280/780]  eta: 0:02:21  lr: 0.000039  loss: 2.3184 (2.4497)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [290/780]  eta: 0:02:19  lr: 0.000039  loss: 2.3683 (2.4491)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [300/780]  eta: 0:02:16  lr: 0.000039  loss: 2.5252 (2.4544)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [310/780]  eta: 0:02:13  lr: 0.000039  loss: 2.5252 (2.4557)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [320/780]  eta: 0:02:10  lr: 0.000039  loss: 2.3556 (2.4493)  time: 0.2811  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [330/780]  eta: 0:02:07  lr: 0.000039  loss: 2.4451 (2.4498)  time: 0.2811  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [340/780]  eta: 0:02:04  lr: 0.000039  loss: 2.4792 (2.4481)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [350/780]  eta: 0:02:01  lr: 0.000039  loss: 2.4792 (2.4508)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [360/780]  eta: 0:01:58  lr: 0.000039  loss: 2.4256 (2.4490)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [370/780]  eta: 0:01:56  lr: 0.000039  loss: 2.5431 (2.4534)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [380/780]  eta: 0:01:53  lr: 0.000039  loss: 2.5375 (2.4504)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [390/780]  eta: 0:01:50  lr: 0.000039  loss: 2.3734 (2.4488)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [400/780]  eta: 0:01:47  lr: 0.000039  loss: 2.4790 (2.4513)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [410/780]  eta: 0:01:44  lr: 0.000039  loss: 2.7197 (2.4593)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [420/780]  eta: 0:01:41  lr: 0.000039  loss: 2.7279 (2.4628)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [430/780]  eta: 0:01:38  lr: 0.000039  loss: 2.5345 (2.4644)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [440/780]  eta: 0:01:36  lr: 0.000039  loss: 2.2668 (2.4584)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [450/780]  eta: 0:01:33  lr: 0.000039  loss: 2.2668 (2.4616)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [460/780]  eta: 0:01:30  lr: 0.000039  loss: 2.5407 (2.4612)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [470/780]  eta: 0:01:27  lr: 0.000039  loss: 2.5440 (2.4606)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [480/780]  eta: 0:01:24  lr: 0.000039  loss: 2.5886 (2.4633)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [490/780]  eta: 0:01:21  lr: 0.000039  loss: 2.5708 (2.4630)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [500/780]  eta: 0:01:19  lr: 0.000039  loss: 2.3517 (2.4584)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [510/780]  eta: 0:01:16  lr: 0.000039  loss: 2.2919 (2.4590)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [520/780]  eta: 0:01:13  lr: 0.000039  loss: 2.5298 (2.4622)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [530/780]  eta: 0:01:10  lr: 0.000039  loss: 2.4830 (2.4617)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [540/780]  eta: 0:01:07  lr: 0.000039  loss: 2.4377 (2.4613)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [550/780]  eta: 0:01:04  lr: 0.000039  loss: 2.4472 (2.4610)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [560/780]  eta: 0:01:02  lr: 0.000039  loss: 2.4359 (2.4592)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [570/780]  eta: 0:00:59  lr: 0.000039  loss: 2.4192 (2.4571)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [580/780]  eta: 0:00:56  lr: 0.000039  loss: 2.3403 (2.4552)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [590/780]  eta: 0:00:53  lr: 0.000039  loss: 2.3597 (2.4553)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [600/780]  eta: 0:00:50  lr: 0.000039  loss: 2.3754 (2.4551)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [610/780]  eta: 0:00:47  lr: 0.000039  loss: 2.6166 (2.4584)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [620/780]  eta: 0:00:45  lr: 0.000039  loss: 2.6178 (2.4591)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [630/780]  eta: 0:00:42  lr: 0.000039  loss: 2.6178 (2.4620)  time: 0.2797  data: 0.0004  max mem: 8734\n",
            "Epoch: [7]  [640/780]  eta: 0:00:39  lr: 0.000039  loss: 2.4586 (2.4580)  time: 0.2795  data: 0.0004  max mem: 8734\n",
            "Epoch: [7]  [650/780]  eta: 0:00:36  lr: 0.000039  loss: 2.2676 (2.4556)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [660/780]  eta: 0:00:33  lr: 0.000039  loss: 2.3272 (2.4546)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [670/780]  eta: 0:00:30  lr: 0.000039  loss: 2.3913 (2.4527)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [680/780]  eta: 0:00:28  lr: 0.000039  loss: 2.1690 (2.4497)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [690/780]  eta: 0:00:25  lr: 0.000039  loss: 2.1600 (2.4472)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [700/780]  eta: 0:00:22  lr: 0.000039  loss: 2.3972 (2.4464)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [710/780]  eta: 0:00:19  lr: 0.000039  loss: 2.3656 (2.4463)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [720/780]  eta: 0:00:16  lr: 0.000039  loss: 2.5862 (2.4494)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [730/780]  eta: 0:00:14  lr: 0.000039  loss: 2.4881 (2.4486)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [740/780]  eta: 0:00:11  lr: 0.000039  loss: 2.4776 (2.4492)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [750/780]  eta: 0:00:08  lr: 0.000039  loss: 2.3152 (2.4465)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [760/780]  eta: 0:00:05  lr: 0.000039  loss: 2.2700 (2.4447)  time: 0.2784  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [770/780]  eta: 0:00:02  lr: 0.000039  loss: 2.3254 (2.4436)  time: 0.2775  data: 0.0002  max mem: 8734\n",
            "Epoch: [7]  [779/780]  eta: 0:00:00  lr: 0.000039  loss: 2.4519 (2.4435)  time: 0.2769  data: 0.0002  max mem: 8734\n",
            "Epoch: [7] Total time: 0:03:39 (0.2813 s / it)\n",
            "Averaged stats: lr: 0.000039  loss: 2.4519 (2.4435)\n",
            "Test:  [  0/105]  eta: 0:01:58  loss: 0.4308 (0.4308)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.1273  data: 0.9733  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:21  loss: 0.5779 (0.5680)  acc1: 86.4583 (87.9735)  acc5: 97.9167 (98.3902)  time: 0.2241  data: 0.0887  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:15  loss: 0.5689 (0.5628)  acc1: 86.4583 (87.4008)  acc5: 98.9583 (98.7103)  time: 0.1333  data: 0.0002  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 0.5818 (0.5956)  acc1: 85.4167 (86.2903)  acc5: 97.9167 (98.3199)  time: 0.1334  data: 0.0002  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 0.6018 (0.5919)  acc1: 85.4167 (86.1789)  acc5: 97.9167 (98.3486)  time: 0.1332  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 0.5874 (0.5970)  acc1: 85.4167 (85.9886)  acc5: 98.9583 (98.3864)  time: 0.1322  data: 0.0002  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 0.5664 (0.5901)  acc1: 86.4583 (86.2363)  acc5: 98.9583 (98.4973)  time: 0.1316  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 0.5686 (0.5935)  acc1: 85.4167 (86.0769)  acc5: 98.9583 (98.4302)  time: 0.1310  data: 0.0002  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 0.5697 (0.5923)  acc1: 84.3750 (86.0468)  acc5: 97.9167 (98.3796)  time: 0.1310  data: 0.0002  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 0.5620 (0.5885)  acc1: 85.4167 (86.1035)  acc5: 98.9583 (98.4318)  time: 0.1310  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.5721 (0.5887)  acc1: 85.4167 (86.0458)  acc5: 98.9583 (98.3911)  time: 0.1310  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.5825 (0.5892)  acc1: 85.4167 (86.0100)  acc5: 98.9583 (98.4200)  time: 0.1256  data: 0.0002  max mem: 8734\n",
            "Test: Total time: 0:00:14 (0.1416 s / it)\n",
            "* Acc@1 86.010 Acc@5 98.420 loss 0.589\n",
            "Accuracy of the network on the 10000 test images: 86.0%\n",
            "Max accuracy: 86.01%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [8]  [  0/780]  eta: 0:17:32  lr: 0.000033  loss: 2.5425 (2.5425)  time: 1.3497  data: 0.9774  max mem: 8734\n",
            "Epoch: [8]  [ 10/780]  eta: 0:04:50  lr: 0.000033  loss: 2.6144 (2.6427)  time: 0.3778  data: 0.0902  max mem: 8734\n",
            "Epoch: [8]  [ 20/780]  eta: 0:04:11  lr: 0.000033  loss: 2.5883 (2.5465)  time: 0.2799  data: 0.0008  max mem: 8734\n",
            "Epoch: [8]  [ 30/780]  eta: 0:03:56  lr: 0.000033  loss: 2.6088 (2.6042)  time: 0.2812  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [ 40/780]  eta: 0:03:47  lr: 0.000033  loss: 2.6088 (2.5559)  time: 0.2817  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [ 50/780]  eta: 0:03:40  lr: 0.000033  loss: 2.4991 (2.5406)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [ 60/780]  eta: 0:03:34  lr: 0.000033  loss: 2.4970 (2.5328)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [ 70/780]  eta: 0:03:29  lr: 0.000033  loss: 2.6419 (2.5590)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [ 80/780]  eta: 0:03:25  lr: 0.000033  loss: 2.6739 (2.5415)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [ 90/780]  eta: 0:03:21  lr: 0.000033  loss: 2.5251 (2.5288)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [100/780]  eta: 0:03:17  lr: 0.000033  loss: 2.4565 (2.5176)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [110/780]  eta: 0:03:13  lr: 0.000033  loss: 2.4313 (2.4989)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [120/780]  eta: 0:03:10  lr: 0.000033  loss: 2.4476 (2.4920)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [130/780]  eta: 0:03:07  lr: 0.000033  loss: 2.6789 (2.4912)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [140/780]  eta: 0:03:03  lr: 0.000033  loss: 2.6292 (2.4908)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [150/780]  eta: 0:03:00  lr: 0.000033  loss: 2.5081 (2.4855)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [160/780]  eta: 0:02:57  lr: 0.000033  loss: 2.5081 (2.4840)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [170/780]  eta: 0:02:54  lr: 0.000033  loss: 2.5181 (2.4765)  time: 0.2783  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [180/780]  eta: 0:02:51  lr: 0.000033  loss: 2.5454 (2.4835)  time: 0.2781  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [190/780]  eta: 0:02:48  lr: 0.000033  loss: 2.5272 (2.4698)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [200/780]  eta: 0:02:45  lr: 0.000033  loss: 2.3389 (2.4615)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [210/780]  eta: 0:02:42  lr: 0.000033  loss: 2.3389 (2.4530)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [220/780]  eta: 0:02:39  lr: 0.000033  loss: 2.4207 (2.4486)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [230/780]  eta: 0:02:36  lr: 0.000033  loss: 2.4207 (2.4421)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [240/780]  eta: 0:02:33  lr: 0.000033  loss: 2.0391 (2.4271)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [250/780]  eta: 0:02:30  lr: 0.000033  loss: 2.2512 (2.4233)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [260/780]  eta: 0:02:27  lr: 0.000033  loss: 2.3147 (2.4163)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [270/780]  eta: 0:02:24  lr: 0.000033  loss: 2.4295 (2.4190)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [280/780]  eta: 0:02:21  lr: 0.000033  loss: 2.4236 (2.4139)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [290/780]  eta: 0:02:18  lr: 0.000033  loss: 2.3076 (2.4141)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [300/780]  eta: 0:02:15  lr: 0.000033  loss: 2.4701 (2.4160)  time: 0.2799  data: 0.0006  max mem: 8734\n",
            "Epoch: [8]  [310/780]  eta: 0:02:13  lr: 0.000033  loss: 2.6071 (2.4202)  time: 0.2801  data: 0.0006  max mem: 8734\n",
            "Epoch: [8]  [320/780]  eta: 0:02:10  lr: 0.000033  loss: 2.4176 (2.4156)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [330/780]  eta: 0:02:07  lr: 0.000033  loss: 2.3707 (2.4205)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [340/780]  eta: 0:02:04  lr: 0.000033  loss: 2.3539 (2.4131)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [350/780]  eta: 0:02:01  lr: 0.000033  loss: 2.4621 (2.4169)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [360/780]  eta: 0:01:58  lr: 0.000033  loss: 2.4961 (2.4159)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [370/780]  eta: 0:01:55  lr: 0.000033  loss: 2.3269 (2.4062)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [380/780]  eta: 0:01:53  lr: 0.000033  loss: 2.2053 (2.4086)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [390/780]  eta: 0:01:50  lr: 0.000033  loss: 2.4563 (2.4050)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [400/780]  eta: 0:01:47  lr: 0.000033  loss: 2.4696 (2.4055)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [410/780]  eta: 0:01:44  lr: 0.000033  loss: 2.4696 (2.4068)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [420/780]  eta: 0:01:41  lr: 0.000033  loss: 2.3250 (2.4054)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [430/780]  eta: 0:01:38  lr: 0.000033  loss: 2.2679 (2.4013)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [440/780]  eta: 0:01:35  lr: 0.000033  loss: 2.2679 (2.3984)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [450/780]  eta: 0:01:33  lr: 0.000033  loss: 2.2996 (2.3942)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [460/780]  eta: 0:01:30  lr: 0.000033  loss: 2.2996 (2.3946)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [470/780]  eta: 0:01:27  lr: 0.000033  loss: 2.4472 (2.3960)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [480/780]  eta: 0:01:24  lr: 0.000033  loss: 2.4450 (2.3982)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [490/780]  eta: 0:01:21  lr: 0.000033  loss: 2.4260 (2.3972)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [500/780]  eta: 0:01:18  lr: 0.000033  loss: 2.2967 (2.3941)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [510/780]  eta: 0:01:16  lr: 0.000033  loss: 2.1371 (2.3873)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [520/780]  eta: 0:01:13  lr: 0.000033  loss: 2.0827 (2.3829)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [530/780]  eta: 0:01:10  lr: 0.000033  loss: 2.2807 (2.3804)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [540/780]  eta: 0:01:07  lr: 0.000033  loss: 2.3641 (2.3828)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [550/780]  eta: 0:01:04  lr: 0.000033  loss: 2.4201 (2.3835)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [560/780]  eta: 0:01:01  lr: 0.000033  loss: 2.4620 (2.3844)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [570/780]  eta: 0:00:59  lr: 0.000033  loss: 2.5520 (2.3836)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [580/780]  eta: 0:00:56  lr: 0.000033  loss: 2.5003 (2.3816)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [590/780]  eta: 0:00:53  lr: 0.000033  loss: 2.4099 (2.3824)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [600/780]  eta: 0:00:50  lr: 0.000033  loss: 2.3893 (2.3802)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [610/780]  eta: 0:00:47  lr: 0.000033  loss: 2.1118 (2.3788)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [620/780]  eta: 0:00:45  lr: 0.000033  loss: 2.2823 (2.3783)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [630/780]  eta: 0:00:42  lr: 0.000033  loss: 2.2781 (2.3759)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [640/780]  eta: 0:00:39  lr: 0.000033  loss: 2.2462 (2.3743)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [650/780]  eta: 0:00:36  lr: 0.000033  loss: 2.3501 (2.3755)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [660/780]  eta: 0:00:33  lr: 0.000033  loss: 2.4318 (2.3739)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [670/780]  eta: 0:00:30  lr: 0.000033  loss: 2.3941 (2.3707)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [680/780]  eta: 0:00:28  lr: 0.000033  loss: 2.2917 (2.3700)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [690/780]  eta: 0:00:25  lr: 0.000033  loss: 2.2984 (2.3693)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [700/780]  eta: 0:00:22  lr: 0.000033  loss: 2.3052 (2.3683)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [710/780]  eta: 0:00:19  lr: 0.000033  loss: 2.3052 (2.3665)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [720/780]  eta: 0:00:16  lr: 0.000033  loss: 2.2156 (2.3638)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [730/780]  eta: 0:00:14  lr: 0.000033  loss: 2.2150 (2.3640)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [740/780]  eta: 0:00:11  lr: 0.000033  loss: 2.4715 (2.3646)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [750/780]  eta: 0:00:08  lr: 0.000033  loss: 2.4351 (2.3632)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [760/780]  eta: 0:00:05  lr: 0.000033  loss: 2.3814 (2.3621)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [770/780]  eta: 0:00:02  lr: 0.000033  loss: 2.1329 (2.3585)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [8]  [779/780]  eta: 0:00:00  lr: 0.000033  loss: 2.2265 (2.3578)  time: 0.2771  data: 0.0001  max mem: 8734\n",
            "Epoch: [8] Total time: 0:03:39 (0.2811 s / it)\n",
            "Averaged stats: lr: 0.000033  loss: 2.2265 (2.3578)\n",
            "Test:  [  0/105]  eta: 0:02:07  loss: 0.4433 (0.4433)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 1.2183  data: 1.0673  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:22  loss: 0.5836 (0.5714)  acc1: 86.4583 (87.5000)  acc5: 97.9167 (98.2008)  time: 0.2325  data: 0.0985  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:15  loss: 0.5402 (0.5572)  acc1: 85.4167 (87.2024)  acc5: 97.9167 (98.3135)  time: 0.1337  data: 0.0009  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 0.5841 (0.5941)  acc1: 85.4167 (86.2903)  acc5: 97.9167 (98.0847)  time: 0.1333  data: 0.0002  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 0.6247 (0.5977)  acc1: 84.3750 (85.9248)  acc5: 97.9167 (98.1961)  time: 0.1329  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 0.6054 (0.5929)  acc1: 85.4167 (86.1315)  acc5: 98.9583 (98.3864)  time: 0.1327  data: 0.0006  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 0.5501 (0.5845)  acc1: 87.5000 (86.5608)  acc5: 98.9583 (98.5314)  time: 0.1325  data: 0.0006  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 0.5583 (0.5864)  acc1: 87.5000 (86.4143)  acc5: 98.9583 (98.4595)  time: 0.1322  data: 0.0006  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 0.5821 (0.5865)  acc1: 85.4167 (86.4712)  acc5: 97.9167 (98.4568)  time: 0.1318  data: 0.0005  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 0.5419 (0.5809)  acc1: 86.4583 (86.5728)  acc5: 97.9167 (98.4890)  time: 0.1316  data: 0.0006  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.5697 (0.5820)  acc1: 86.4583 (86.4996)  acc5: 98.9583 (98.4323)  time: 0.1314  data: 0.0006  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.5697 (0.5819)  acc1: 86.4583 (86.4800)  acc5: 98.9583 (98.4300)  time: 0.1261  data: 0.0005  max mem: 8734\n",
            "Test: Total time: 0:00:15 (0.1429 s / it)\n",
            "* Acc@1 86.480 Acc@5 98.430 loss 0.582\n",
            "Accuracy of the network on the 10000 test images: 86.5%\n",
            "Max accuracy: 86.48%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [9]  [  0/780]  eta: 0:16:34  lr: 0.000027  loss: 2.1275 (2.1275)  time: 1.2748  data: 0.8865  max mem: 8734\n",
            "Epoch: [9]  [ 10/780]  eta: 0:04:46  lr: 0.000027  loss: 2.4153 (2.3679)  time: 0.3719  data: 0.0808  max mem: 8734\n",
            "Epoch: [9]  [ 20/780]  eta: 0:04:08  lr: 0.000027  loss: 2.3738 (2.3309)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [ 30/780]  eta: 0:03:53  lr: 0.000027  loss: 2.4153 (2.3248)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [ 40/780]  eta: 0:03:45  lr: 0.000027  loss: 2.5059 (2.3325)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [ 50/780]  eta: 0:03:38  lr: 0.000027  loss: 2.4264 (2.3620)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [ 60/780]  eta: 0:03:33  lr: 0.000027  loss: 2.3105 (2.2898)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [ 70/780]  eta: 0:03:28  lr: 0.000027  loss: 2.0584 (2.2997)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [ 80/780]  eta: 0:03:24  lr: 0.000027  loss: 2.4736 (2.3260)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [ 90/780]  eta: 0:03:20  lr: 0.000027  loss: 2.1189 (2.2949)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [100/780]  eta: 0:03:16  lr: 0.000027  loss: 2.1130 (2.3023)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [110/780]  eta: 0:03:13  lr: 0.000027  loss: 2.3749 (2.2995)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [120/780]  eta: 0:03:09  lr: 0.000027  loss: 2.3317 (2.2975)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [130/780]  eta: 0:03:06  lr: 0.000027  loss: 2.4371 (2.3076)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [140/780]  eta: 0:03:03  lr: 0.000027  loss: 2.6618 (2.3329)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [150/780]  eta: 0:03:00  lr: 0.000027  loss: 2.6086 (2.3419)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [160/780]  eta: 0:02:57  lr: 0.000027  loss: 2.3902 (2.3298)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [170/780]  eta: 0:02:54  lr: 0.000027  loss: 2.2322 (2.3289)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [180/780]  eta: 0:02:51  lr: 0.000027  loss: 2.3903 (2.3301)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [190/780]  eta: 0:02:47  lr: 0.000027  loss: 2.2369 (2.3235)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [200/780]  eta: 0:02:44  lr: 0.000027  loss: 2.2811 (2.3293)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [210/780]  eta: 0:02:42  lr: 0.000027  loss: 2.2902 (2.3265)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [220/780]  eta: 0:02:39  lr: 0.000027  loss: 2.4529 (2.3376)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [230/780]  eta: 0:02:36  lr: 0.000027  loss: 2.4804 (2.3392)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [240/780]  eta: 0:02:33  lr: 0.000027  loss: 2.3460 (2.3364)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [250/780]  eta: 0:02:30  lr: 0.000027  loss: 2.2730 (2.3362)  time: 0.2814  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [260/780]  eta: 0:02:27  lr: 0.000027  loss: 2.2356 (2.3309)  time: 0.2816  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [270/780]  eta: 0:02:24  lr: 0.000027  loss: 2.2731 (2.3280)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [280/780]  eta: 0:02:21  lr: 0.000027  loss: 2.2272 (2.3260)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [290/780]  eta: 0:02:18  lr: 0.000027  loss: 2.2272 (2.3276)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [300/780]  eta: 0:02:15  lr: 0.000027  loss: 2.2043 (2.3218)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [310/780]  eta: 0:02:12  lr: 0.000027  loss: 2.2100 (2.3220)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [320/780]  eta: 0:02:10  lr: 0.000027  loss: 2.4021 (2.3229)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [330/780]  eta: 0:02:07  lr: 0.000027  loss: 2.4475 (2.3264)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [340/780]  eta: 0:02:04  lr: 0.000027  loss: 2.4203 (2.3257)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [350/780]  eta: 0:02:01  lr: 0.000027  loss: 2.1861 (2.3211)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [360/780]  eta: 0:01:58  lr: 0.000027  loss: 2.1861 (2.3246)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [370/780]  eta: 0:01:55  lr: 0.000027  loss: 2.5267 (2.3253)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [380/780]  eta: 0:01:52  lr: 0.000027  loss: 2.2513 (2.3196)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [390/780]  eta: 0:01:50  lr: 0.000027  loss: 2.1844 (2.3173)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [400/780]  eta: 0:01:47  lr: 0.000027  loss: 2.4088 (2.3175)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [410/780]  eta: 0:01:44  lr: 0.000027  loss: 2.4109 (2.3158)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [420/780]  eta: 0:01:41  lr: 0.000027  loss: 2.2388 (2.3122)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [430/780]  eta: 0:01:38  lr: 0.000027  loss: 2.2256 (2.3057)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [440/780]  eta: 0:01:35  lr: 0.000027  loss: 2.3187 (2.3068)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [450/780]  eta: 0:01:33  lr: 0.000027  loss: 2.5245 (2.3106)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [460/780]  eta: 0:01:30  lr: 0.000027  loss: 2.3460 (2.3105)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [470/780]  eta: 0:01:27  lr: 0.000027  loss: 2.2575 (2.3057)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [480/780]  eta: 0:01:24  lr: 0.000027  loss: 2.1583 (2.3032)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [490/780]  eta: 0:01:21  lr: 0.000027  loss: 2.2134 (2.3023)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [500/780]  eta: 0:01:18  lr: 0.000027  loss: 2.2645 (2.3006)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [510/780]  eta: 0:01:16  lr: 0.000027  loss: 2.3885 (2.3031)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [520/780]  eta: 0:01:13  lr: 0.000027  loss: 2.3071 (2.2978)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [530/780]  eta: 0:01:10  lr: 0.000027  loss: 2.1052 (2.2989)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [540/780]  eta: 0:01:07  lr: 0.000027  loss: 2.2710 (2.2947)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [550/780]  eta: 0:01:04  lr: 0.000027  loss: 1.9767 (2.2910)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [560/780]  eta: 0:01:01  lr: 0.000027  loss: 2.2482 (2.2916)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [570/780]  eta: 0:00:59  lr: 0.000027  loss: 2.3391 (2.2917)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [580/780]  eta: 0:00:56  lr: 0.000027  loss: 2.3391 (2.2912)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [590/780]  eta: 0:00:53  lr: 0.000027  loss: 2.3785 (2.2917)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [600/780]  eta: 0:00:50  lr: 0.000027  loss: 2.4469 (2.2889)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [610/780]  eta: 0:00:47  lr: 0.000027  loss: 2.4425 (2.2924)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [620/780]  eta: 0:00:45  lr: 0.000027  loss: 2.4288 (2.2926)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [630/780]  eta: 0:00:42  lr: 0.000027  loss: 2.3530 (2.2937)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [640/780]  eta: 0:00:39  lr: 0.000027  loss: 2.4293 (2.2940)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [650/780]  eta: 0:00:36  lr: 0.000027  loss: 2.5245 (2.2964)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [660/780]  eta: 0:00:33  lr: 0.000027  loss: 2.3294 (2.2937)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [670/780]  eta: 0:00:30  lr: 0.000027  loss: 2.2899 (2.2958)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [680/780]  eta: 0:00:28  lr: 0.000027  loss: 2.4118 (2.2977)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [690/780]  eta: 0:00:25  lr: 0.000027  loss: 2.2909 (2.2991)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [700/780]  eta: 0:00:22  lr: 0.000027  loss: 2.0687 (2.2913)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [710/780]  eta: 0:00:19  lr: 0.000027  loss: 1.9939 (2.2909)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [720/780]  eta: 0:00:16  lr: 0.000027  loss: 2.2360 (2.2896)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [730/780]  eta: 0:00:14  lr: 0.000027  loss: 2.1985 (2.2879)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [740/780]  eta: 0:00:11  lr: 0.000027  loss: 2.1524 (2.2878)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [750/780]  eta: 0:00:08  lr: 0.000027  loss: 2.1681 (2.2875)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [760/780]  eta: 0:00:05  lr: 0.000027  loss: 2.2195 (2.2869)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [770/780]  eta: 0:00:02  lr: 0.000027  loss: 2.3299 (2.2873)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [9]  [779/780]  eta: 0:00:00  lr: 0.000027  loss: 2.2643 (2.2848)  time: 0.2778  data: 0.0002  max mem: 8734\n",
            "Epoch: [9] Total time: 0:03:39 (0.2812 s / it)\n",
            "Averaged stats: lr: 0.000027  loss: 2.2643 (2.2848)\n",
            "Test:  [  0/105]  eta: 0:02:32  loss: 0.4296 (0.4296)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.4498  data: 1.3029  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:24  loss: 0.5142 (0.5559)  acc1: 88.5417 (88.6364)  acc5: 98.9583 (98.4849)  time: 0.2539  data: 0.1186  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:16  loss: 0.5152 (0.5483)  acc1: 87.5000 (88.3433)  acc5: 98.9583 (98.7599)  time: 0.1328  data: 0.0002  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:13  loss: 0.5890 (0.5828)  acc1: 86.4583 (87.2312)  acc5: 97.9167 (98.3535)  time: 0.1316  data: 0.0002  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 0.6079 (0.5827)  acc1: 84.3750 (86.9665)  acc5: 97.9167 (98.3740)  time: 0.1322  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 0.5777 (0.5825)  acc1: 84.3750 (86.8464)  acc5: 98.9583 (98.4477)  time: 0.1324  data: 0.0002  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 0.5510 (0.5738)  acc1: 87.5000 (87.1585)  acc5: 98.9583 (98.5656)  time: 0.1324  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 0.5510 (0.5780)  acc1: 87.5000 (87.0452)  acc5: 98.9583 (98.4889)  time: 0.1319  data: 0.0002  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 0.5686 (0.5772)  acc1: 86.4583 (87.1142)  acc5: 97.9167 (98.4697)  time: 0.1322  data: 0.0006  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 0.5390 (0.5742)  acc1: 87.5000 (87.2024)  acc5: 97.9167 (98.5005)  time: 0.1323  data: 0.0006  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.5473 (0.5748)  acc1: 87.5000 (87.1287)  acc5: 98.9583 (98.4530)  time: 0.1314  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.5589 (0.5754)  acc1: 87.5000 (87.0600)  acc5: 98.9583 (98.4700)  time: 0.1257  data: 0.0002  max mem: 8734\n",
            "Test: Total time: 0:00:15 (0.1448 s / it)\n",
            "* Acc@1 87.060 Acc@5 98.470 loss 0.575\n",
            "Accuracy of the network on the 10000 test images: 87.1%\n",
            "Max accuracy: 87.06%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [10]  [  0/780]  eta: 0:15:40  lr: 0.000021  loss: 2.5250 (2.5250)  time: 1.2056  data: 0.8553  max mem: 8734\n",
            "Epoch: [10]  [ 10/780]  eta: 0:04:41  lr: 0.000021  loss: 2.4927 (2.3870)  time: 0.3657  data: 0.0779  max mem: 8734\n",
            "Epoch: [10]  [ 20/780]  eta: 0:04:07  lr: 0.000021  loss: 2.4290 (2.4034)  time: 0.2811  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [ 30/780]  eta: 0:03:52  lr: 0.000021  loss: 2.3012 (2.3380)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [ 40/780]  eta: 0:03:44  lr: 0.000021  loss: 2.2206 (2.2999)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [ 50/780]  eta: 0:03:37  lr: 0.000021  loss: 2.5286 (2.3341)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [ 60/780]  eta: 0:03:32  lr: 0.000021  loss: 2.3989 (2.2909)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [ 70/780]  eta: 0:03:27  lr: 0.000021  loss: 2.3450 (2.3265)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [ 80/780]  eta: 0:03:23  lr: 0.000021  loss: 2.4127 (2.3279)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [ 90/780]  eta: 0:03:20  lr: 0.000021  loss: 2.4765 (2.3333)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [100/780]  eta: 0:03:16  lr: 0.000021  loss: 2.4932 (2.3451)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [110/780]  eta: 0:03:13  lr: 0.000021  loss: 2.4539 (2.3523)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [120/780]  eta: 0:03:09  lr: 0.000021  loss: 2.2680 (2.3358)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [130/780]  eta: 0:03:06  lr: 0.000021  loss: 2.1205 (2.3272)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [140/780]  eta: 0:03:03  lr: 0.000021  loss: 2.2899 (2.3292)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [150/780]  eta: 0:03:00  lr: 0.000021  loss: 2.3589 (2.3322)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [160/780]  eta: 0:02:57  lr: 0.000021  loss: 2.2779 (2.3288)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [170/780]  eta: 0:02:54  lr: 0.000021  loss: 2.2662 (2.3146)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [180/780]  eta: 0:02:51  lr: 0.000021  loss: 2.0655 (2.3092)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [190/780]  eta: 0:02:48  lr: 0.000021  loss: 2.2488 (2.3090)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [200/780]  eta: 0:02:45  lr: 0.000021  loss: 2.2488 (2.2984)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [210/780]  eta: 0:02:42  lr: 0.000021  loss: 2.2612 (2.2984)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [220/780]  eta: 0:02:39  lr: 0.000021  loss: 2.3324 (2.3043)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [230/780]  eta: 0:02:36  lr: 0.000021  loss: 2.3324 (2.2955)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [240/780]  eta: 0:02:33  lr: 0.000021  loss: 2.1180 (2.2904)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [250/780]  eta: 0:02:30  lr: 0.000021  loss: 2.2542 (2.2818)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [260/780]  eta: 0:02:27  lr: 0.000021  loss: 2.3079 (2.2782)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [270/780]  eta: 0:02:24  lr: 0.000021  loss: 2.4751 (2.2830)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [280/780]  eta: 0:02:21  lr: 0.000021  loss: 2.5100 (2.2862)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [290/780]  eta: 0:02:18  lr: 0.000021  loss: 2.5027 (2.2887)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [300/780]  eta: 0:02:15  lr: 0.000021  loss: 2.2511 (2.2888)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [310/780]  eta: 0:02:12  lr: 0.000021  loss: 2.2511 (2.2852)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [320/780]  eta: 0:02:10  lr: 0.000021  loss: 2.2970 (2.2827)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [330/780]  eta: 0:02:07  lr: 0.000021  loss: 2.3608 (2.2863)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [340/780]  eta: 0:02:04  lr: 0.000021  loss: 2.3199 (2.2812)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [350/780]  eta: 0:02:01  lr: 0.000021  loss: 2.2108 (2.2799)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [360/780]  eta: 0:01:58  lr: 0.000021  loss: 2.2515 (2.2786)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [370/780]  eta: 0:01:55  lr: 0.000021  loss: 2.2857 (2.2762)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [380/780]  eta: 0:01:52  lr: 0.000021  loss: 2.2075 (2.2756)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [390/780]  eta: 0:01:50  lr: 0.000021  loss: 2.3008 (2.2758)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [400/780]  eta: 0:01:47  lr: 0.000021  loss: 2.3942 (2.2757)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [410/780]  eta: 0:01:44  lr: 0.000021  loss: 2.3561 (2.2756)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [420/780]  eta: 0:01:41  lr: 0.000021  loss: 2.1826 (2.2722)  time: 0.2785  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [430/780]  eta: 0:01:38  lr: 0.000021  loss: 2.1336 (2.2713)  time: 0.2786  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [440/780]  eta: 0:01:35  lr: 0.000021  loss: 2.0875 (2.2682)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [450/780]  eta: 0:01:32  lr: 0.000021  loss: 2.2566 (2.2655)  time: 0.2794  data: 0.0004  max mem: 8734\n",
            "Epoch: [10]  [460/780]  eta: 0:01:30  lr: 0.000021  loss: 2.3346 (2.2660)  time: 0.2792  data: 0.0004  max mem: 8734\n",
            "Epoch: [10]  [470/780]  eta: 0:01:27  lr: 0.000021  loss: 2.4285 (2.2700)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [480/780]  eta: 0:01:24  lr: 0.000021  loss: 2.3443 (2.2689)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [490/780]  eta: 0:01:21  lr: 0.000021  loss: 2.2963 (2.2708)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [500/780]  eta: 0:01:18  lr: 0.000021  loss: 2.2222 (2.2674)  time: 0.2810  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [510/780]  eta: 0:01:16  lr: 0.000021  loss: 2.1032 (2.2634)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [520/780]  eta: 0:01:13  lr: 0.000021  loss: 2.1032 (2.2594)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [530/780]  eta: 0:01:10  lr: 0.000021  loss: 2.2461 (2.2610)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [540/780]  eta: 0:01:07  lr: 0.000021  loss: 2.4246 (2.2613)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [550/780]  eta: 0:01:04  lr: 0.000021  loss: 2.4145 (2.2603)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [560/780]  eta: 0:01:01  lr: 0.000021  loss: 2.3589 (2.2614)  time: 0.2811  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [570/780]  eta: 0:00:59  lr: 0.000021  loss: 2.3432 (2.2601)  time: 0.2813  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [580/780]  eta: 0:00:56  lr: 0.000021  loss: 2.2339 (2.2604)  time: 0.2817  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [590/780]  eta: 0:00:53  lr: 0.000021  loss: 2.2789 (2.2633)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [600/780]  eta: 0:00:50  lr: 0.000021  loss: 2.2161 (2.2619)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [610/780]  eta: 0:00:47  lr: 0.000021  loss: 2.2161 (2.2641)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [620/780]  eta: 0:00:45  lr: 0.000021  loss: 2.1116 (2.2581)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [630/780]  eta: 0:00:42  lr: 0.000021  loss: 2.0641 (2.2601)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [640/780]  eta: 0:00:39  lr: 0.000021  loss: 2.3038 (2.2588)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [650/780]  eta: 0:00:36  lr: 0.000021  loss: 2.3486 (2.2628)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [660/780]  eta: 0:00:33  lr: 0.000021  loss: 2.4067 (2.2632)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [670/780]  eta: 0:00:30  lr: 0.000021  loss: 2.2311 (2.2616)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [680/780]  eta: 0:00:28  lr: 0.000021  loss: 2.2317 (2.2605)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [690/780]  eta: 0:00:25  lr: 0.000021  loss: 2.1879 (2.2590)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [700/780]  eta: 0:00:22  lr: 0.000021  loss: 2.2494 (2.2598)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [710/780]  eta: 0:00:19  lr: 0.000021  loss: 2.4132 (2.2612)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [720/780]  eta: 0:00:16  lr: 0.000021  loss: 2.3637 (2.2592)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [730/780]  eta: 0:00:14  lr: 0.000021  loss: 2.1575 (2.2590)  time: 0.2811  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [740/780]  eta: 0:00:11  lr: 0.000021  loss: 2.3262 (2.2616)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [750/780]  eta: 0:00:08  lr: 0.000021  loss: 2.3330 (2.2620)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [760/780]  eta: 0:00:05  lr: 0.000021  loss: 2.3336 (2.2625)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [770/780]  eta: 0:00:02  lr: 0.000021  loss: 2.3336 (2.2622)  time: 0.2782  data: 0.0002  max mem: 8734\n",
            "Epoch: [10]  [779/780]  eta: 0:00:00  lr: 0.000021  loss: 2.3223 (2.2622)  time: 0.2774  data: 0.0001  max mem: 8734\n",
            "Epoch: [10] Total time: 0:03:39 (0.2812 s / it)\n",
            "Averaged stats: lr: 0.000021  loss: 2.3223 (2.2622)\n",
            "Test:  [  0/105]  eta: 0:01:44  loss: 0.4426 (0.4426)  acc1: 92.7083 (92.7083)  acc5: 100.0000 (100.0000)  time: 0.9936  data: 0.8481  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:21  loss: 0.5816 (0.5720)  acc1: 88.5417 (89.0152)  acc5: 97.9167 (98.2955)  time: 0.2294  data: 0.0950  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:15  loss: 0.5816 (0.5642)  acc1: 87.5000 (88.4425)  acc5: 97.9167 (98.3631)  time: 0.1440  data: 0.0099  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 0.6089 (0.5987)  acc1: 85.4167 (87.0296)  acc5: 97.9167 (98.0847)  time: 0.1343  data: 0.0010  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 0.6234 (0.5970)  acc1: 84.3750 (86.9411)  acc5: 97.9167 (98.1707)  time: 0.1330  data: 0.0010  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 0.6166 (0.5991)  acc1: 86.4583 (86.8056)  acc5: 98.9583 (98.3252)  time: 0.1327  data: 0.0002  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 0.5488 (0.5883)  acc1: 87.5000 (87.1585)  acc5: 98.9583 (98.4290)  time: 0.1333  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 0.5693 (0.5920)  acc1: 86.4583 (87.0305)  acc5: 98.9583 (98.3568)  time: 0.1329  data: 0.0010  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 0.6048 (0.5923)  acc1: 86.4583 (87.0113)  acc5: 97.9167 (98.3153)  time: 0.1325  data: 0.0010  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 0.5377 (0.5878)  acc1: 87.5000 (87.1909)  acc5: 97.9167 (98.3745)  time: 0.1317  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.5639 (0.5883)  acc1: 87.5000 (87.2112)  acc5: 98.9583 (98.3911)  time: 0.1310  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.5643 (0.5888)  acc1: 87.5000 (87.1800)  acc5: 98.9583 (98.3800)  time: 0.1257  data: 0.0002  max mem: 8734\n",
            "Test: Total time: 0:00:15 (0.1429 s / it)\n",
            "* Acc@1 87.180 Acc@5 98.380 loss 0.589\n",
            "Accuracy of the network on the 10000 test images: 87.2%\n",
            "Max accuracy: 87.18%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [11]  [  0/780]  eta: 0:17:57  lr: 0.000017  loss: 2.3165 (2.3165)  time: 1.3812  data: 1.0188  max mem: 8734\n",
            "Epoch: [11]  [ 10/780]  eta: 0:04:51  lr: 0.000017  loss: 2.3165 (2.2612)  time: 0.3790  data: 0.0928  max mem: 8734\n",
            "Epoch: [11]  [ 20/780]  eta: 0:04:11  lr: 0.000017  loss: 2.3270 (2.3334)  time: 0.2785  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [ 30/780]  eta: 0:03:55  lr: 0.000017  loss: 2.4699 (2.3800)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [ 40/780]  eta: 0:03:46  lr: 0.000017  loss: 2.3767 (2.3668)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [ 50/780]  eta: 0:03:39  lr: 0.000017  loss: 2.3436 (2.3420)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [ 60/780]  eta: 0:03:33  lr: 0.000017  loss: 2.1797 (2.3264)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [ 70/780]  eta: 0:03:29  lr: 0.000017  loss: 2.3594 (2.3180)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [ 80/780]  eta: 0:03:25  lr: 0.000017  loss: 2.3594 (2.2990)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [ 90/780]  eta: 0:03:21  lr: 0.000017  loss: 2.1590 (2.2842)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [100/780]  eta: 0:03:17  lr: 0.000017  loss: 2.2432 (2.2775)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [110/780]  eta: 0:03:13  lr: 0.000017  loss: 2.1007 (2.2639)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [120/780]  eta: 0:03:10  lr: 0.000017  loss: 2.1932 (2.2555)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [130/780]  eta: 0:03:07  lr: 0.000017  loss: 2.3595 (2.2570)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [140/780]  eta: 0:03:03  lr: 0.000017  loss: 2.3666 (2.2550)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [150/780]  eta: 0:03:00  lr: 0.000017  loss: 2.2097 (2.2519)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [160/780]  eta: 0:02:57  lr: 0.000017  loss: 2.4502 (2.2654)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [170/780]  eta: 0:02:54  lr: 0.000017  loss: 2.4502 (2.2664)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [180/780]  eta: 0:02:51  lr: 0.000017  loss: 2.2739 (2.2696)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [190/780]  eta: 0:02:48  lr: 0.000017  loss: 2.2420 (2.2672)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [200/780]  eta: 0:02:45  lr: 0.000017  loss: 2.3719 (2.2735)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [210/780]  eta: 0:02:42  lr: 0.000017  loss: 2.3719 (2.2746)  time: 0.2814  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [220/780]  eta: 0:02:39  lr: 0.000017  loss: 2.3368 (2.2731)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [230/780]  eta: 0:02:36  lr: 0.000017  loss: 2.1851 (2.2674)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [240/780]  eta: 0:02:33  lr: 0.000017  loss: 2.0703 (2.2618)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [250/780]  eta: 0:02:30  lr: 0.000017  loss: 2.2365 (2.2643)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [260/780]  eta: 0:02:27  lr: 0.000017  loss: 2.3431 (2.2671)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [270/780]  eta: 0:02:24  lr: 0.000017  loss: 2.3431 (2.2644)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [280/780]  eta: 0:02:21  lr: 0.000017  loss: 2.3041 (2.2655)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [290/780]  eta: 0:02:18  lr: 0.000017  loss: 2.3500 (2.2656)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [300/780]  eta: 0:02:16  lr: 0.000017  loss: 2.3282 (2.2625)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [310/780]  eta: 0:02:13  lr: 0.000017  loss: 2.2786 (2.2621)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [320/780]  eta: 0:02:10  lr: 0.000017  loss: 2.3044 (2.2585)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [330/780]  eta: 0:02:07  lr: 0.000017  loss: 2.3044 (2.2597)  time: 0.2808  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [340/780]  eta: 0:02:04  lr: 0.000017  loss: 2.1990 (2.2590)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [350/780]  eta: 0:02:01  lr: 0.000017  loss: 2.2429 (2.2611)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [360/780]  eta: 0:01:58  lr: 0.000017  loss: 2.3297 (2.2636)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [370/780]  eta: 0:01:55  lr: 0.000017  loss: 2.3021 (2.2613)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [380/780]  eta: 0:01:53  lr: 0.000017  loss: 2.2327 (2.2617)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [390/780]  eta: 0:01:50  lr: 0.000017  loss: 2.3559 (2.2655)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [400/780]  eta: 0:01:47  lr: 0.000017  loss: 2.1952 (2.2577)  time: 0.2801  data: 0.0004  max mem: 8734\n",
            "Epoch: [11]  [410/780]  eta: 0:01:44  lr: 0.000017  loss: 2.2789 (2.2628)  time: 0.2802  data: 0.0004  max mem: 8734\n",
            "Epoch: [11]  [420/780]  eta: 0:01:41  lr: 0.000017  loss: 2.2051 (2.2534)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [430/780]  eta: 0:01:38  lr: 0.000017  loss: 1.9532 (2.2474)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [440/780]  eta: 0:01:36  lr: 0.000017  loss: 2.1446 (2.2510)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [450/780]  eta: 0:01:33  lr: 0.000017  loss: 2.2437 (2.2491)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [460/780]  eta: 0:01:30  lr: 0.000017  loss: 2.1756 (2.2482)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [470/780]  eta: 0:01:27  lr: 0.000017  loss: 2.0579 (2.2412)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [480/780]  eta: 0:01:24  lr: 0.000017  loss: 1.9635 (2.2389)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [490/780]  eta: 0:01:21  lr: 0.000017  loss: 2.1964 (2.2372)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [500/780]  eta: 0:01:18  lr: 0.000017  loss: 2.2983 (2.2380)  time: 0.2794  data: 0.0004  max mem: 8734\n",
            "Epoch: [11]  [510/780]  eta: 0:01:16  lr: 0.000017  loss: 2.2983 (2.2387)  time: 0.2796  data: 0.0004  max mem: 8734\n",
            "Epoch: [11]  [520/780]  eta: 0:01:13  lr: 0.000017  loss: 2.0868 (2.2347)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [530/780]  eta: 0:01:10  lr: 0.000017  loss: 2.2764 (2.2382)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [540/780]  eta: 0:01:07  lr: 0.000017  loss: 2.3271 (2.2355)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [550/780]  eta: 0:01:04  lr: 0.000017  loss: 2.2236 (2.2355)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [560/780]  eta: 0:01:01  lr: 0.000017  loss: 2.2704 (2.2343)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [570/780]  eta: 0:00:59  lr: 0.000017  loss: 2.2230 (2.2322)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [580/780]  eta: 0:00:56  lr: 0.000017  loss: 2.0652 (2.2295)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [590/780]  eta: 0:00:53  lr: 0.000017  loss: 2.2640 (2.2307)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [600/780]  eta: 0:00:50  lr: 0.000017  loss: 2.2640 (2.2259)  time: 0.2802  data: 0.0004  max mem: 8734\n",
            "Epoch: [11]  [610/780]  eta: 0:00:47  lr: 0.000017  loss: 2.0755 (2.2254)  time: 0.2796  data: 0.0004  max mem: 8734\n",
            "Epoch: [11]  [620/780]  eta: 0:00:45  lr: 0.000017  loss: 2.1002 (2.2266)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [630/780]  eta: 0:00:42  lr: 0.000017  loss: 2.3500 (2.2277)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [640/780]  eta: 0:00:39  lr: 0.000017  loss: 2.4928 (2.2287)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [650/780]  eta: 0:00:36  lr: 0.000017  loss: 2.2996 (2.2287)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [660/780]  eta: 0:00:33  lr: 0.000017  loss: 2.1519 (2.2273)  time: 0.2823  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [670/780]  eta: 0:00:30  lr: 0.000017  loss: 2.0319 (2.2258)  time: 0.2830  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [680/780]  eta: 0:00:28  lr: 0.000017  loss: 2.1519 (2.2260)  time: 0.2818  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [690/780]  eta: 0:00:25  lr: 0.000017  loss: 2.1743 (2.2258)  time: 0.2809  data: 0.0004  max mem: 8734\n",
            "Epoch: [11]  [700/780]  eta: 0:00:22  lr: 0.000017  loss: 2.2582 (2.2268)  time: 0.2809  data: 0.0004  max mem: 8734\n",
            "Epoch: [11]  [710/780]  eta: 0:00:19  lr: 0.000017  loss: 2.1841 (2.2236)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [720/780]  eta: 0:00:16  lr: 0.000017  loss: 2.0613 (2.2232)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [730/780]  eta: 0:00:14  lr: 0.000017  loss: 2.3128 (2.2228)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [740/780]  eta: 0:00:11  lr: 0.000017  loss: 2.1514 (2.2216)  time: 0.2810  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [750/780]  eta: 0:00:08  lr: 0.000017  loss: 2.0847 (2.2188)  time: 0.2812  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [760/780]  eta: 0:00:05  lr: 0.000017  loss: 2.1001 (2.2188)  time: 0.2811  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [770/780]  eta: 0:00:02  lr: 0.000017  loss: 2.1228 (2.2168)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [11]  [779/780]  eta: 0:00:00  lr: 0.000017  loss: 2.1589 (2.2160)  time: 0.2775  data: 0.0001  max mem: 8734\n",
            "Epoch: [11] Total time: 0:03:39 (0.2815 s / it)\n",
            "Averaged stats: lr: 0.000017  loss: 2.1589 (2.2160)\n",
            "Test:  [  0/105]  eta: 0:02:15  loss: 0.4024 (0.4024)  acc1: 96.8750 (96.8750)  acc5: 100.0000 (100.0000)  time: 1.2952  data: 1.1398  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:22  loss: 0.5361 (0.5555)  acc1: 88.5417 (89.0152)  acc5: 97.9167 (98.4849)  time: 0.2391  data: 0.1038  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:15  loss: 0.5361 (0.5530)  acc1: 88.5417 (88.7401)  acc5: 97.9167 (98.5119)  time: 0.1326  data: 0.0002  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 0.5744 (0.5890)  acc1: 85.4167 (87.3656)  acc5: 97.9167 (98.2191)  time: 0.1322  data: 0.0002  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 0.6402 (0.5909)  acc1: 85.4167 (87.2713)  acc5: 97.9167 (98.2724)  time: 0.1324  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 0.6330 (0.5925)  acc1: 86.4583 (87.3979)  acc5: 97.9167 (98.3864)  time: 0.1323  data: 0.0002  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 0.5769 (0.5830)  acc1: 87.5000 (87.6025)  acc5: 98.9583 (98.4119)  time: 0.1321  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 0.5746 (0.5870)  acc1: 86.4583 (87.3239)  acc5: 97.9167 (98.3862)  time: 0.1323  data: 0.0006  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 0.5819 (0.5857)  acc1: 85.4167 (87.3200)  acc5: 97.9167 (98.3539)  time: 0.1329  data: 0.0006  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 0.5389 (0.5808)  acc1: 88.5417 (87.4199)  acc5: 97.9167 (98.4089)  time: 0.1328  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.5415 (0.5825)  acc1: 87.5000 (87.4072)  acc5: 98.9583 (98.3808)  time: 0.1317  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.5637 (0.5830)  acc1: 86.4583 (87.3500)  acc5: 98.9583 (98.3800)  time: 0.1258  data: 0.0002  max mem: 8734\n",
            "Test: Total time: 0:00:15 (0.1437 s / it)\n",
            "* Acc@1 87.350 Acc@5 98.380 loss 0.583\n",
            "Accuracy of the network on the 10000 test images: 87.4%\n",
            "Max accuracy: 87.35%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [12]  [  0/780]  eta: 0:18:16  lr: 0.000013  loss: 2.4962 (2.4962)  time: 1.4058  data: 1.0937  max mem: 8734\n",
            "Epoch: [12]  [ 10/780]  eta: 0:04:54  lr: 0.000013  loss: 2.4609 (2.3325)  time: 0.3825  data: 0.0996  max mem: 8734\n",
            "Epoch: [12]  [ 20/780]  eta: 0:04:13  lr: 0.000013  loss: 2.3462 (2.2919)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [ 30/780]  eta: 0:03:57  lr: 0.000013  loss: 2.3462 (2.2910)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [ 40/780]  eta: 0:03:47  lr: 0.000013  loss: 2.2861 (2.2650)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [ 50/780]  eta: 0:03:40  lr: 0.000013  loss: 2.3951 (2.2690)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [ 60/780]  eta: 0:03:34  lr: 0.000013  loss: 2.4091 (2.2692)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [ 70/780]  eta: 0:03:30  lr: 0.000013  loss: 2.2966 (2.2870)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [ 80/780]  eta: 0:03:25  lr: 0.000013  loss: 2.2966 (2.2909)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [ 90/780]  eta: 0:03:21  lr: 0.000013  loss: 2.3904 (2.2909)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [100/780]  eta: 0:03:18  lr: 0.000013  loss: 2.0912 (2.2754)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [110/780]  eta: 0:03:14  lr: 0.000013  loss: 2.2255 (2.2825)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [120/780]  eta: 0:03:10  lr: 0.000013  loss: 2.2748 (2.2840)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [130/780]  eta: 0:03:07  lr: 0.000013  loss: 2.3088 (2.2796)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [140/780]  eta: 0:03:04  lr: 0.000013  loss: 2.2051 (2.2741)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [150/780]  eta: 0:03:01  lr: 0.000013  loss: 1.9778 (2.2429)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [160/780]  eta: 0:02:57  lr: 0.000013  loss: 1.9535 (2.2323)  time: 0.2789  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [170/780]  eta: 0:02:54  lr: 0.000013  loss: 2.1339 (2.2365)  time: 0.2793  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [180/780]  eta: 0:02:51  lr: 0.000013  loss: 2.3473 (2.2404)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [190/780]  eta: 0:02:48  lr: 0.000013  loss: 2.2690 (2.2341)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [200/780]  eta: 0:02:45  lr: 0.000013  loss: 2.1676 (2.2334)  time: 0.2818  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [210/780]  eta: 0:02:42  lr: 0.000013  loss: 2.2205 (2.2343)  time: 0.2824  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [220/780]  eta: 0:02:39  lr: 0.000013  loss: 2.2205 (2.2317)  time: 0.2813  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [230/780]  eta: 0:02:36  lr: 0.000013  loss: 2.1749 (2.2319)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [240/780]  eta: 0:02:33  lr: 0.000013  loss: 2.1749 (2.2244)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [250/780]  eta: 0:02:30  lr: 0.000013  loss: 2.2188 (2.2245)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [260/780]  eta: 0:02:27  lr: 0.000013  loss: 2.0967 (2.2165)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [270/780]  eta: 0:02:24  lr: 0.000013  loss: 2.1571 (2.2194)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [280/780]  eta: 0:02:22  lr: 0.000013  loss: 2.0781 (2.2088)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [290/780]  eta: 0:02:19  lr: 0.000013  loss: 2.0781 (2.2100)  time: 0.2800  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [300/780]  eta: 0:02:16  lr: 0.000013  loss: 2.2014 (2.2092)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [310/780]  eta: 0:02:13  lr: 0.000013  loss: 2.2822 (2.2151)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [320/780]  eta: 0:02:10  lr: 0.000013  loss: 2.2807 (2.2167)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [330/780]  eta: 0:02:07  lr: 0.000013  loss: 2.2611 (2.2179)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [340/780]  eta: 0:02:04  lr: 0.000013  loss: 2.2843 (2.2198)  time: 0.2813  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [350/780]  eta: 0:02:01  lr: 0.000013  loss: 2.2772 (2.2218)  time: 0.2845  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [360/780]  eta: 0:01:59  lr: 0.000013  loss: 2.2724 (2.2222)  time: 0.2840  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [370/780]  eta: 0:01:56  lr: 0.000013  loss: 2.2592 (2.2199)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [380/780]  eta: 0:01:53  lr: 0.000013  loss: 2.2884 (2.2211)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [390/780]  eta: 0:01:50  lr: 0.000013  loss: 2.2517 (2.2197)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [400/780]  eta: 0:01:47  lr: 0.000013  loss: 2.1244 (2.2170)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [410/780]  eta: 0:01:44  lr: 0.000013  loss: 2.1244 (2.2154)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [420/780]  eta: 0:01:41  lr: 0.000013  loss: 2.1235 (2.2119)  time: 0.2795  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [430/780]  eta: 0:01:38  lr: 0.000013  loss: 2.2713 (2.2154)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [440/780]  eta: 0:01:36  lr: 0.000013  loss: 2.3624 (2.2175)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [450/780]  eta: 0:01:33  lr: 0.000013  loss: 2.2033 (2.2137)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [460/780]  eta: 0:01:30  lr: 0.000013  loss: 2.1692 (2.2135)  time: 0.2801  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [470/780]  eta: 0:01:27  lr: 0.000013  loss: 2.3373 (2.2117)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [480/780]  eta: 0:01:24  lr: 0.000013  loss: 2.1662 (2.2088)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [490/780]  eta: 0:01:21  lr: 0.000013  loss: 2.0404 (2.2056)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [500/780]  eta: 0:01:19  lr: 0.000013  loss: 2.0562 (2.2026)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [510/780]  eta: 0:01:16  lr: 0.000013  loss: 2.0685 (2.2025)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [520/780]  eta: 0:01:13  lr: 0.000013  loss: 2.2197 (2.2002)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [530/780]  eta: 0:01:10  lr: 0.000013  loss: 2.2802 (2.2030)  time: 0.2802  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [540/780]  eta: 0:01:07  lr: 0.000013  loss: 2.1678 (2.1994)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [550/780]  eta: 0:01:04  lr: 0.000013  loss: 1.9677 (2.1967)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [560/780]  eta: 0:01:02  lr: 0.000013  loss: 2.1611 (2.1964)  time: 0.2788  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [570/780]  eta: 0:00:59  lr: 0.000013  loss: 2.1910 (2.1965)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [580/780]  eta: 0:00:56  lr: 0.000013  loss: 2.2340 (2.1975)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [590/780]  eta: 0:00:53  lr: 0.000013  loss: 2.3704 (2.1977)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [600/780]  eta: 0:00:50  lr: 0.000013  loss: 2.0069 (2.1935)  time: 0.2799  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [610/780]  eta: 0:00:47  lr: 0.000013  loss: 2.0992 (2.1955)  time: 0.2797  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [620/780]  eta: 0:00:45  lr: 0.000013  loss: 2.3392 (2.1975)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [630/780]  eta: 0:00:42  lr: 0.000013  loss: 2.3392 (2.1974)  time: 0.2787  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [640/780]  eta: 0:00:39  lr: 0.000013  loss: 2.1141 (2.1950)  time: 0.2790  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [650/780]  eta: 0:00:36  lr: 0.000013  loss: 2.1389 (2.1963)  time: 0.2798  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [660/780]  eta: 0:00:33  lr: 0.000013  loss: 2.2518 (2.1978)  time: 0.2805  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [670/780]  eta: 0:00:30  lr: 0.000013  loss: 2.2518 (2.1982)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [680/780]  eta: 0:00:28  lr: 0.000013  loss: 2.3286 (2.2013)  time: 0.2794  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [690/780]  eta: 0:00:25  lr: 0.000013  loss: 2.0684 (2.1978)  time: 0.2791  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [700/780]  eta: 0:00:22  lr: 0.000013  loss: 2.1093 (2.1993)  time: 0.2807  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [710/780]  eta: 0:00:19  lr: 0.000013  loss: 2.3444 (2.2000)  time: 0.2819  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [720/780]  eta: 0:00:16  lr: 0.000013  loss: 2.3570 (2.2009)  time: 0.2803  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [730/780]  eta: 0:00:14  lr: 0.000013  loss: 2.1399 (2.1985)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [740/780]  eta: 0:00:11  lr: 0.000013  loss: 2.2108 (2.1995)  time: 0.2796  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [750/780]  eta: 0:00:08  lr: 0.000013  loss: 2.3675 (2.2003)  time: 0.2804  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [760/780]  eta: 0:00:05  lr: 0.000013  loss: 2.2761 (2.2018)  time: 0.2806  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [770/780]  eta: 0:00:02  lr: 0.000013  loss: 2.1271 (2.2003)  time: 0.2792  data: 0.0002  max mem: 8734\n",
            "Epoch: [12]  [779/780]  eta: 0:00:00  lr: 0.000013  loss: 2.1033 (2.1990)  time: 0.2782  data: 0.0001  max mem: 8734\n",
            "Epoch: [12] Total time: 0:03:39 (0.2817 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 2.1033 (2.1990)\n",
            "Test:  [  0/105]  eta: 0:02:17  loss: 0.4252 (0.4252)  acc1: 91.6667 (91.6667)  acc5: 100.0000 (100.0000)  time: 1.3117  data: 1.1667  max mem: 8734\n",
            "Test:  [ 10/105]  eta: 0:00:22  loss: 0.5454 (0.5505)  acc1: 89.5833 (88.8258)  acc5: 97.9167 (98.1061)  time: 0.2410  data: 0.1063  max mem: 8734\n",
            "Test:  [ 20/105]  eta: 0:00:16  loss: 0.5386 (0.5481)  acc1: 89.5833 (88.4425)  acc5: 97.9167 (98.3631)  time: 0.1326  data: 0.0002  max mem: 8734\n",
            "Test:  [ 30/105]  eta: 0:00:12  loss: 0.5868 (0.5823)  acc1: 85.4167 (87.1976)  acc5: 97.9167 (98.0175)  time: 0.1318  data: 0.0002  max mem: 8734\n",
            "Test:  [ 40/105]  eta: 0:00:10  loss: 0.6226 (0.5823)  acc1: 85.4167 (87.2205)  acc5: 97.9167 (98.0945)  time: 0.1323  data: 0.0002  max mem: 8734\n",
            "Test:  [ 50/105]  eta: 0:00:08  loss: 0.5899 (0.5835)  acc1: 86.4583 (87.2141)  acc5: 97.9167 (98.2026)  time: 0.1324  data: 0.0002  max mem: 8734\n",
            "Test:  [ 60/105]  eta: 0:00:06  loss: 0.5566 (0.5751)  acc1: 88.5417 (87.4317)  acc5: 97.9167 (98.2582)  time: 0.1324  data: 0.0002  max mem: 8734\n",
            "Test:  [ 70/105]  eta: 0:00:05  loss: 0.5541 (0.5805)  acc1: 87.5000 (87.2506)  acc5: 97.9167 (98.2248)  time: 0.1320  data: 0.0002  max mem: 8734\n",
            "Test:  [ 80/105]  eta: 0:00:03  loss: 0.5676 (0.5789)  acc1: 86.4583 (87.2685)  acc5: 97.9167 (98.1996)  time: 0.1316  data: 0.0002  max mem: 8734\n",
            "Test:  [ 90/105]  eta: 0:00:02  loss: 0.5260 (0.5739)  acc1: 88.5417 (87.4084)  acc5: 98.9583 (98.2715)  time: 0.1313  data: 0.0002  max mem: 8734\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.5464 (0.5748)  acc1: 87.5000 (87.3453)  acc5: 98.9583 (98.2364)  time: 0.1311  data: 0.0002  max mem: 8734\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.5484 (0.5750)  acc1: 87.5000 (87.3300)  acc5: 98.9583 (98.2600)  time: 0.1257  data: 0.0002  max mem: 8734\n",
            "Test: Total time: 0:00:15 (0.1434 s / it)\n",
            "* Acc@1 87.330 Acc@5 98.260 loss 0.575\n",
            "Accuracy of the network on the 10000 test images: 87.3%\n",
            "Max accuracy: 87.35%\n",
            "Training time 0:51:28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQLh6hE2Zj0e",
        "outputId": "ed90622d-2970-4a42-a51e-06ab312acc40"
      },
      "source": [
        "#if running on a local divice, comment these lines\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deit_base_cifar100.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "481fe63d433f4bb68353dbc217360d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52008afc76af46e187642e835270340d",
              "IPY_MODEL_9c3eb7787e7c42a1bf3f16e737a0b485",
              "IPY_MODEL_81c712ba88dd4a1a9352f5dd61178ce1"
            ],
            "layout": "IPY_MODEL_d98b6aba2831489eba00563e9820e590"
          }
        },
        "52008afc76af46e187642e835270340d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e01b15ed4bb46529aac6bd41d208987",
            "placeholder": "​",
            "style": "IPY_MODEL_d6efbae9e2e8453da58fd2e043fed52c",
            "value": ""
          }
        },
        "9c3eb7787e7c42a1bf3f16e737a0b485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37a73608bfa54c47aacc5f66a4fae6ec",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acfd700929a34bb79df79cc8f354db60",
            "value": 170498071
          }
        },
        "81c712ba88dd4a1a9352f5dd61178ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89a49a6b969b4e59912f1f788310a5df",
            "placeholder": "​",
            "style": "IPY_MODEL_7f131ac7fc854ee7b7f8390741d527da",
            "value": " 170499072/? [00:05&lt;00:00, 33224557.73it/s]"
          }
        },
        "d98b6aba2831489eba00563e9820e590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e01b15ed4bb46529aac6bd41d208987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6efbae9e2e8453da58fd2e043fed52c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37a73608bfa54c47aacc5f66a4fae6ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acfd700929a34bb79df79cc8f354db60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89a49a6b969b4e59912f1f788310a5df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f131ac7fc854ee7b7f8390741d527da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}