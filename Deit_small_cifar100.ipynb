{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70Vxlv9lMRZ",
        "outputId": "cb96e027-e6df-497e-a7bd-c303ba963a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ViT-pytorch'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 170 (delta 32), reused 27 (delta 27), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (170/170), 21.31 MiB | 8.28 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "/content/ViT-pytorch\n",
            "Collecting ml-collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from ml-collections) (1.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ml-collections) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ml-collections) (1.15.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from ml-collections) (0.5.5)\n",
            "Building wheels for collected packages: ml-collections\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94524 sha256=9620ddead2e4c8143ca2c4fb0efc74affdd2753994e1de58ee66cb621dde9f23\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/da/64/33c926a1b10ff19791081b705879561b715a8341a856a3bbd2\n",
            "Successfully built ml-collections\n",
            "Installing collected packages: ml-collections\n",
            "Successfully installed ml-collections-0.1.1\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n",
            "--2022-03-14 13:55:24--  https://docs.google.com/uc?export=download&confirm=t&id=1-AxL45qSt354FadCyK375_60ehXZfCJs\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.127.101, 108.177.127.102, 108.177.127.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.127.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/e23i3qjva2ue0gkc4v42vfar80pafugg/1647266100000/17691537378993098219/*/1-AxL45qSt354FadCyK375_60ehXZfCJs?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-03-14 13:55:24--  https://doc-08-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/e23i3qjva2ue0gkc4v42vfar80pafugg/1647266100000/17691537378993098219/*/1-AxL45qSt354FadCyK375_60ehXZfCJs?e=download\n",
            "Resolving doc-08-5g-docs.googleusercontent.com (doc-08-5g-docs.googleusercontent.com)... 108.177.119.132, 2a00:1450:4013:c00::84\n",
            "Connecting to doc-08-5g-docs.googleusercontent.com (doc-08-5g-docs.googleusercontent.com)|108.177.119.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 343302577 (327M) [application/x-zip]\n",
            "Saving to: ‘cifar10-100_500_checkpoint.bin’\n",
            "\n",
            "cifar10-100_500_che 100%[===================>] 327.40M   268MB/s    in 1.2s    \n",
            "\n",
            "2022-03-14 13:55:25 (268 MB/s) - ‘cifar10-100_500_checkpoint.bin’ saved [343302577/343302577]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jeonsworld/ViT-pytorch.git\n",
        "%cd ViT-pytorch/\n",
        "!pip install ml-collections\n",
        "!pip install einops\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-AxL45qSt354FadCyK375_60ehXZfCJs' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-AxL45qSt354FadCyK375_60ehXZfCJs\" -O cifar10-100_500_checkpoint.bin && rm -rf /tmp/cookies.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hGKear4S3Q_p"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from datetime import timedelta\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from models.modeling import VisionTransformer, CONFIGS\n",
        "from utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule\n",
        "from utils.data_utils import get_loader\n",
        "from utils.dist_util import get_world_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "# Required parameters\n",
        "parser.add_argument(\"--name\", default=\"cifar10\",\n",
        "                    help=\"Name of this run. Used for monitoring.\")\n",
        "parser.add_argument(\"--dataset\", choices=[\"cifar10\", \"cifar100\"], default=\"cifar10\",\n",
        "                    help=\"Which downstream task.\")\n",
        "parser.add_argument(\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\",\n",
        "                                              \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"],\n",
        "                    default=\"ViT-B_16\",\n",
        "                    help=\"Which variant to use.\")\n",
        "parser.add_argument(\"--pretrained_dir\", type=str, default=\"checkpoint/ViT-B_16.npz\",\n",
        "                    help=\"Where to search for pretrained ViT models.\")\n",
        "parser.add_argument(\"--pretrained_model\", type=str, default=\"cifar10-100_500_checkpoint.bin\")\n",
        "parser.add_argument(\"--output_dir\", default=\"output\", type=str,\n",
        "                    help=\"The output directory where checkpoints will be written.\")\n",
        "\n",
        "parser.add_argument(\"--img_size\", default=224, type=int,\n",
        "                    help=\"Resolution size\")\n",
        "parser.add_argument(\"--train_batch_size\", default=512, type=int,\n",
        "                    help=\"Total batch size for training.\")\n",
        "parser.add_argument(\"--eval_batch_size\", default=64, type=int,\n",
        "                    help=\"Total batch size for eval.\")\n",
        "parser.add_argument(\"--eval_every\", default=100, type=int,\n",
        "                    help=\"Run prediction on validation set every so many steps.\"\n",
        "                          \"Will always run one evaluation at the end of training.\")\n",
        "\n",
        "parser.add_argument(\"--learning_rate\", default=3e-2, type=float,\n",
        "                    help=\"The initial learning rate for SGD.\")\n",
        "parser.add_argument(\"--weight_decay\", default=0, type=float,\n",
        "                    help=\"Weight deay if we apply some.\")\n",
        "parser.add_argument(\"--num_steps\", default=10000, type=int,\n",
        "                    help=\"Total number of training epochs to perform.\")\n",
        "parser.add_argument(\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"cosine\",\n",
        "                    help=\"How to decay the learning rate.\")\n",
        "parser.add_argument(\"--warmup_steps\", default=500, type=int,\n",
        "                    help=\"Step of training to perform learning rate warmup for.\")\n",
        "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                    help=\"Max gradient norm.\")\n",
        "\n",
        "parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                    help=\"local_rank for distributed training on gpus\")\n",
        "parser.add_argument('--seed', type=int, default=42,\n",
        "                    help=\"random seed for initialization\")\n",
        "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "parser.add_argument('--fp16', action='store_true',\n",
        "                    help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
        "parser.add_argument('--fp16_opt_level', type=str, default='O2',\n",
        "                    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                          \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "parser.add_argument('--loss_scale', type=float, default=0,\n",
        "                    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
        "                          \"0 (default value): dynamic loss scaling.\\n\"\n",
        "                          \"Positive power of 2: static loss scaling value.\\n\")\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl',\n",
        "                                          timeout=timedelta(minutes=60))\n",
        "    args.n_gpu = 1\n",
        "\n",
        "args.name = 'cifar10'\n",
        "args.device = device"
      ],
      "metadata": {
        "id": "2t5UUX-qR_lO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = CONFIGS[args.model_type]\n",
        "num_classes = 10 if args.dataset == \"cifar10\" else 100\n",
        "model = VisionTransformer(config, args.img_size, zero_head=True, num_classes=num_classes)\n",
        "model.load_state_dict(torch.load(args.pretrained_model))\n",
        "model.to(args.device)\n",
        "train_loader, test_loader = get_loader(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "1143428bd6084a8ebc1d2286eb04a225",
            "c6f8d94e22304d3b854c380971b3b0f3",
            "547eecc9ed414835ae48df973d6cbc33",
            "7ab25c75239e4f5f9cac2a9fcb04065e",
            "df09fb5b5bf8451dae9f8b2d25b49a14",
            "8aacce6d789d406584cad9f9b7b4638e",
            "1eaa359b92414d1792d8fc4e200ec5f2",
            "a961d7df708347148d8c045008d249d1",
            "7ffa08b48ba5477a97a77c9fd3354796",
            "daf2c417fc6c4ff8ace803bc0a0cdbf2",
            "175ef95433344baa931043e604446edb"
          ]
        },
        "id": "w9chZMfcSYaW",
        "outputId": "01d9fae1-4b9b-42c5-94a6-dd49737e50e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1143428bd6084a8ebc1d2286eb04a225"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def test(model, test_loader):\n",
        "  model.eval()\n",
        "  all_preds, all_label = [], []\n",
        "  epoch_iterator = tqdm(test_loader,\n",
        "                        desc=\"Validating... (loss=X.X)\",\n",
        "                        bar_format=\"{l_bar}{r_bar}\",\n",
        "                        dynamic_ncols=True,\n",
        "                        disable=args.local_rank not in [-1, 0])\n",
        "  loss_fct = torch.nn.CrossEntropyLoss()\n",
        "  for step, batch in enumerate(epoch_iterator):\n",
        "      batch = tuple(t.to(args.device) for t in batch)\n",
        "      x, y = batch\n",
        "      with torch.no_grad():\n",
        "          logits = model(x)[0]\n",
        "\n",
        "          eval_loss = loss_fct(logits, y)\n",
        "          # eval_losses.update(eval_loss.item())\n",
        "\n",
        "          preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "      if len(all_preds) == 0:\n",
        "          all_preds.append(preds.detach().cpu().numpy())\n",
        "          all_label.append(y.detach().cpu().numpy())\n",
        "      else:\n",
        "          all_preds[0] = np.append(\n",
        "              all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
        "          )\n",
        "          all_label[0] = np.append(\n",
        "              all_label[0], y.detach().cpu().numpy(), axis=0\n",
        "          )\n",
        "      # epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
        "\n",
        "  all_preds, all_label = all_preds[0], all_label[0]\n",
        "  accuracy = simple_accuracy(all_preds, all_label)\n",
        "  print(\"Valid Accuracy: %2.5f\" % accuracy)"
      ],
      "metadata": {
        "id": "3208CvASUeR3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbUXkRNxVjls",
        "outputId": "4e9dda5f-710c-4ad6-8134-58a50f969849"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating... (loss=X.X): 100%|| 157/157 [00:34<00:00,  4.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.98940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Eurus-Holmes/DeiT-CIFAR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubDg6H_ocUXT",
        "outputId": "86735eac-dfcf-4926-c347-903067d07512"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeiT-CIFAR'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 28 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DeiT-CIFAR/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cXR17cTcca7",
        "outputId": "effe5469-a0b2-4bb7-f01c-819d1a9fbc63"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-pytorch/DeiT-CIFAR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm==0.4.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJk7qe7Kcfzq",
        "outputId": "a6a24eed-4f19-4d7e-e257-f3a89fecadaf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 34.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 40.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 71 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 215 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 225 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 266 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 276 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 286 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 317 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 327 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 337 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 368 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 376 kB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.4.12) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.4.12) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm==0.4.12) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.12) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.12) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth --data-set CIFAR --model deit_tiny_patch16_224 --batch-size 64 --epochs 13 --output_dir output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-17nSmwb-Wq",
        "outputId": "2e5fa64f-6808-459a-b477-f091ace5ae1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(aa='rand-m9-mstd0.5-inc1', batch_size=64, clip_grad=None, color_jitter=0.4, cooldown_epochs=10, cutmix=1.0, cutmix_minmax=None, data_path='/datasets01/imagenet_full_size/061417/', data_set='CIFAR', decay_epochs=30, decay_rate=0.1, device='cuda', dist_eval=False, dist_url='env://', distillation_alpha=0.5, distillation_tau=1.0, distillation_type='none', distributed=False, drop=0.0, drop_path=0.1, epochs=13, eval=False, finetune='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth', inat_category='name', input_size=224, lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, min_lr=1e-05, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='deit_tiny_patch16_224', model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, momentum=0.9, num_workers=10, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='output', patience_epochs=10, pin_mem=True, recount=1, remode='pixel', repeated_aug=True, reprob=0.25, resplit=False, resume='', sched='cosine', seed=0, smoothing=0.1, start_epoch=0, teacher_model='regnety_160', teacher_path='', train_interpolation='bicubic', warmup_epochs=5, warmup_lr=1e-06, weight_decay=0.05, world_size=1)\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Creating model: deit_tiny_patch16_224\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\" to /root/.cache/torch/hub/checkpoints/deit_tiny_patch16_224-a1311bcf.pth\n",
            "100% 21.9M/21.9M [00:02<00:00, 10.8MB/s]\n",
            "Removing key head.weight from pretrained checkpoint\n",
            "Removing key head.bias from pretrained checkpoint\n",
            "number of params: 5543716\n",
            "Start training for 13 epochs\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [0]  [  0/780]  eta: 0:23:57  lr: 0.000001  loss: 4.7974 (4.7974)  time: 1.8434  data: 1.3502  max mem: 1857\n",
            "Epoch: [0]  [ 10/780]  eta: 0:03:20  lr: 0.000001  loss: 4.7251 (4.7076)  time: 0.2606  data: 0.1229  max mem: 1920\n",
            "Epoch: [0]  [ 20/780]  eta: 0:02:21  lr: 0.000001  loss: 4.7100 (4.7092)  time: 0.1031  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [ 30/780]  eta: 0:01:58  lr: 0.000001  loss: 4.7073 (4.7030)  time: 0.1026  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [ 40/780]  eta: 0:01:46  lr: 0.000001  loss: 4.7035 (4.6977)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [ 50/780]  eta: 0:01:38  lr: 0.000001  loss: 4.6863 (4.6952)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [ 60/780]  eta: 0:01:32  lr: 0.000001  loss: 4.6863 (4.6951)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [ 70/780]  eta: 0:01:28  lr: 0.000001  loss: 4.6829 (4.6933)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [ 80/780]  eta: 0:01:24  lr: 0.000001  loss: 4.7039 (4.6957)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [ 90/780]  eta: 0:01:22  lr: 0.000001  loss: 4.7039 (4.6944)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [100/780]  eta: 0:01:19  lr: 0.000001  loss: 4.7069 (4.6968)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [110/780]  eta: 0:01:17  lr: 0.000001  loss: 4.6772 (4.6945)  time: 0.0967  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [120/780]  eta: 0:01:15  lr: 0.000001  loss: 4.6532 (4.6908)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [130/780]  eta: 0:01:13  lr: 0.000001  loss: 4.6552 (4.6881)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [140/780]  eta: 0:01:11  lr: 0.000001  loss: 4.6867 (4.6879)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [150/780]  eta: 0:01:09  lr: 0.000001  loss: 4.7051 (4.6880)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [160/780]  eta: 0:01:08  lr: 0.000001  loss: 4.6718 (4.6865)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [170/780]  eta: 0:01:06  lr: 0.000001  loss: 4.6718 (4.6847)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [180/780]  eta: 0:01:04  lr: 0.000001  loss: 4.6652 (4.6839)  time: 0.0949  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [190/780]  eta: 0:01:03  lr: 0.000001  loss: 4.6765 (4.6830)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [200/780]  eta: 0:01:02  lr: 0.000001  loss: 4.6866 (4.6836)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [210/780]  eta: 0:01:00  lr: 0.000001  loss: 4.6590 (4.6816)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [220/780]  eta: 0:00:59  lr: 0.000001  loss: 4.6389 (4.6799)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [230/780]  eta: 0:00:58  lr: 0.000001  loss: 4.6444 (4.6788)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [240/780]  eta: 0:00:57  lr: 0.000001  loss: 4.6396 (4.6766)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [250/780]  eta: 0:00:55  lr: 0.000001  loss: 4.6396 (4.6760)  time: 0.0956  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [260/780]  eta: 0:00:54  lr: 0.000001  loss: 4.6393 (4.6745)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [270/780]  eta: 0:00:53  lr: 0.000001  loss: 4.6297 (4.6733)  time: 0.0976  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [280/780]  eta: 0:00:52  lr: 0.000001  loss: 4.6382 (4.6725)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [290/780]  eta: 0:00:51  lr: 0.000001  loss: 4.6473 (4.6712)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [300/780]  eta: 0:00:50  lr: 0.000001  loss: 4.6473 (4.6712)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [310/780]  eta: 0:00:48  lr: 0.000001  loss: 4.6658 (4.6710)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [320/780]  eta: 0:00:47  lr: 0.000001  loss: 4.6244 (4.6689)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [330/780]  eta: 0:00:46  lr: 0.000001  loss: 4.6212 (4.6681)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [340/780]  eta: 0:00:45  lr: 0.000001  loss: 4.6318 (4.6674)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [350/780]  eta: 0:00:44  lr: 0.000001  loss: 4.6302 (4.6664)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [360/780]  eta: 0:00:43  lr: 0.000001  loss: 4.6460 (4.6661)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [370/780]  eta: 0:00:42  lr: 0.000001  loss: 4.6419 (4.6654)  time: 0.0966  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [380/780]  eta: 0:00:41  lr: 0.000001  loss: 4.6391 (4.6646)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [390/780]  eta: 0:00:40  lr: 0.000001  loss: 4.6247 (4.6632)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [400/780]  eta: 0:00:39  lr: 0.000001  loss: 4.6272 (4.6625)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [410/780]  eta: 0:00:38  lr: 0.000001  loss: 4.6313 (4.6613)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [420/780]  eta: 0:00:36  lr: 0.000001  loss: 4.6320 (4.6613)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [430/780]  eta: 0:00:35  lr: 0.000001  loss: 4.6431 (4.6604)  time: 0.0959  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [440/780]  eta: 0:00:34  lr: 0.000001  loss: 4.6259 (4.6597)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [450/780]  eta: 0:00:33  lr: 0.000001  loss: 4.6340 (4.6593)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [460/780]  eta: 0:00:32  lr: 0.000001  loss: 4.6348 (4.6587)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [470/780]  eta: 0:00:31  lr: 0.000001  loss: 4.6429 (4.6586)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [480/780]  eta: 0:00:30  lr: 0.000001  loss: 4.6392 (4.6582)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [490/780]  eta: 0:00:29  lr: 0.000001  loss: 4.6297 (4.6579)  time: 0.0966  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [500/780]  eta: 0:00:28  lr: 0.000001  loss: 4.6369 (4.6574)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [510/780]  eta: 0:00:27  lr: 0.000001  loss: 4.6482 (4.6573)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [520/780]  eta: 0:00:26  lr: 0.000001  loss: 4.6329 (4.6567)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [530/780]  eta: 0:00:25  lr: 0.000001  loss: 4.6272 (4.6562)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [540/780]  eta: 0:00:24  lr: 0.000001  loss: 4.6249 (4.6556)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [550/780]  eta: 0:00:23  lr: 0.000001  loss: 4.6161 (4.6549)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [560/780]  eta: 0:00:22  lr: 0.000001  loss: 4.6318 (4.6544)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [570/780]  eta: 0:00:21  lr: 0.000001  loss: 4.6086 (4.6536)  time: 0.0963  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [580/780]  eta: 0:00:20  lr: 0.000001  loss: 4.6190 (4.6532)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [590/780]  eta: 0:00:19  lr: 0.000001  loss: 4.6190 (4.6525)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [600/780]  eta: 0:00:18  lr: 0.000001  loss: 4.6182 (4.6519)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [610/780]  eta: 0:00:17  lr: 0.000001  loss: 4.6160 (4.6511)  time: 0.0956  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [620/780]  eta: 0:00:16  lr: 0.000001  loss: 4.6048 (4.6504)  time: 0.0962  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [630/780]  eta: 0:00:15  lr: 0.000001  loss: 4.6076 (4.6499)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [640/780]  eta: 0:00:14  lr: 0.000001  loss: 4.6076 (4.6492)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [650/780]  eta: 0:00:13  lr: 0.000001  loss: 4.6059 (4.6486)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [660/780]  eta: 0:00:12  lr: 0.000001  loss: 4.6296 (4.6485)  time: 0.0965  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [670/780]  eta: 0:00:11  lr: 0.000001  loss: 4.6330 (4.6480)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [680/780]  eta: 0:00:10  lr: 0.000001  loss: 4.6058 (4.6473)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [690/780]  eta: 0:00:09  lr: 0.000001  loss: 4.6014 (4.6469)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [700/780]  eta: 0:00:08  lr: 0.000001  loss: 4.6266 (4.6465)  time: 0.1014  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [710/780]  eta: 0:00:07  lr: 0.000001  loss: 4.6050 (4.6459)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [720/780]  eta: 0:00:06  lr: 0.000001  loss: 4.6050 (4.6454)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [730/780]  eta: 0:00:05  lr: 0.000001  loss: 4.6196 (4.6451)  time: 0.1011  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [740/780]  eta: 0:00:04  lr: 0.000001  loss: 4.6202 (4.6448)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [750/780]  eta: 0:00:03  lr: 0.000001  loss: 4.6230 (4.6443)  time: 0.1013  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [760/780]  eta: 0:00:02  lr: 0.000001  loss: 4.5993 (4.6437)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [770/780]  eta: 0:00:01  lr: 0.000001  loss: 4.5993 (4.6433)  time: 0.0955  data: 0.0002  max mem: 1920\n",
            "Epoch: [0]  [779/780]  eta: 0:00:00  lr: 0.000001  loss: 4.5994 (4.6430)  time: 0.0935  data: 0.0001  max mem: 1920\n",
            "Epoch: [0] Total time: 0:01:18 (0.1009 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 4.5994 (4.6430)\n",
            "Test:  [  0/105]  eta: 0:01:46  loss: 4.6052 (4.6052)  acc1: 0.0000 (0.0000)  acc5: 5.2083 (5.2083)  time: 1.0169  data: 0.7098  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:15  loss: 4.6052 (4.6114)  acc1: 1.0417 (0.8523)  acc5: 7.2917 (7.7652)  time: 0.1644  data: 0.0878  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:09  loss: 4.6083 (4.6120)  acc1: 1.0417 (0.7937)  acc5: 6.2500 (6.2500)  time: 0.0648  data: 0.0131  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 4.6123 (4.6102)  acc1: 0.0000 (0.7392)  acc5: 4.1667 (5.9140)  time: 0.0504  data: 0.0008  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:05  loss: 4.6051 (4.6079)  acc1: 1.0417 (0.9909)  acc5: 5.2083 (6.3516)  time: 0.0545  data: 0.0024  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:04  loss: 4.5983 (4.6020)  acc1: 1.0417 (1.1642)  acc5: 8.3333 (6.6585)  time: 0.0526  data: 0.0036  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 4.5916 (4.6024)  acc1: 1.0417 (1.1100)  acc5: 6.2500 (6.4720)  time: 0.0506  data: 0.0025  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 4.6023 (4.6029)  acc1: 1.0417 (1.1444)  acc5: 5.2083 (6.4261)  time: 0.0547  data: 0.0021  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 4.6103 (4.6047)  acc1: 1.0417 (1.1831)  acc5: 5.2083 (6.1600)  time: 0.0523  data: 0.0038  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 4.6028 (4.6037)  acc1: 1.0417 (1.1332)  acc5: 4.1667 (6.1126)  time: 0.0492  data: 0.0036  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 4.6000 (4.6033)  acc1: 1.0417 (1.1964)  acc5: 5.2083 (6.1056)  time: 0.0422  data: 0.0012  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 4.6028 (4.6038)  acc1: 1.0417 (1.2300)  acc5: 6.2500 (6.1700)  time: 0.0376  data: 0.0001  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0624 s / it)\n",
            "* Acc@1 1.230 Acc@5 6.170 loss 4.604\n",
            "Accuracy of the network on the 10000 test images: 1.2%\n",
            "Max accuracy: 1.23%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [1]  [  0/780]  eta: 0:14:01  lr: 0.000001  loss: 4.6565 (4.6565)  time: 1.0786  data: 0.7666  max mem: 1920\n",
            "Epoch: [1]  [ 10/780]  eta: 0:02:36  lr: 0.000001  loss: 4.6277 (4.6197)  time: 0.2028  data: 0.0755  max mem: 1920\n",
            "Epoch: [1]  [ 20/780]  eta: 0:01:58  lr: 0.000001  loss: 4.6224 (4.6151)  time: 0.1093  data: 0.0033  max mem: 1920\n",
            "Epoch: [1]  [ 30/780]  eta: 0:01:43  lr: 0.000001  loss: 4.5969 (4.6093)  time: 0.1022  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [ 40/780]  eta: 0:01:35  lr: 0.000001  loss: 4.5972 (4.6095)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [ 50/780]  eta: 0:01:29  lr: 0.000001  loss: 4.6178 (4.6116)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [ 60/780]  eta: 0:01:25  lr: 0.000001  loss: 4.6076 (4.6098)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [ 70/780]  eta: 0:01:22  lr: 0.000001  loss: 4.5994 (4.6087)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [ 80/780]  eta: 0:01:19  lr: 0.000001  loss: 4.6045 (4.6092)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [ 90/780]  eta: 0:01:17  lr: 0.000001  loss: 4.6031 (4.6086)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [100/780]  eta: 0:01:15  lr: 0.000001  loss: 4.6165 (4.6104)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [110/780]  eta: 0:01:13  lr: 0.000001  loss: 4.6223 (4.6104)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [120/780]  eta: 0:01:12  lr: 0.000001  loss: 4.5979 (4.6094)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [130/780]  eta: 0:01:10  lr: 0.000001  loss: 4.5996 (4.6087)  time: 0.1024  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [140/780]  eta: 0:01:09  lr: 0.000001  loss: 4.6038 (4.6089)  time: 0.1015  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [150/780]  eta: 0:01:07  lr: 0.000001  loss: 4.6119 (4.6100)  time: 0.1017  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [160/780]  eta: 0:01:06  lr: 0.000001  loss: 4.6100 (4.6093)  time: 0.1011  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [170/780]  eta: 0:01:05  lr: 0.000001  loss: 4.5976 (4.6089)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [180/780]  eta: 0:01:03  lr: 0.000001  loss: 4.6080 (4.6092)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [190/780]  eta: 0:01:02  lr: 0.000001  loss: 4.6203 (4.6099)  time: 0.0959  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [200/780]  eta: 0:01:00  lr: 0.000001  loss: 4.6264 (4.6107)  time: 0.0945  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [210/780]  eta: 0:00:59  lr: 0.000001  loss: 4.6060 (4.6106)  time: 0.0956  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [220/780]  eta: 0:00:58  lr: 0.000001  loss: 4.6014 (4.6098)  time: 0.1014  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [230/780]  eta: 0:00:57  lr: 0.000001  loss: 4.6083 (4.6097)  time: 0.1042  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [240/780]  eta: 0:00:56  lr: 0.000001  loss: 4.6040 (4.6091)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [250/780]  eta: 0:00:55  lr: 0.000001  loss: 4.6040 (4.6091)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [260/780]  eta: 0:00:53  lr: 0.000001  loss: 4.6136 (4.6091)  time: 0.0989  data: 0.0005  max mem: 1920\n",
            "Epoch: [1]  [270/780]  eta: 0:00:52  lr: 0.000001  loss: 4.5944 (4.6082)  time: 0.0995  data: 0.0005  max mem: 1920\n",
            "Epoch: [1]  [280/780]  eta: 0:00:51  lr: 0.000001  loss: 4.6019 (4.6085)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [290/780]  eta: 0:00:50  lr: 0.000001  loss: 4.6082 (4.6083)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [300/780]  eta: 0:00:49  lr: 0.000001  loss: 4.6082 (4.6086)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [310/780]  eta: 0:00:48  lr: 0.000001  loss: 4.6151 (4.6087)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [320/780]  eta: 0:00:47  lr: 0.000001  loss: 4.5961 (4.6078)  time: 0.1007  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [330/780]  eta: 0:00:46  lr: 0.000001  loss: 4.5892 (4.6077)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [340/780]  eta: 0:00:45  lr: 0.000001  loss: 4.6007 (4.6076)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [350/780]  eta: 0:00:44  lr: 0.000001  loss: 4.6007 (4.6074)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [360/780]  eta: 0:00:42  lr: 0.000001  loss: 4.6037 (4.6073)  time: 0.0958  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [370/780]  eta: 0:00:41  lr: 0.000001  loss: 4.6028 (4.6072)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [380/780]  eta: 0:00:40  lr: 0.000001  loss: 4.5947 (4.6068)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [390/780]  eta: 0:00:39  lr: 0.000001  loss: 4.5992 (4.6065)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [400/780]  eta: 0:00:38  lr: 0.000001  loss: 4.5960 (4.6062)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [410/780]  eta: 0:00:37  lr: 0.000001  loss: 4.5994 (4.6060)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [420/780]  eta: 0:00:36  lr: 0.000001  loss: 4.6034 (4.6058)  time: 0.0967  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [430/780]  eta: 0:00:35  lr: 0.000001  loss: 4.5979 (4.6056)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [440/780]  eta: 0:00:34  lr: 0.000001  loss: 4.5943 (4.6053)  time: 0.0963  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [450/780]  eta: 0:00:33  lr: 0.000001  loss: 4.5940 (4.6054)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [460/780]  eta: 0:00:32  lr: 0.000001  loss: 4.5957 (4.6053)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [470/780]  eta: 0:00:31  lr: 0.000001  loss: 4.6022 (4.6055)  time: 0.0960  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [480/780]  eta: 0:00:30  lr: 0.000001  loss: 4.6035 (4.6054)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [490/780]  eta: 0:00:29  lr: 0.000001  loss: 4.5996 (4.6054)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [500/780]  eta: 0:00:28  lr: 0.000001  loss: 4.6042 (4.6052)  time: 0.1010  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [510/780]  eta: 0:00:27  lr: 0.000001  loss: 4.6053 (4.6054)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [520/780]  eta: 0:00:26  lr: 0.000001  loss: 4.6017 (4.6054)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [530/780]  eta: 0:00:25  lr: 0.000001  loss: 4.5978 (4.6053)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [540/780]  eta: 0:00:24  lr: 0.000001  loss: 4.6004 (4.6051)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [550/780]  eta: 0:00:23  lr: 0.000001  loss: 4.5998 (4.6050)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [560/780]  eta: 0:00:22  lr: 0.000001  loss: 4.5966 (4.6048)  time: 0.0969  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [570/780]  eta: 0:00:21  lr: 0.000001  loss: 4.5781 (4.6043)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [580/780]  eta: 0:00:20  lr: 0.000001  loss: 4.5994 (4.6044)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [590/780]  eta: 0:00:19  lr: 0.000001  loss: 4.6040 (4.6043)  time: 0.1022  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [600/780]  eta: 0:00:18  lr: 0.000001  loss: 4.5982 (4.6040)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [610/780]  eta: 0:00:17  lr: 0.000001  loss: 4.5786 (4.6036)  time: 0.0946  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [620/780]  eta: 0:00:16  lr: 0.000001  loss: 4.5835 (4.6034)  time: 0.0962  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [630/780]  eta: 0:00:15  lr: 0.000001  loss: 4.5881 (4.6034)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [640/780]  eta: 0:00:14  lr: 0.000001  loss: 4.6019 (4.6033)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [650/780]  eta: 0:00:13  lr: 0.000001  loss: 4.5931 (4.6030)  time: 0.1007  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [660/780]  eta: 0:00:12  lr: 0.000001  loss: 4.5949 (4.6029)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [670/780]  eta: 0:00:11  lr: 0.000001  loss: 4.6002 (4.6029)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [680/780]  eta: 0:00:10  lr: 0.000001  loss: 4.6002 (4.6028)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [690/780]  eta: 0:00:09  lr: 0.000001  loss: 4.6055 (4.6027)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [700/780]  eta: 0:00:08  lr: 0.000001  loss: 4.5933 (4.6024)  time: 0.1031  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [710/780]  eta: 0:00:07  lr: 0.000001  loss: 4.5926 (4.6025)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [720/780]  eta: 0:00:06  lr: 0.000001  loss: 4.5926 (4.6022)  time: 0.0966  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [730/780]  eta: 0:00:05  lr: 0.000001  loss: 4.5956 (4.6022)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [740/780]  eta: 0:00:04  lr: 0.000001  loss: 4.6048 (4.6022)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [750/780]  eta: 0:00:03  lr: 0.000001  loss: 4.5947 (4.6020)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [760/780]  eta: 0:00:02  lr: 0.000001  loss: 4.5947 (4.6019)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [1]  [770/780]  eta: 0:00:01  lr: 0.000001  loss: 4.5898 (4.6017)  time: 0.0964  data: 0.0001  max mem: 1920\n",
            "Epoch: [1]  [779/780]  eta: 0:00:00  lr: 0.000001  loss: 4.5919 (4.6018)  time: 0.0944  data: 0.0001  max mem: 1920\n",
            "Epoch: [1] Total time: 0:01:18 (0.1003 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 4.5919 (4.6018)\n",
            "Test:  [  0/105]  eta: 0:01:35  loss: 4.5468 (4.5468)  acc1: 0.0000 (0.0000)  acc5: 7.2917 (7.2917)  time: 0.9058  data: 0.8206  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:12  loss: 4.5608 (4.5624)  acc1: 2.0833 (1.7992)  acc5: 10.4167 (10.0379)  time: 0.1354  data: 0.0857  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 4.5608 (4.5623)  acc1: 1.0417 (1.6369)  acc5: 9.3750 (8.8294)  time: 0.0573  data: 0.0087  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 4.5608 (4.5601)  acc1: 1.0417 (1.6465)  acc5: 7.2917 (8.4341)  time: 0.0549  data: 0.0040  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:04  loss: 4.5654 (4.5586)  acc1: 1.0417 (1.8547)  acc5: 8.3333 (8.8161)  time: 0.0529  data: 0.0051  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:03  loss: 4.5525 (4.5530)  acc1: 2.0833 (1.9608)  acc5: 10.4167 (9.1708)  time: 0.0537  data: 0.0067  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 4.5435 (4.5534)  acc1: 2.0833 (1.8443)  acc5: 9.3750 (9.2384)  time: 0.0536  data: 0.0059  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 4.5557 (4.5546)  acc1: 1.0417 (1.7606)  acc5: 8.3333 (9.0962)  time: 0.0520  data: 0.0054  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 4.5612 (4.5567)  acc1: 1.0417 (1.7490)  acc5: 6.2500 (8.7706)  time: 0.0530  data: 0.0055  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 4.5561 (4.5555)  acc1: 1.0417 (1.7285)  acc5: 6.2500 (8.6653)  time: 0.0468  data: 0.0030  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 4.5487 (4.5550)  acc1: 1.0417 (1.6811)  acc5: 8.3333 (8.6531)  time: 0.0377  data: 0.0002  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 4.5508 (4.5555)  acc1: 1.0417 (1.7000)  acc5: 8.3333 (8.6800)  time: 0.0354  data: 0.0001  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0592 s / it)\n",
            "* Acc@1 1.700 Acc@5 8.680 loss 4.556\n",
            "Accuracy of the network on the 10000 test images: 1.7%\n",
            "Max accuracy: 1.70%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [2]  [  0/780]  eta: 0:12:23  lr: 0.000013  loss: 4.6227 (4.6227)  time: 0.9532  data: 0.7381  max mem: 1920\n",
            "Epoch: [2]  [ 10/780]  eta: 0:02:34  lr: 0.000013  loss: 4.5971 (4.5910)  time: 0.2006  data: 0.0673  max mem: 1920\n",
            "Epoch: [2]  [ 20/780]  eta: 0:01:55  lr: 0.000013  loss: 4.5951 (4.5903)  time: 0.1123  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [ 30/780]  eta: 0:01:41  lr: 0.000013  loss: 4.5907 (4.5888)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [ 40/780]  eta: 0:01:33  lr: 0.000013  loss: 4.5857 (4.5861)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [ 50/780]  eta: 0:01:28  lr: 0.000013  loss: 4.5763 (4.5834)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [ 60/780]  eta: 0:01:24  lr: 0.000013  loss: 4.5620 (4.5791)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [ 70/780]  eta: 0:01:21  lr: 0.000013  loss: 4.5557 (4.5772)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [ 80/780]  eta: 0:01:18  lr: 0.000013  loss: 4.5779 (4.5777)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [ 90/780]  eta: 0:01:16  lr: 0.000013  loss: 4.5638 (4.5738)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [100/780]  eta: 0:01:14  lr: 0.000013  loss: 4.5437 (4.5712)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [110/780]  eta: 0:01:12  lr: 0.000013  loss: 4.5476 (4.5699)  time: 0.0962  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [120/780]  eta: 0:01:10  lr: 0.000013  loss: 4.5440 (4.5660)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [130/780]  eta: 0:01:09  lr: 0.000013  loss: 4.5440 (4.5630)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [140/780]  eta: 0:01:07  lr: 0.000013  loss: 4.5464 (4.5613)  time: 0.0947  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [150/780]  eta: 0:01:06  lr: 0.000013  loss: 4.5496 (4.5602)  time: 0.0942  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [160/780]  eta: 0:01:04  lr: 0.000013  loss: 4.5496 (4.5570)  time: 0.0945  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [170/780]  eta: 0:01:03  lr: 0.000013  loss: 4.5061 (4.5551)  time: 0.0959  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [180/780]  eta: 0:01:02  lr: 0.000013  loss: 4.5136 (4.5525)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [190/780]  eta: 0:01:00  lr: 0.000013  loss: 4.5038 (4.5494)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [200/780]  eta: 0:00:59  lr: 0.000013  loss: 4.4966 (4.5460)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [210/780]  eta: 0:00:58  lr: 0.000013  loss: 4.5015 (4.5441)  time: 0.1013  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [220/780]  eta: 0:00:57  lr: 0.000013  loss: 4.5054 (4.5427)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [230/780]  eta: 0:00:56  lr: 0.000013  loss: 4.4836 (4.5381)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [240/780]  eta: 0:00:55  lr: 0.000013  loss: 4.4297 (4.5336)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [250/780]  eta: 0:00:54  lr: 0.000013  loss: 4.4158 (4.5283)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [260/780]  eta: 0:00:53  lr: 0.000013  loss: 4.4420 (4.5256)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [270/780]  eta: 0:00:52  lr: 0.000013  loss: 4.4296 (4.5208)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [280/780]  eta: 0:00:51  lr: 0.000013  loss: 4.3941 (4.5173)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [290/780]  eta: 0:00:49  lr: 0.000013  loss: 4.3745 (4.5119)  time: 0.0967  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [300/780]  eta: 0:00:48  lr: 0.000013  loss: 4.3745 (4.5077)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [310/780]  eta: 0:00:47  lr: 0.000013  loss: 4.3680 (4.5044)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [320/780]  eta: 0:00:46  lr: 0.000013  loss: 4.3680 (4.5011)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [330/780]  eta: 0:00:45  lr: 0.000013  loss: 4.3792 (4.4960)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [340/780]  eta: 0:00:44  lr: 0.000013  loss: 4.3436 (4.4904)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [350/780]  eta: 0:00:43  lr: 0.000013  loss: 4.3387 (4.4874)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [360/780]  eta: 0:00:42  lr: 0.000013  loss: 4.3042 (4.4808)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [370/780]  eta: 0:00:41  lr: 0.000013  loss: 4.2836 (4.4758)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [380/780]  eta: 0:00:40  lr: 0.000013  loss: 4.2822 (4.4700)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [390/780]  eta: 0:00:39  lr: 0.000013  loss: 4.2158 (4.4641)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [400/780]  eta: 0:00:38  lr: 0.000013  loss: 4.2158 (4.4587)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [410/780]  eta: 0:00:37  lr: 0.000013  loss: 4.2186 (4.4530)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [420/780]  eta: 0:00:36  lr: 0.000013  loss: 4.2113 (4.4476)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [430/780]  eta: 0:00:35  lr: 0.000013  loss: 4.2418 (4.4428)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [440/780]  eta: 0:00:34  lr: 0.000013  loss: 4.2872 (4.4382)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [450/780]  eta: 0:00:33  lr: 0.000013  loss: 4.2923 (4.4354)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [460/780]  eta: 0:00:32  lr: 0.000013  loss: 4.2511 (4.4303)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [470/780]  eta: 0:00:31  lr: 0.000013  loss: 4.2467 (4.4272)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [480/780]  eta: 0:00:30  lr: 0.000013  loss: 4.2870 (4.4241)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [490/780]  eta: 0:00:29  lr: 0.000013  loss: 4.2447 (4.4189)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [500/780]  eta: 0:00:28  lr: 0.000013  loss: 4.1361 (4.4125)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [510/780]  eta: 0:00:27  lr: 0.000013  loss: 4.2294 (4.4090)  time: 0.0960  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [520/780]  eta: 0:00:26  lr: 0.000013  loss: 4.2294 (4.4047)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [530/780]  eta: 0:00:25  lr: 0.000013  loss: 4.2242 (4.4021)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [540/780]  eta: 0:00:24  lr: 0.000013  loss: 4.2304 (4.3982)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [550/780]  eta: 0:00:23  lr: 0.000013  loss: 4.2157 (4.3950)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [560/780]  eta: 0:00:22  lr: 0.000013  loss: 4.1926 (4.3897)  time: 0.0965  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [570/780]  eta: 0:00:21  lr: 0.000013  loss: 4.0946 (4.3859)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [580/780]  eta: 0:00:20  lr: 0.000013  loss: 4.2121 (4.3825)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [590/780]  eta: 0:00:19  lr: 0.000013  loss: 4.2177 (4.3781)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [600/780]  eta: 0:00:18  lr: 0.000013  loss: 4.2435 (4.3747)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [610/780]  eta: 0:00:17  lr: 0.000013  loss: 4.2091 (4.3716)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [620/780]  eta: 0:00:16  lr: 0.000013  loss: 4.1168 (4.3667)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [630/780]  eta: 0:00:15  lr: 0.000013  loss: 4.1786 (4.3641)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [640/780]  eta: 0:00:14  lr: 0.000013  loss: 4.1366 (4.3588)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [650/780]  eta: 0:00:13  lr: 0.000013  loss: 4.0397 (4.3552)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [660/780]  eta: 0:00:12  lr: 0.000013  loss: 4.1279 (4.3510)  time: 0.0962  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [670/780]  eta: 0:00:11  lr: 0.000013  loss: 4.0407 (4.3454)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [680/780]  eta: 0:00:10  lr: 0.000013  loss: 4.0407 (4.3414)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [690/780]  eta: 0:00:08  lr: 0.000013  loss: 4.0614 (4.3369)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [700/780]  eta: 0:00:07  lr: 0.000013  loss: 4.0102 (4.3319)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [710/780]  eta: 0:00:06  lr: 0.000013  loss: 4.0195 (4.3284)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [720/780]  eta: 0:00:05  lr: 0.000013  loss: 4.1385 (4.3260)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [730/780]  eta: 0:00:04  lr: 0.000013  loss: 4.1048 (4.3211)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [740/780]  eta: 0:00:03  lr: 0.000013  loss: 3.9865 (4.3163)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [750/780]  eta: 0:00:02  lr: 0.000013  loss: 3.9849 (4.3121)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [760/780]  eta: 0:00:01  lr: 0.000013  loss: 4.0364 (4.3090)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [770/780]  eta: 0:00:00  lr: 0.000013  loss: 4.1200 (4.3063)  time: 0.0967  data: 0.0002  max mem: 1920\n",
            "Epoch: [2]  [779/780]  eta: 0:00:00  lr: 0.000013  loss: 4.0531 (4.3028)  time: 0.0964  data: 0.0001  max mem: 1920\n",
            "Epoch: [2] Total time: 0:01:17 (0.1000 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 4.0531 (4.3028)\n",
            "Test:  [  0/105]  eta: 0:01:41  loss: 3.2261 (3.2261)  acc1: 38.5417 (38.5417)  acc5: 66.6667 (66.6667)  time: 0.9690  data: 0.8987  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:13  loss: 3.2573 (3.2540)  acc1: 32.2917 (33.1439)  acc5: 65.6250 (62.7841)  time: 0.1410  data: 0.0841  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 3.2578 (3.2469)  acc1: 32.2917 (32.3413)  acc5: 64.5833 (64.2361)  time: 0.0562  data: 0.0037  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 3.2619 (3.2453)  acc1: 30.2083 (32.2581)  acc5: 64.5833 (64.2137)  time: 0.0517  data: 0.0040  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:04  loss: 3.2619 (3.2432)  acc1: 31.2500 (32.5711)  acc5: 65.6250 (64.6596)  time: 0.0513  data: 0.0049  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:03  loss: 3.2552 (3.2464)  acc1: 32.2917 (32.7002)  acc5: 65.6250 (64.8693)  time: 0.0499  data: 0.0049  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 3.2297 (3.2430)  acc1: 33.3333 (32.6332)  acc5: 65.6250 (65.1810)  time: 0.0499  data: 0.0045  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 3.2287 (3.2440)  acc1: 33.3333 (32.7171)  acc5: 65.6250 (65.2142)  time: 0.0514  data: 0.0050  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 3.2330 (3.2473)  acc1: 32.2917 (32.4717)  acc5: 65.6250 (64.9306)  time: 0.0505  data: 0.0032  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 3.2330 (3.2482)  acc1: 31.2500 (32.4290)  acc5: 65.6250 (65.1328)  time: 0.0499  data: 0.0019  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 3.2260 (3.2462)  acc1: 32.2917 (32.5701)  acc5: 65.6250 (65.2228)  time: 0.0421  data: 0.0009  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 3.2260 (3.2499)  acc1: 33.3333 (32.5300)  acc5: 65.6250 (65.2500)  time: 0.0376  data: 0.0001  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0595 s / it)\n",
            "* Acc@1 32.530 Acc@5 65.250 loss 3.250\n",
            "Accuracy of the network on the 10000 test images: 32.5%\n",
            "Max accuracy: 32.53%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [3]  [  0/780]  eta: 0:14:46  lr: 0.000026  loss: 3.9671 (3.9671)  time: 1.1368  data: 0.8803  max mem: 1920\n",
            "Epoch: [3]  [ 10/780]  eta: 0:02:41  lr: 0.000026  loss: 4.0419 (4.0644)  time: 0.2092  data: 0.0832  max mem: 1920\n",
            "Epoch: [3]  [ 20/780]  eta: 0:01:59  lr: 0.000026  loss: 4.0116 (3.9952)  time: 0.1081  data: 0.0018  max mem: 1920\n",
            "Epoch: [3]  [ 30/780]  eta: 0:01:44  lr: 0.000026  loss: 4.0003 (4.0165)  time: 0.1012  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [ 40/780]  eta: 0:01:35  lr: 0.000026  loss: 4.0974 (4.0281)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [ 50/780]  eta: 0:01:29  lr: 0.000026  loss: 3.9515 (4.0042)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [ 60/780]  eta: 0:01:25  lr: 0.000026  loss: 3.9660 (4.0029)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [ 70/780]  eta: 0:01:22  lr: 0.000026  loss: 4.0236 (4.0246)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [ 80/780]  eta: 0:01:19  lr: 0.000026  loss: 4.0236 (4.0208)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [ 90/780]  eta: 0:01:17  lr: 0.000026  loss: 3.9677 (4.0169)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [100/780]  eta: 0:01:15  lr: 0.000026  loss: 3.9682 (4.0173)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [110/780]  eta: 0:01:14  lr: 0.000026  loss: 3.9789 (4.0130)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [120/780]  eta: 0:01:12  lr: 0.000026  loss: 3.9921 (4.0116)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [130/780]  eta: 0:01:10  lr: 0.000026  loss: 3.9362 (3.9961)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [140/780]  eta: 0:01:09  lr: 0.000026  loss: 3.8537 (3.9828)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [150/780]  eta: 0:01:07  lr: 0.000026  loss: 3.9679 (3.9878)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [160/780]  eta: 0:01:06  lr: 0.000026  loss: 3.9534 (3.9805)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [170/780]  eta: 0:01:04  lr: 0.000026  loss: 3.8006 (3.9712)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [180/780]  eta: 0:01:03  lr: 0.000026  loss: 3.8488 (3.9705)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [190/780]  eta: 0:01:02  lr: 0.000026  loss: 3.9833 (3.9701)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [200/780]  eta: 0:01:00  lr: 0.000026  loss: 3.9181 (3.9633)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [210/780]  eta: 0:00:59  lr: 0.000026  loss: 3.8864 (3.9641)  time: 0.0963  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [220/780]  eta: 0:00:58  lr: 0.000026  loss: 4.0477 (3.9695)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [230/780]  eta: 0:00:57  lr: 0.000026  loss: 4.0545 (3.9693)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [240/780]  eta: 0:00:56  lr: 0.000026  loss: 3.9121 (3.9602)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [250/780]  eta: 0:00:54  lr: 0.000026  loss: 3.8062 (3.9534)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [260/780]  eta: 0:00:53  lr: 0.000026  loss: 3.8401 (3.9518)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [270/780]  eta: 0:00:52  lr: 0.000026  loss: 3.8032 (3.9429)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [280/780]  eta: 0:00:51  lr: 0.000026  loss: 3.7196 (3.9425)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [290/780]  eta: 0:00:50  lr: 0.000026  loss: 3.8428 (3.9391)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [300/780]  eta: 0:00:49  lr: 0.000026  loss: 3.9453 (3.9391)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [310/780]  eta: 0:00:48  lr: 0.000026  loss: 3.9831 (3.9404)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [320/780]  eta: 0:00:47  lr: 0.000026  loss: 3.7956 (3.9350)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [330/780]  eta: 0:00:46  lr: 0.000026  loss: 3.7944 (3.9334)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [340/780]  eta: 0:00:44  lr: 0.000026  loss: 3.8908 (3.9314)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [350/780]  eta: 0:00:43  lr: 0.000026  loss: 3.8758 (3.9286)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [360/780]  eta: 0:00:42  lr: 0.000026  loss: 3.8758 (3.9264)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [370/780]  eta: 0:00:41  lr: 0.000026  loss: 3.8718 (3.9242)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [380/780]  eta: 0:00:40  lr: 0.000026  loss: 3.8300 (3.9197)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [390/780]  eta: 0:00:39  lr: 0.000026  loss: 3.8306 (3.9178)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [400/780]  eta: 0:00:38  lr: 0.000026  loss: 3.6373 (3.9107)  time: 0.0960  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [410/780]  eta: 0:00:37  lr: 0.000026  loss: 3.7868 (3.9089)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [420/780]  eta: 0:00:36  lr: 0.000026  loss: 3.7868 (3.9022)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [430/780]  eta: 0:00:35  lr: 0.000026  loss: 3.8351 (3.9039)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [440/780]  eta: 0:00:34  lr: 0.000026  loss: 3.9429 (3.9003)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [450/780]  eta: 0:00:33  lr: 0.000026  loss: 3.6103 (3.8961)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [460/780]  eta: 0:00:32  lr: 0.000026  loss: 3.7011 (3.8914)  time: 0.1007  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [470/780]  eta: 0:00:31  lr: 0.000026  loss: 3.7597 (3.8895)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [480/780]  eta: 0:00:30  lr: 0.000026  loss: 3.7597 (3.8881)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [490/780]  eta: 0:00:29  lr: 0.000026  loss: 3.6427 (3.8830)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [500/780]  eta: 0:00:28  lr: 0.000026  loss: 3.5942 (3.8776)  time: 0.0963  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [510/780]  eta: 0:00:27  lr: 0.000026  loss: 3.5942 (3.8724)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [520/780]  eta: 0:00:26  lr: 0.000026  loss: 3.5388 (3.8654)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [530/780]  eta: 0:00:25  lr: 0.000026  loss: 3.6081 (3.8627)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [540/780]  eta: 0:00:24  lr: 0.000026  loss: 3.7642 (3.8597)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [550/780]  eta: 0:00:23  lr: 0.000026  loss: 3.6449 (3.8549)  time: 0.0952  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [560/780]  eta: 0:00:22  lr: 0.000026  loss: 3.6449 (3.8534)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [570/780]  eta: 0:00:21  lr: 0.000026  loss: 3.7718 (3.8493)  time: 0.1013  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [580/780]  eta: 0:00:20  lr: 0.000026  loss: 3.5636 (3.8469)  time: 0.1020  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [590/780]  eta: 0:00:19  lr: 0.000026  loss: 3.6678 (3.8466)  time: 0.1009  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [600/780]  eta: 0:00:18  lr: 0.000026  loss: 3.8376 (3.8464)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [610/780]  eta: 0:00:17  lr: 0.000026  loss: 3.7680 (3.8415)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [620/780]  eta: 0:00:16  lr: 0.000026  loss: 3.5353 (3.8379)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [630/780]  eta: 0:00:15  lr: 0.000026  loss: 3.6780 (3.8356)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [640/780]  eta: 0:00:14  lr: 0.000026  loss: 3.7682 (3.8331)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [650/780]  eta: 0:00:13  lr: 0.000026  loss: 3.7629 (3.8317)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [660/780]  eta: 0:00:12  lr: 0.000026  loss: 3.7435 (3.8292)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [670/780]  eta: 0:00:11  lr: 0.000026  loss: 3.6600 (3.8266)  time: 0.0962  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [680/780]  eta: 0:00:10  lr: 0.000026  loss: 3.6876 (3.8250)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [690/780]  eta: 0:00:09  lr: 0.000026  loss: 3.6490 (3.8201)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [700/780]  eta: 0:00:08  lr: 0.000026  loss: 3.6432 (3.8184)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [710/780]  eta: 0:00:07  lr: 0.000026  loss: 3.7044 (3.8171)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [720/780]  eta: 0:00:06  lr: 0.000026  loss: 3.7044 (3.8154)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [730/780]  eta: 0:00:05  lr: 0.000026  loss: 3.4907 (3.8107)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [740/780]  eta: 0:00:04  lr: 0.000026  loss: 3.4785 (3.8076)  time: 0.0965  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [750/780]  eta: 0:00:03  lr: 0.000026  loss: 3.5848 (3.8050)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [760/780]  eta: 0:00:02  lr: 0.000026  loss: 3.7702 (3.8054)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [770/780]  eta: 0:00:01  lr: 0.000026  loss: 3.7643 (3.8018)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [3]  [779/780]  eta: 0:00:00  lr: 0.000026  loss: 3.5496 (3.7992)  time: 0.0943  data: 0.0001  max mem: 1920\n",
            "Epoch: [3] Total time: 0:01:18 (0.1002 s / it)\n",
            "Averaged stats: lr: 0.000026  loss: 3.5496 (3.7992)\n",
            "Test:  [  0/105]  eta: 0:01:45  loss: 2.3250 (2.3250)  acc1: 51.0417 (51.0417)  acc5: 88.5417 (88.5417)  time: 1.0037  data: 0.9576  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:14  loss: 2.3250 (2.3470)  acc1: 54.1667 (52.0833)  acc5: 82.2917 (82.8599)  time: 0.1501  data: 0.0989  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 2.3305 (2.3322)  acc1: 52.0833 (50.5456)  acc5: 84.3750 (83.5814)  time: 0.0560  data: 0.0086  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 2.3315 (2.3340)  acc1: 50.0000 (50.5376)  acc5: 84.3750 (83.7702)  time: 0.0505  data: 0.0058  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:04  loss: 2.3458 (2.3252)  acc1: 50.0000 (50.7876)  acc5: 85.4167 (84.2226)  time: 0.0502  data: 0.0058  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:03  loss: 2.3097 (2.3260)  acc1: 52.0833 (50.9600)  acc5: 85.4167 (84.3137)  time: 0.0497  data: 0.0041  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 2.2924 (2.3190)  acc1: 54.1667 (51.3490)  acc5: 85.4167 (84.4945)  time: 0.0506  data: 0.0026  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 2.2808 (2.3187)  acc1: 52.0833 (51.4085)  acc5: 84.3750 (84.3750)  time: 0.0533  data: 0.0020  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 2.3103 (2.3255)  acc1: 50.0000 (51.1574)  acc5: 84.3750 (84.2207)  time: 0.0549  data: 0.0019  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 2.3246 (2.3249)  acc1: 50.0000 (51.1103)  acc5: 83.3333 (84.2491)  time: 0.0509  data: 0.0007  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 2.2877 (2.3247)  acc1: 51.0417 (51.0932)  acc5: 83.3333 (84.2719)  time: 0.0432  data: 0.0002  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 2.2845 (2.3264)  acc1: 52.0833 (50.9900)  acc5: 82.2917 (84.1400)  time: 0.0383  data: 0.0001  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0604 s / it)\n",
            "* Acc@1 50.990 Acc@5 84.140 loss 2.326\n",
            "Accuracy of the network on the 10000 test images: 51.0%\n",
            "Max accuracy: 50.99%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [4]  [  0/780]  eta: 0:11:36  lr: 0.000038  loss: 3.2589 (3.2589)  time: 0.8933  data: 0.7670  max mem: 1920\n",
            "Epoch: [4]  [ 10/780]  eta: 0:02:32  lr: 0.000038  loss: 3.4486 (3.5585)  time: 0.1986  data: 0.0699  max mem: 1920\n",
            "Epoch: [4]  [ 20/780]  eta: 0:01:54  lr: 0.000038  loss: 3.5125 (3.5453)  time: 0.1132  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [ 30/780]  eta: 0:01:39  lr: 0.000038  loss: 3.6904 (3.5862)  time: 0.0962  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [ 40/780]  eta: 0:01:32  lr: 0.000038  loss: 3.6118 (3.5756)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [ 50/780]  eta: 0:01:26  lr: 0.000038  loss: 3.6444 (3.5848)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [ 60/780]  eta: 0:01:23  lr: 0.000038  loss: 3.6811 (3.5905)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [ 70/780]  eta: 0:01:20  lr: 0.000038  loss: 3.5615 (3.6024)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [ 80/780]  eta: 0:01:17  lr: 0.000038  loss: 3.7231 (3.6139)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [ 90/780]  eta: 0:01:15  lr: 0.000038  loss: 3.7942 (3.6141)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [100/780]  eta: 0:01:14  lr: 0.000038  loss: 3.7614 (3.6192)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [110/780]  eta: 0:01:12  lr: 0.000038  loss: 3.6269 (3.6191)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [120/780]  eta: 0:01:10  lr: 0.000038  loss: 3.8277 (3.6395)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [130/780]  eta: 0:01:09  lr: 0.000038  loss: 3.7958 (3.6292)  time: 0.1009  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [140/780]  eta: 0:01:07  lr: 0.000038  loss: 3.4806 (3.6257)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [150/780]  eta: 0:01:06  lr: 0.000038  loss: 3.5918 (3.6261)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [160/780]  eta: 0:01:05  lr: 0.000038  loss: 3.6046 (3.6262)  time: 0.1026  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [170/780]  eta: 0:01:04  lr: 0.000038  loss: 3.5117 (3.6224)  time: 0.1014  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [180/780]  eta: 0:01:03  lr: 0.000038  loss: 3.6756 (3.6215)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [190/780]  eta: 0:01:01  lr: 0.000038  loss: 3.7210 (3.6280)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [200/780]  eta: 0:01:00  lr: 0.000038  loss: 3.7878 (3.6276)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [210/780]  eta: 0:00:59  lr: 0.000038  loss: 3.4685 (3.6200)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [220/780]  eta: 0:00:58  lr: 0.000038  loss: 3.4107 (3.6141)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [230/780]  eta: 0:00:56  lr: 0.000038  loss: 3.6053 (3.6157)  time: 0.0966  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [240/780]  eta: 0:00:55  lr: 0.000038  loss: 3.6327 (3.6106)  time: 0.0965  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [250/780]  eta: 0:00:54  lr: 0.000038  loss: 3.5291 (3.6067)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [260/780]  eta: 0:00:53  lr: 0.000038  loss: 3.6935 (3.6085)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [270/780]  eta: 0:00:52  lr: 0.000038  loss: 3.7198 (3.6086)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [280/780]  eta: 0:00:51  lr: 0.000038  loss: 3.5453 (3.6021)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [290/780]  eta: 0:00:50  lr: 0.000038  loss: 3.6419 (3.6041)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [300/780]  eta: 0:00:49  lr: 0.000038  loss: 3.6980 (3.6033)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [310/780]  eta: 0:00:48  lr: 0.000038  loss: 3.4329 (3.6000)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [320/780]  eta: 0:00:46  lr: 0.000038  loss: 3.3710 (3.5916)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [330/780]  eta: 0:00:45  lr: 0.000038  loss: 3.3710 (3.5903)  time: 0.0966  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [340/780]  eta: 0:00:44  lr: 0.000038  loss: 3.6771 (3.5886)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [350/780]  eta: 0:00:43  lr: 0.000038  loss: 3.4215 (3.5826)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [360/780]  eta: 0:00:42  lr: 0.000038  loss: 3.3898 (3.5829)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [370/780]  eta: 0:00:41  lr: 0.000038  loss: 3.4303 (3.5768)  time: 0.1032  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [380/780]  eta: 0:00:40  lr: 0.000038  loss: 3.4303 (3.5712)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [390/780]  eta: 0:00:39  lr: 0.000038  loss: 3.5645 (3.5692)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [400/780]  eta: 0:00:38  lr: 0.000038  loss: 3.5201 (3.5664)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [410/780]  eta: 0:00:37  lr: 0.000038  loss: 3.4861 (3.5646)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [420/780]  eta: 0:00:36  lr: 0.000038  loss: 3.4861 (3.5619)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [430/780]  eta: 0:00:35  lr: 0.000038  loss: 3.3092 (3.5562)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [440/780]  eta: 0:00:34  lr: 0.000038  loss: 3.5842 (3.5602)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [450/780]  eta: 0:00:33  lr: 0.000038  loss: 3.6759 (3.5591)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [460/780]  eta: 0:00:32  lr: 0.000038  loss: 3.6332 (3.5613)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [470/780]  eta: 0:00:31  lr: 0.000038  loss: 3.5841 (3.5603)  time: 0.0960  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [480/780]  eta: 0:00:30  lr: 0.000038  loss: 3.4789 (3.5566)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [490/780]  eta: 0:00:29  lr: 0.000038  loss: 3.4719 (3.5548)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [500/780]  eta: 0:00:28  lr: 0.000038  loss: 3.5020 (3.5539)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [510/780]  eta: 0:00:27  lr: 0.000038  loss: 3.6023 (3.5540)  time: 0.1009  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [520/780]  eta: 0:00:26  lr: 0.000038  loss: 3.5209 (3.5506)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [530/780]  eta: 0:00:25  lr: 0.000038  loss: 3.5110 (3.5526)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [540/780]  eta: 0:00:24  lr: 0.000038  loss: 3.6684 (3.5520)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [550/780]  eta: 0:00:23  lr: 0.000038  loss: 3.6161 (3.5533)  time: 0.0969  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [560/780]  eta: 0:00:22  lr: 0.000038  loss: 3.5675 (3.5508)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [570/780]  eta: 0:00:21  lr: 0.000038  loss: 3.5675 (3.5508)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [580/780]  eta: 0:00:20  lr: 0.000038  loss: 3.6001 (3.5487)  time: 0.1016  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [590/780]  eta: 0:00:19  lr: 0.000038  loss: 3.4760 (3.5482)  time: 0.1017  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [600/780]  eta: 0:00:18  lr: 0.000038  loss: 3.4987 (3.5475)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [610/780]  eta: 0:00:17  lr: 0.000038  loss: 3.6442 (3.5475)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [620/780]  eta: 0:00:16  lr: 0.000038  loss: 3.6272 (3.5480)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [630/780]  eta: 0:00:15  lr: 0.000038  loss: 3.6072 (3.5465)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [640/780]  eta: 0:00:14  lr: 0.000038  loss: 3.5384 (3.5449)  time: 0.1011  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [650/780]  eta: 0:00:13  lr: 0.000038  loss: 3.5412 (3.5456)  time: 0.1014  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [660/780]  eta: 0:00:12  lr: 0.000038  loss: 3.3776 (3.5408)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [670/780]  eta: 0:00:11  lr: 0.000038  loss: 3.3082 (3.5401)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [680/780]  eta: 0:00:10  lr: 0.000038  loss: 3.4666 (3.5388)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [690/780]  eta: 0:00:09  lr: 0.000038  loss: 3.4058 (3.5357)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [700/780]  eta: 0:00:08  lr: 0.000038  loss: 3.2521 (3.5310)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [710/780]  eta: 0:00:07  lr: 0.000038  loss: 3.4355 (3.5316)  time: 0.0954  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [720/780]  eta: 0:00:06  lr: 0.000038  loss: 3.4780 (3.5290)  time: 0.0976  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [730/780]  eta: 0:00:05  lr: 0.000038  loss: 3.3909 (3.5287)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [740/780]  eta: 0:00:04  lr: 0.000038  loss: 3.4452 (3.5257)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [750/780]  eta: 0:00:03  lr: 0.000038  loss: 3.4169 (3.5239)  time: 0.0976  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [760/780]  eta: 0:00:02  lr: 0.000038  loss: 3.4169 (3.5213)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [770/780]  eta: 0:00:01  lr: 0.000038  loss: 3.4047 (3.5194)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [4]  [779/780]  eta: 0:00:00  lr: 0.000038  loss: 3.3474 (3.5150)  time: 0.0969  data: 0.0001  max mem: 1920\n",
            "Epoch: [4] Total time: 0:01:18 (0.1005 s / it)\n",
            "Averaged stats: lr: 0.000038  loss: 3.3474 (3.5150)\n",
            "Test:  [  0/105]  eta: 0:01:47  loss: 1.8237 (1.8237)  acc1: 57.2917 (57.2917)  acc5: 91.6667 (91.6667)  time: 1.0229  data: 0.9656  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:13  loss: 1.8237 (1.8453)  acc1: 61.4583 (60.7955)  acc5: 89.5833 (88.3523)  time: 0.1370  data: 0.0886  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 1.8114 (1.8238)  acc1: 59.3750 (60.5655)  acc5: 89.5833 (89.1369)  time: 0.0530  data: 0.0058  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 1.8354 (1.8298)  acc1: 58.3333 (59.7782)  acc5: 89.5833 (89.0121)  time: 0.0544  data: 0.0067  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:04  loss: 1.8354 (1.8208)  acc1: 58.3333 (59.8323)  acc5: 89.5833 (89.2276)  time: 0.0512  data: 0.0046  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:03  loss: 1.8161 (1.8225)  acc1: 61.4583 (60.1716)  acc5: 89.5833 (89.2974)  time: 0.0509  data: 0.0034  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 1.7710 (1.8122)  acc1: 61.4583 (60.5021)  acc5: 89.5833 (89.3955)  time: 0.0547  data: 0.0029  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 1.7671 (1.8144)  acc1: 61.4583 (60.4460)  acc5: 89.5833 (89.3339)  time: 0.0557  data: 0.0039  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 1.8219 (1.8200)  acc1: 59.3750 (60.3266)  acc5: 88.5417 (89.2618)  time: 0.0508  data: 0.0017  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 1.8336 (1.8186)  acc1: 59.3750 (60.3365)  acc5: 89.5833 (89.3544)  time: 0.0492  data: 0.0027  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 1.7907 (1.8180)  acc1: 59.3750 (60.2207)  acc5: 89.5833 (89.3874)  time: 0.0428  data: 0.0022  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 1.7575 (1.8179)  acc1: 61.4583 (60.1700)  acc5: 89.5833 (89.3400)  time: 0.0378  data: 0.0009  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0603 s / it)\n",
            "* Acc@1 60.170 Acc@5 89.340 loss 1.818\n",
            "Accuracy of the network on the 10000 test images: 60.2%\n",
            "Max accuracy: 60.17%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [5]  [  0/780]  eta: 0:14:28  lr: 0.000050  loss: 3.2074 (3.2074)  time: 1.1130  data: 0.9005  max mem: 1920\n",
            "Epoch: [5]  [ 10/780]  eta: 0:02:34  lr: 0.000050  loss: 3.4538 (3.4259)  time: 0.2005  data: 0.0831  max mem: 1920\n",
            "Epoch: [5]  [ 20/780]  eta: 0:01:54  lr: 0.000050  loss: 3.5542 (3.4343)  time: 0.1029  data: 0.0008  max mem: 1920\n",
            "Epoch: [5]  [ 30/780]  eta: 0:01:41  lr: 0.000050  loss: 3.5004 (3.3985)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [ 40/780]  eta: 0:01:32  lr: 0.000050  loss: 3.2813 (3.3652)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [ 50/780]  eta: 0:01:27  lr: 0.000050  loss: 3.3169 (3.3510)  time: 0.0955  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [ 60/780]  eta: 0:01:23  lr: 0.000050  loss: 3.4060 (3.3611)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [ 70/780]  eta: 0:01:21  lr: 0.000050  loss: 3.6797 (3.4105)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [ 80/780]  eta: 0:01:18  lr: 0.000050  loss: 3.6797 (3.4174)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [ 90/780]  eta: 0:01:16  lr: 0.000050  loss: 3.5070 (3.4196)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [100/780]  eta: 0:01:14  lr: 0.000050  loss: 3.4270 (3.4121)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [110/780]  eta: 0:01:12  lr: 0.000050  loss: 3.3148 (3.4059)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [120/780]  eta: 0:01:11  lr: 0.000050  loss: 3.4629 (3.4066)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [130/780]  eta: 0:01:09  lr: 0.000050  loss: 3.2839 (3.3996)  time: 0.0952  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [140/780]  eta: 0:01:07  lr: 0.000050  loss: 3.4051 (3.4047)  time: 0.0945  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [150/780]  eta: 0:01:06  lr: 0.000050  loss: 3.5658 (3.4192)  time: 0.0966  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [160/780]  eta: 0:01:05  lr: 0.000050  loss: 3.5510 (3.4248)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [170/780]  eta: 0:01:03  lr: 0.000050  loss: 3.4362 (3.4131)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [180/780]  eta: 0:01:02  lr: 0.000050  loss: 3.2001 (3.4076)  time: 0.0976  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [190/780]  eta: 0:01:01  lr: 0.000050  loss: 3.3678 (3.4009)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [200/780]  eta: 0:01:00  lr: 0.000050  loss: 3.3703 (3.3956)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [210/780]  eta: 0:00:58  lr: 0.000050  loss: 3.4518 (3.3957)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [220/780]  eta: 0:00:57  lr: 0.000050  loss: 3.3910 (3.3904)  time: 0.0965  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [230/780]  eta: 0:00:56  lr: 0.000050  loss: 3.4194 (3.3959)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [240/780]  eta: 0:00:55  lr: 0.000050  loss: 3.3884 (3.3887)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [250/780]  eta: 0:00:54  lr: 0.000050  loss: 3.3097 (3.3832)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [260/780]  eta: 0:00:53  lr: 0.000050  loss: 3.2405 (3.3808)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [270/780]  eta: 0:00:52  lr: 0.000050  loss: 3.3617 (3.3797)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [280/780]  eta: 0:00:51  lr: 0.000050  loss: 3.3766 (3.3864)  time: 0.0961  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [290/780]  eta: 0:00:49  lr: 0.000050  loss: 3.5352 (3.3866)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [300/780]  eta: 0:00:48  lr: 0.000050  loss: 3.1987 (3.3815)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [310/780]  eta: 0:00:47  lr: 0.000050  loss: 3.3581 (3.3874)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [320/780]  eta: 0:00:46  lr: 0.000050  loss: 3.4356 (3.3867)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [330/780]  eta: 0:00:45  lr: 0.000050  loss: 3.2536 (3.3786)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [340/780]  eta: 0:00:44  lr: 0.000050  loss: 3.2549 (3.3767)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [350/780]  eta: 0:00:43  lr: 0.000050  loss: 3.4362 (3.3797)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [360/780]  eta: 0:00:42  lr: 0.000050  loss: 3.6091 (3.3827)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [370/780]  eta: 0:00:41  lr: 0.000050  loss: 3.2441 (3.3750)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [380/780]  eta: 0:00:40  lr: 0.000050  loss: 3.1710 (3.3747)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [390/780]  eta: 0:00:39  lr: 0.000050  loss: 3.3273 (3.3725)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [400/780]  eta: 0:00:38  lr: 0.000050  loss: 3.3286 (3.3703)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [410/780]  eta: 0:00:37  lr: 0.000050  loss: 3.4455 (3.3711)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [420/780]  eta: 0:00:36  lr: 0.000050  loss: 3.5171 (3.3718)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [430/780]  eta: 0:00:35  lr: 0.000050  loss: 3.4621 (3.3699)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [440/780]  eta: 0:00:34  lr: 0.000050  loss: 3.3907 (3.3687)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [450/780]  eta: 0:00:33  lr: 0.000050  loss: 3.2293 (3.3660)  time: 0.1007  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [460/780]  eta: 0:00:32  lr: 0.000050  loss: 3.2715 (3.3642)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [470/780]  eta: 0:00:31  lr: 0.000050  loss: 3.3936 (3.3636)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [480/780]  eta: 0:00:30  lr: 0.000050  loss: 3.4425 (3.3624)  time: 0.1029  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [490/780]  eta: 0:00:29  lr: 0.000050  loss: 3.3705 (3.3597)  time: 0.1016  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [500/780]  eta: 0:00:28  lr: 0.000050  loss: 3.3126 (3.3588)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [510/780]  eta: 0:00:27  lr: 0.000050  loss: 3.3800 (3.3608)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [520/780]  eta: 0:00:26  lr: 0.000050  loss: 3.3005 (3.3576)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [530/780]  eta: 0:00:25  lr: 0.000050  loss: 3.1703 (3.3561)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [540/780]  eta: 0:00:24  lr: 0.000050  loss: 3.4897 (3.3578)  time: 0.0965  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [550/780]  eta: 0:00:23  lr: 0.000050  loss: 3.5031 (3.3584)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [560/780]  eta: 0:00:22  lr: 0.000050  loss: 3.4121 (3.3543)  time: 0.1022  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [570/780]  eta: 0:00:21  lr: 0.000050  loss: 3.4121 (3.3586)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [580/780]  eta: 0:00:20  lr: 0.000050  loss: 3.5181 (3.3588)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [590/780]  eta: 0:00:19  lr: 0.000050  loss: 3.1965 (3.3541)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [600/780]  eta: 0:00:18  lr: 0.000050  loss: 3.3559 (3.3539)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [610/780]  eta: 0:00:17  lr: 0.000050  loss: 3.4454 (3.3553)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [620/780]  eta: 0:00:16  lr: 0.000050  loss: 3.4288 (3.3538)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [630/780]  eta: 0:00:15  lr: 0.000050  loss: 3.4485 (3.3553)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [640/780]  eta: 0:00:14  lr: 0.000050  loss: 3.4765 (3.3529)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [650/780]  eta: 0:00:13  lr: 0.000050  loss: 3.3473 (3.3547)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [660/780]  eta: 0:00:12  lr: 0.000050  loss: 3.4576 (3.3541)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [670/780]  eta: 0:00:11  lr: 0.000050  loss: 3.4558 (3.3535)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [680/780]  eta: 0:00:10  lr: 0.000050  loss: 3.3984 (3.3540)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [690/780]  eta: 0:00:09  lr: 0.000050  loss: 3.3984 (3.3542)  time: 0.0969  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [700/780]  eta: 0:00:08  lr: 0.000050  loss: 3.2012 (3.3523)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [710/780]  eta: 0:00:07  lr: 0.000050  loss: 3.1407 (3.3506)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [720/780]  eta: 0:00:06  lr: 0.000050  loss: 3.4110 (3.3519)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [730/780]  eta: 0:00:05  lr: 0.000050  loss: 3.3601 (3.3497)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [740/780]  eta: 0:00:04  lr: 0.000050  loss: 3.3922 (3.3512)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [750/780]  eta: 0:00:03  lr: 0.000050  loss: 3.3922 (3.3503)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [760/780]  eta: 0:00:02  lr: 0.000050  loss: 3.3324 (3.3522)  time: 0.1011  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [770/780]  eta: 0:00:01  lr: 0.000050  loss: 3.3324 (3.3521)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [5]  [779/780]  eta: 0:00:00  lr: 0.000050  loss: 3.2452 (3.3506)  time: 0.0940  data: 0.0001  max mem: 1920\n",
            "Epoch: [5] Total time: 0:01:18 (0.1001 s / it)\n",
            "Averaged stats: lr: 0.000050  loss: 3.2452 (3.3506)\n",
            "Test:  [  0/105]  eta: 0:01:22  loss: 1.4476 (1.4476)  acc1: 66.6667 (66.6667)  acc5: 94.7917 (94.7917)  time: 0.7864  data: 0.7312  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:13  loss: 1.5302 (1.5597)  acc1: 64.5833 (64.7727)  acc5: 90.6250 (90.6250)  time: 0.1425  data: 0.0932  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 1.4988 (1.5274)  acc1: 66.6667 (65.3770)  acc5: 91.6667 (91.4187)  time: 0.0679  data: 0.0189  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 1.4962 (1.5343)  acc1: 65.6250 (64.9530)  acc5: 92.7083 (91.4315)  time: 0.0563  data: 0.0078  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:05  loss: 1.5235 (1.5270)  acc1: 64.5833 (65.1677)  acc5: 92.7083 (91.7175)  time: 0.0533  data: 0.0047  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:03  loss: 1.5455 (1.5328)  acc1: 66.6667 (65.5229)  acc5: 91.6667 (91.7279)  time: 0.0501  data: 0.0014  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 1.4998 (1.5182)  acc1: 66.6667 (66.0178)  acc5: 91.6667 (91.7008)  time: 0.0499  data: 0.0032  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 1.4627 (1.5235)  acc1: 64.5833 (65.7424)  acc5: 91.6667 (91.5346)  time: 0.0512  data: 0.0055  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 1.5441 (1.5262)  acc1: 63.5417 (65.7279)  acc5: 90.6250 (91.4609)  time: 0.0517  data: 0.0027  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 1.5250 (1.5239)  acc1: 66.6667 (65.9455)  acc5: 92.7083 (91.5636)  time: 0.0490  data: 0.0003  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 1.5129 (1.5236)  acc1: 65.6250 (65.8003)  acc5: 92.7083 (91.5945)  time: 0.0415  data: 0.0002  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 1.4764 (1.5215)  acc1: 66.6667 (65.7700)  acc5: 92.7083 (91.6200)  time: 0.0368  data: 0.0002  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0602 s / it)\n",
            "* Acc@1 65.770 Acc@5 91.620 loss 1.522\n",
            "Accuracy of the network on the 10000 test images: 65.8%\n",
            "Max accuracy: 65.77%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [6]  [  0/780]  eta: 0:16:20  lr: 0.000046  loss: 2.9239 (2.9239)  time: 1.2567  data: 1.0932  max mem: 1920\n",
            "Epoch: [6]  [ 10/780]  eta: 0:02:39  lr: 0.000046  loss: 3.3413 (3.3304)  time: 0.2077  data: 0.1004  max mem: 1920\n",
            "Epoch: [6]  [ 20/780]  eta: 0:01:58  lr: 0.000046  loss: 3.3472 (3.3859)  time: 0.1012  data: 0.0007  max mem: 1920\n",
            "Epoch: [6]  [ 30/780]  eta: 0:01:43  lr: 0.000046  loss: 3.4383 (3.3371)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [ 40/780]  eta: 0:01:35  lr: 0.000046  loss: 3.3070 (3.3298)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [ 50/780]  eta: 0:01:29  lr: 0.000046  loss: 3.3070 (3.2976)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [ 60/780]  eta: 0:01:25  lr: 0.000046  loss: 3.3176 (3.3106)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [ 70/780]  eta: 0:01:22  lr: 0.000046  loss: 3.3851 (3.3149)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [ 80/780]  eta: 0:01:20  lr: 0.000046  loss: 3.3276 (3.3009)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [ 90/780]  eta: 0:01:17  lr: 0.000046  loss: 3.1153 (3.2888)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [100/780]  eta: 0:01:15  lr: 0.000046  loss: 3.1601 (3.2786)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [110/780]  eta: 0:01:13  lr: 0.000046  loss: 3.1749 (3.2771)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [120/780]  eta: 0:01:12  lr: 0.000046  loss: 3.3026 (3.2693)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [130/780]  eta: 0:01:10  lr: 0.000046  loss: 3.1476 (3.2517)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [140/780]  eta: 0:01:09  lr: 0.000046  loss: 3.2176 (3.2539)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [150/780]  eta: 0:01:07  lr: 0.000046  loss: 3.3042 (3.2488)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [160/780]  eta: 0:01:06  lr: 0.000046  loss: 3.3042 (3.2548)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [170/780]  eta: 0:01:04  lr: 0.000046  loss: 3.3878 (3.2566)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [180/780]  eta: 0:01:03  lr: 0.000046  loss: 3.2150 (3.2433)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [190/780]  eta: 0:01:02  lr: 0.000046  loss: 3.0426 (3.2324)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [200/780]  eta: 0:01:01  lr: 0.000046  loss: 3.0426 (3.2277)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [210/780]  eta: 0:00:59  lr: 0.000046  loss: 3.2094 (3.2308)  time: 0.1024  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [220/780]  eta: 0:00:58  lr: 0.000046  loss: 3.2588 (3.2285)  time: 0.1014  data: 0.0005  max mem: 1920\n",
            "Epoch: [6]  [230/780]  eta: 0:00:57  lr: 0.000046  loss: 3.2502 (3.2301)  time: 0.0999  data: 0.0005  max mem: 1920\n",
            "Epoch: [6]  [240/780]  eta: 0:00:56  lr: 0.000046  loss: 3.2502 (3.2309)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [250/780]  eta: 0:00:55  lr: 0.000046  loss: 3.2529 (3.2292)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [260/780]  eta: 0:00:54  lr: 0.000046  loss: 3.2909 (3.2360)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [270/780]  eta: 0:00:52  lr: 0.000046  loss: 3.2909 (3.2329)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [280/780]  eta: 0:00:51  lr: 0.000046  loss: 3.2587 (3.2343)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [290/780]  eta: 0:00:50  lr: 0.000046  loss: 3.2235 (3.2307)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [300/780]  eta: 0:00:49  lr: 0.000046  loss: 3.4974 (3.2404)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [310/780]  eta: 0:00:48  lr: 0.000046  loss: 3.4974 (3.2442)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [320/780]  eta: 0:00:47  lr: 0.000046  loss: 3.3144 (3.2460)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [330/780]  eta: 0:00:46  lr: 0.000046  loss: 3.1445 (3.2440)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [340/780]  eta: 0:00:45  lr: 0.000046  loss: 3.1445 (3.2413)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [350/780]  eta: 0:00:44  lr: 0.000046  loss: 3.2444 (3.2369)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [360/780]  eta: 0:00:42  lr: 0.000046  loss: 3.2483 (3.2363)  time: 0.0976  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [370/780]  eta: 0:00:41  lr: 0.000046  loss: 3.2459 (3.2339)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [380/780]  eta: 0:00:40  lr: 0.000046  loss: 3.1910 (3.2371)  time: 0.1015  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [390/780]  eta: 0:00:39  lr: 0.000046  loss: 3.1335 (3.2296)  time: 0.1018  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [400/780]  eta: 0:00:38  lr: 0.000046  loss: 3.0927 (3.2296)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [410/780]  eta: 0:00:37  lr: 0.000046  loss: 3.2648 (3.2296)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [420/780]  eta: 0:00:36  lr: 0.000046  loss: 3.0912 (3.2247)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [430/780]  eta: 0:00:35  lr: 0.000046  loss: 3.0912 (3.2273)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [440/780]  eta: 0:00:34  lr: 0.000046  loss: 3.3237 (3.2285)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [450/780]  eta: 0:00:33  lr: 0.000046  loss: 3.3532 (3.2327)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [460/780]  eta: 0:00:32  lr: 0.000046  loss: 3.3532 (3.2329)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [470/780]  eta: 0:00:31  lr: 0.000046  loss: 3.2317 (3.2317)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [480/780]  eta: 0:00:30  lr: 0.000046  loss: 3.2612 (3.2319)  time: 0.0961  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [490/780]  eta: 0:00:29  lr: 0.000046  loss: 3.2709 (3.2306)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [500/780]  eta: 0:00:28  lr: 0.000046  loss: 3.3798 (3.2327)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [510/780]  eta: 0:00:27  lr: 0.000046  loss: 3.2059 (3.2295)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [520/780]  eta: 0:00:26  lr: 0.000046  loss: 3.1872 (3.2291)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [530/780]  eta: 0:00:25  lr: 0.000046  loss: 3.4792 (3.2337)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [540/780]  eta: 0:00:24  lr: 0.000046  loss: 3.4792 (3.2348)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [550/780]  eta: 0:00:23  lr: 0.000046  loss: 3.2590 (3.2325)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [560/780]  eta: 0:00:22  lr: 0.000046  loss: 3.3531 (3.2360)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [570/780]  eta: 0:00:21  lr: 0.000046  loss: 3.3568 (3.2360)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [580/780]  eta: 0:00:20  lr: 0.000046  loss: 3.2408 (3.2371)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [590/780]  eta: 0:00:19  lr: 0.000046  loss: 3.2803 (3.2383)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [600/780]  eta: 0:00:18  lr: 0.000046  loss: 3.3136 (3.2395)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [610/780]  eta: 0:00:17  lr: 0.000046  loss: 3.3044 (3.2403)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [620/780]  eta: 0:00:16  lr: 0.000046  loss: 3.3552 (3.2413)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [630/780]  eta: 0:00:15  lr: 0.000046  loss: 3.4026 (3.2418)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [640/780]  eta: 0:00:14  lr: 0.000046  loss: 3.0790 (3.2366)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [650/780]  eta: 0:00:13  lr: 0.000046  loss: 3.0312 (3.2368)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [660/780]  eta: 0:00:12  lr: 0.000046  loss: 3.3166 (3.2356)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [670/780]  eta: 0:00:11  lr: 0.000046  loss: 3.3194 (3.2366)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [680/780]  eta: 0:00:10  lr: 0.000046  loss: 3.2666 (3.2341)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [690/780]  eta: 0:00:09  lr: 0.000046  loss: 3.1755 (3.2314)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [700/780]  eta: 0:00:08  lr: 0.000046  loss: 3.2381 (3.2307)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [710/780]  eta: 0:00:07  lr: 0.000046  loss: 3.2399 (3.2278)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [720/780]  eta: 0:00:06  lr: 0.000046  loss: 3.3588 (3.2317)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [730/780]  eta: 0:00:05  lr: 0.000046  loss: 3.2958 (3.2293)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [740/780]  eta: 0:00:04  lr: 0.000046  loss: 3.2087 (3.2307)  time: 0.1014  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [750/780]  eta: 0:00:03  lr: 0.000046  loss: 3.2771 (3.2285)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [760/780]  eta: 0:00:02  lr: 0.000046  loss: 3.0841 (3.2272)  time: 0.0955  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [770/780]  eta: 0:00:01  lr: 0.000046  loss: 3.2674 (3.2274)  time: 0.0930  data: 0.0002  max mem: 1920\n",
            "Epoch: [6]  [779/780]  eta: 0:00:00  lr: 0.000046  loss: 3.4306 (3.2256)  time: 0.0915  data: 0.0001  max mem: 1920\n",
            "Epoch: [6] Total time: 0:01:18 (0.1006 s / it)\n",
            "Averaged stats: lr: 0.000046  loss: 3.4306 (3.2256)\n",
            "Test:  [  0/105]  eta: 0:01:46  loss: 1.2536 (1.2536)  acc1: 69.7917 (69.7917)  acc5: 95.8333 (95.8333)  time: 1.0160  data: 0.9727  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:15  loss: 1.3371 (1.3626)  acc1: 69.7917 (69.4129)  acc5: 93.7500 (93.0871)  time: 0.1597  data: 0.1026  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:09  loss: 1.3021 (1.3369)  acc1: 69.7917 (69.2956)  acc5: 93.7500 (93.4524)  time: 0.0620  data: 0.0097  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 1.3241 (1.3482)  acc1: 68.7500 (68.6828)  acc5: 93.7500 (93.3468)  time: 0.0504  data: 0.0035  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:05  loss: 1.3368 (1.3382)  acc1: 66.6667 (68.7500)  acc5: 94.7917 (93.7246)  time: 0.0491  data: 0.0032  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:04  loss: 1.3750 (1.3435)  acc1: 68.7500 (68.7092)  acc5: 94.7917 (93.7092)  time: 0.0537  data: 0.0046  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 1.3280 (1.3331)  acc1: 69.7917 (69.0232)  acc5: 93.7500 (93.7158)  time: 0.0562  data: 0.0054  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 1.2712 (1.3411)  acc1: 69.7917 (68.8234)  acc5: 93.7500 (93.5006)  time: 0.0504  data: 0.0025  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 1.3563 (1.3443)  acc1: 68.7500 (68.6600)  acc5: 92.7083 (93.4671)  time: 0.0511  data: 0.0022  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 1.3453 (1.3401)  acc1: 68.7500 (68.7042)  acc5: 93.7500 (93.5440)  time: 0.0508  data: 0.0054  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 1.3166 (1.3388)  acc1: 69.7917 (68.7603)  acc5: 93.7500 (93.5953)  time: 0.0420  data: 0.0035  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 1.3116 (1.3365)  acc1: 70.8333 (68.7400)  acc5: 93.7500 (93.5900)  time: 0.0353  data: 0.0001  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0615 s / it)\n",
            "* Acc@1 68.740 Acc@5 93.590 loss 1.337\n",
            "Accuracy of the network on the 10000 test images: 68.7%\n",
            "Max accuracy: 68.74%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [7]  [  0/780]  eta: 0:15:39  lr: 0.000039  loss: 3.1366 (3.1366)  time: 1.2051  data: 1.0167  max mem: 1920\n",
            "Epoch: [7]  [ 10/780]  eta: 0:02:40  lr: 0.000039  loss: 3.3395 (3.3125)  time: 0.2083  data: 0.0926  max mem: 1920\n",
            "Epoch: [7]  [ 20/780]  eta: 0:01:59  lr: 0.000039  loss: 3.2168 (3.2230)  time: 0.1053  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [ 30/780]  eta: 0:01:43  lr: 0.000039  loss: 3.1341 (3.2255)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [ 40/780]  eta: 0:01:34  lr: 0.000039  loss: 3.1341 (3.1762)  time: 0.0969  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [ 50/780]  eta: 0:01:29  lr: 0.000039  loss: 3.2153 (3.1890)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [ 60/780]  eta: 0:01:25  lr: 0.000039  loss: 3.2422 (3.1738)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [ 70/780]  eta: 0:01:22  lr: 0.000039  loss: 3.2784 (3.1897)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [ 80/780]  eta: 0:01:19  lr: 0.000039  loss: 3.2896 (3.1739)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [ 90/780]  eta: 0:01:17  lr: 0.000039  loss: 3.1613 (3.1426)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [100/780]  eta: 0:01:15  lr: 0.000039  loss: 3.0828 (3.1354)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [110/780]  eta: 0:01:13  lr: 0.000039  loss: 3.3030 (3.1535)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [120/780]  eta: 0:01:11  lr: 0.000039  loss: 3.2950 (3.1435)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [130/780]  eta: 0:01:09  lr: 0.000039  loss: 3.1959 (3.1500)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [140/780]  eta: 0:01:08  lr: 0.000039  loss: 3.2421 (3.1453)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [150/780]  eta: 0:01:07  lr: 0.000039  loss: 3.2014 (3.1452)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [160/780]  eta: 0:01:05  lr: 0.000039  loss: 3.0642 (3.1440)  time: 0.1024  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [170/780]  eta: 0:01:04  lr: 0.000039  loss: 3.3076 (3.1548)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [180/780]  eta: 0:01:03  lr: 0.000039  loss: 3.3602 (3.1612)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [190/780]  eta: 0:01:02  lr: 0.000039  loss: 3.2835 (3.1510)  time: 0.1013  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [200/780]  eta: 0:01:01  lr: 0.000039  loss: 3.0807 (3.1500)  time: 0.1017  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [210/780]  eta: 0:00:59  lr: 0.000039  loss: 3.1449 (3.1473)  time: 0.1016  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [220/780]  eta: 0:00:58  lr: 0.000039  loss: 3.1449 (3.1516)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [230/780]  eta: 0:00:57  lr: 0.000039  loss: 3.1789 (3.1587)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [240/780]  eta: 0:00:56  lr: 0.000039  loss: 3.2621 (3.1556)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [250/780]  eta: 0:00:55  lr: 0.000039  loss: 3.1092 (3.1475)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [260/780]  eta: 0:00:54  lr: 0.000039  loss: 3.0032 (3.1464)  time: 0.1009  data: 0.0004  max mem: 1920\n",
            "Epoch: [7]  [270/780]  eta: 0:00:52  lr: 0.000039  loss: 2.9191 (3.1373)  time: 0.1016  data: 0.0005  max mem: 1920\n",
            "Epoch: [7]  [280/780]  eta: 0:00:51  lr: 0.000039  loss: 2.9325 (3.1408)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [290/780]  eta: 0:00:50  lr: 0.000039  loss: 3.2233 (3.1442)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [300/780]  eta: 0:00:49  lr: 0.000039  loss: 3.1714 (3.1467)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [310/780]  eta: 0:00:48  lr: 0.000039  loss: 3.1714 (3.1470)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [320/780]  eta: 0:00:47  lr: 0.000039  loss: 2.9470 (3.1409)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [330/780]  eta: 0:00:46  lr: 0.000039  loss: 2.9470 (3.1386)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [340/780]  eta: 0:00:45  lr: 0.000039  loss: 3.0266 (3.1359)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [350/780]  eta: 0:00:44  lr: 0.000039  loss: 3.0367 (3.1384)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [360/780]  eta: 0:00:43  lr: 0.000039  loss: 3.2146 (3.1367)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [370/780]  eta: 0:00:42  lr: 0.000039  loss: 3.1977 (3.1404)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [380/780]  eta: 0:00:40  lr: 0.000039  loss: 3.1977 (3.1383)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [390/780]  eta: 0:00:39  lr: 0.000039  loss: 3.1640 (3.1356)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [400/780]  eta: 0:00:38  lr: 0.000039  loss: 3.1876 (3.1371)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [410/780]  eta: 0:00:37  lr: 0.000039  loss: 3.3418 (3.1435)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [420/780]  eta: 0:00:36  lr: 0.000039  loss: 3.3639 (3.1467)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [430/780]  eta: 0:00:35  lr: 0.000039  loss: 3.2977 (3.1470)  time: 0.0963  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [440/780]  eta: 0:00:34  lr: 0.000039  loss: 2.7004 (3.1396)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [450/780]  eta: 0:00:33  lr: 0.000039  loss: 2.7085 (3.1439)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [460/780]  eta: 0:00:32  lr: 0.000039  loss: 3.3533 (3.1435)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [470/780]  eta: 0:00:31  lr: 0.000039  loss: 3.1296 (3.1419)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [480/780]  eta: 0:00:30  lr: 0.000039  loss: 3.3116 (3.1450)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [490/780]  eta: 0:00:29  lr: 0.000039  loss: 3.2631 (3.1447)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [500/780]  eta: 0:00:28  lr: 0.000039  loss: 2.9527 (3.1372)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [510/780]  eta: 0:00:27  lr: 0.000039  loss: 3.0235 (3.1392)  time: 0.1009  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [520/780]  eta: 0:00:26  lr: 0.000039  loss: 3.2237 (3.1417)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [530/780]  eta: 0:00:25  lr: 0.000039  loss: 3.1937 (3.1419)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [540/780]  eta: 0:00:24  lr: 0.000039  loss: 3.0466 (3.1414)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [550/780]  eta: 0:00:23  lr: 0.000039  loss: 3.1231 (3.1413)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [560/780]  eta: 0:00:22  lr: 0.000039  loss: 3.1342 (3.1403)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [570/780]  eta: 0:00:21  lr: 0.000039  loss: 3.1016 (3.1395)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [580/780]  eta: 0:00:20  lr: 0.000039  loss: 3.0300 (3.1356)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [590/780]  eta: 0:00:19  lr: 0.000039  loss: 3.0319 (3.1349)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [600/780]  eta: 0:00:18  lr: 0.000039  loss: 3.1488 (3.1340)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [610/780]  eta: 0:00:17  lr: 0.000039  loss: 3.2084 (3.1368)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [620/780]  eta: 0:00:16  lr: 0.000039  loss: 3.2662 (3.1377)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [630/780]  eta: 0:00:15  lr: 0.000039  loss: 3.2957 (3.1401)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [640/780]  eta: 0:00:14  lr: 0.000039  loss: 3.2320 (3.1367)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [650/780]  eta: 0:00:13  lr: 0.000039  loss: 2.9377 (3.1340)  time: 0.1018  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [660/780]  eta: 0:00:12  lr: 0.000039  loss: 3.1221 (3.1334)  time: 0.1010  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [670/780]  eta: 0:00:11  lr: 0.000039  loss: 3.1221 (3.1317)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [680/780]  eta: 0:00:10  lr: 0.000039  loss: 2.8782 (3.1297)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [690/780]  eta: 0:00:09  lr: 0.000039  loss: 3.0110 (3.1274)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [700/780]  eta: 0:00:08  lr: 0.000039  loss: 3.1316 (3.1256)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [710/780]  eta: 0:00:07  lr: 0.000039  loss: 2.9810 (3.1261)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [720/780]  eta: 0:00:06  lr: 0.000039  loss: 3.3795 (3.1301)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [730/780]  eta: 0:00:05  lr: 0.000039  loss: 3.2262 (3.1292)  time: 0.0967  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [740/780]  eta: 0:00:04  lr: 0.000039  loss: 3.1333 (3.1297)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [750/780]  eta: 0:00:03  lr: 0.000039  loss: 2.9955 (3.1264)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [760/780]  eta: 0:00:02  lr: 0.000039  loss: 2.8553 (3.1248)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [770/780]  eta: 0:00:01  lr: 0.000039  loss: 2.9393 (3.1225)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [7]  [779/780]  eta: 0:00:00  lr: 0.000039  loss: 3.0252 (3.1230)  time: 0.0949  data: 0.0001  max mem: 1920\n",
            "Epoch: [7] Total time: 0:01:18 (0.1008 s / it)\n",
            "Averaged stats: lr: 0.000039  loss: 3.0252 (3.1230)\n",
            "Test:  [  0/105]  eta: 0:02:04  loss: 1.1387 (1.1387)  acc1: 70.8333 (70.8333)  acc5: 95.8333 (95.8333)  time: 1.1812  data: 1.1096  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:14  loss: 1.2839 (1.2790)  acc1: 69.7917 (69.6970)  acc5: 93.7500 (93.5606)  time: 0.1494  data: 0.1047  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 1.2236 (1.2449)  acc1: 69.7917 (70.2877)  acc5: 93.7500 (93.8988)  time: 0.0494  data: 0.0039  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 1.2197 (1.2523)  acc1: 69.7917 (70.2285)  acc5: 93.7500 (93.8844)  time: 0.0515  data: 0.0022  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:05  loss: 1.2308 (1.2386)  acc1: 71.8750 (70.6555)  acc5: 94.7917 (94.2073)  time: 0.0499  data: 0.0017  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:03  loss: 1.2599 (1.2439)  acc1: 70.8333 (70.7108)  acc5: 94.7917 (94.2402)  time: 0.0516  data: 0.0027  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 1.2225 (1.2330)  acc1: 70.8333 (71.0383)  acc5: 94.7917 (94.2452)  time: 0.0541  data: 0.0055  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 1.2194 (1.2393)  acc1: 70.8333 (70.6133)  acc5: 94.7917 (94.2195)  time: 0.0539  data: 0.0048  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 1.2488 (1.2418)  acc1: 68.7500 (70.6533)  acc5: 93.7500 (94.1872)  time: 0.0567  data: 0.0012  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 1.2177 (1.2371)  acc1: 71.8750 (70.8677)  acc5: 93.7500 (94.1850)  time: 0.0532  data: 0.0012  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 1.2231 (1.2368)  acc1: 71.8750 (70.8540)  acc5: 94.7917 (94.1935)  time: 0.0414  data: 0.0007  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 1.1928 (1.2350)  acc1: 71.8750 (70.8400)  acc5: 94.7917 (94.2200)  time: 0.0372  data: 0.0001  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0613 s / it)\n",
            "* Acc@1 70.840 Acc@5 94.220 loss 1.235\n",
            "Accuracy of the network on the 10000 test images: 70.8%\n",
            "Max accuracy: 70.84%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [8]  [  0/780]  eta: 0:15:08  lr: 0.000033  loss: 2.9520 (2.9520)  time: 1.1643  data: 0.9975  max mem: 1920\n",
            "Epoch: [8]  [ 10/780]  eta: 0:02:34  lr: 0.000033  loss: 3.3929 (3.3144)  time: 0.2010  data: 0.0909  max mem: 1920\n",
            "Epoch: [8]  [ 20/780]  eta: 0:01:55  lr: 0.000033  loss: 3.3554 (3.2387)  time: 0.1010  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [ 30/780]  eta: 0:01:41  lr: 0.000033  loss: 3.3687 (3.2806)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [ 40/780]  eta: 0:01:33  lr: 0.000033  loss: 3.3213 (3.2223)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [ 50/780]  eta: 0:01:28  lr: 0.000033  loss: 3.2020 (3.2221)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [ 60/780]  eta: 0:01:24  lr: 0.000033  loss: 3.2441 (3.2277)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [ 70/780]  eta: 0:01:21  lr: 0.000033  loss: 3.3985 (3.2600)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [ 80/780]  eta: 0:01:18  lr: 0.000033  loss: 3.2996 (3.2321)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [ 90/780]  eta: 0:01:16  lr: 0.000033  loss: 3.0213 (3.2224)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [100/780]  eta: 0:01:14  lr: 0.000033  loss: 3.2258 (3.2128)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [110/780]  eta: 0:01:12  lr: 0.000033  loss: 3.2067 (3.2030)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [120/780]  eta: 0:01:11  lr: 0.000033  loss: 3.1971 (3.1945)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [130/780]  eta: 0:01:09  lr: 0.000033  loss: 3.0787 (3.1823)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [140/780]  eta: 0:01:08  lr: 0.000033  loss: 3.2490 (3.1827)  time: 0.1014  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [150/780]  eta: 0:01:06  lr: 0.000033  loss: 3.2704 (3.1785)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [160/780]  eta: 0:01:05  lr: 0.000033  loss: 3.0422 (3.1692)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [170/780]  eta: 0:01:04  lr: 0.000033  loss: 3.0684 (3.1636)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [180/780]  eta: 0:01:02  lr: 0.000033  loss: 3.2325 (3.1682)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [190/780]  eta: 0:01:01  lr: 0.000033  loss: 3.0824 (3.1561)  time: 0.0960  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [200/780]  eta: 0:01:00  lr: 0.000033  loss: 2.9685 (3.1470)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [210/780]  eta: 0:00:59  lr: 0.000033  loss: 2.9950 (3.1419)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [220/780]  eta: 0:00:58  lr: 0.000033  loss: 3.0542 (3.1327)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [230/780]  eta: 0:00:56  lr: 0.000033  loss: 3.0455 (3.1266)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [240/780]  eta: 0:00:55  lr: 0.000033  loss: 2.9188 (3.1134)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [250/780]  eta: 0:00:54  lr: 0.000033  loss: 2.8987 (3.1097)  time: 0.0962  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [260/780]  eta: 0:00:53  lr: 0.000033  loss: 3.0603 (3.1023)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [270/780]  eta: 0:00:52  lr: 0.000033  loss: 3.0825 (3.1034)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [280/780]  eta: 0:00:51  lr: 0.000033  loss: 3.0825 (3.0978)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [290/780]  eta: 0:00:50  lr: 0.000033  loss: 3.0300 (3.1010)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [300/780]  eta: 0:00:49  lr: 0.000033  loss: 3.1634 (3.1043)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [310/780]  eta: 0:00:48  lr: 0.000033  loss: 3.2813 (3.1067)  time: 0.1007  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [320/780]  eta: 0:00:47  lr: 0.000033  loss: 3.0625 (3.1012)  time: 0.1031  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [330/780]  eta: 0:00:45  lr: 0.000033  loss: 3.0733 (3.1024)  time: 0.1017  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [340/780]  eta: 0:00:44  lr: 0.000033  loss: 3.0733 (3.0958)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [350/780]  eta: 0:00:43  lr: 0.000033  loss: 3.1458 (3.1006)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [360/780]  eta: 0:00:42  lr: 0.000033  loss: 3.1906 (3.1026)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [370/780]  eta: 0:00:41  lr: 0.000033  loss: 3.0383 (3.0926)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [380/780]  eta: 0:00:40  lr: 0.000033  loss: 3.0858 (3.0949)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [390/780]  eta: 0:00:39  lr: 0.000033  loss: 3.1294 (3.0894)  time: 0.0969  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [400/780]  eta: 0:00:38  lr: 0.000033  loss: 3.1294 (3.0898)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [410/780]  eta: 0:00:37  lr: 0.000033  loss: 3.0491 (3.0902)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [420/780]  eta: 0:00:36  lr: 0.000033  loss: 3.0491 (3.0881)  time: 0.0957  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [430/780]  eta: 0:00:35  lr: 0.000033  loss: 3.0401 (3.0832)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [440/780]  eta: 0:00:34  lr: 0.000033  loss: 2.8961 (3.0787)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [450/780]  eta: 0:00:33  lr: 0.000033  loss: 2.8961 (3.0745)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [460/780]  eta: 0:00:32  lr: 0.000033  loss: 3.1480 (3.0757)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [470/780]  eta: 0:00:31  lr: 0.000033  loss: 3.1480 (3.0765)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [480/780]  eta: 0:00:30  lr: 0.000033  loss: 3.1463 (3.0799)  time: 0.0953  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [490/780]  eta: 0:00:29  lr: 0.000033  loss: 3.2005 (3.0808)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [500/780]  eta: 0:00:28  lr: 0.000033  loss: 3.0748 (3.0780)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [510/780]  eta: 0:00:27  lr: 0.000033  loss: 2.6822 (3.0704)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [520/780]  eta: 0:00:26  lr: 0.000033  loss: 2.7579 (3.0668)  time: 0.0959  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [530/780]  eta: 0:00:25  lr: 0.000033  loss: 2.7812 (3.0639)  time: 0.0961  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [540/780]  eta: 0:00:24  lr: 0.000033  loss: 3.1321 (3.0664)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [550/780]  eta: 0:00:23  lr: 0.000033  loss: 3.2879 (3.0698)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [560/780]  eta: 0:00:22  lr: 0.000033  loss: 3.2299 (3.0703)  time: 0.0967  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [570/780]  eta: 0:00:21  lr: 0.000033  loss: 3.2208 (3.0703)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [580/780]  eta: 0:00:20  lr: 0.000033  loss: 3.2246 (3.0700)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [590/780]  eta: 0:00:19  lr: 0.000033  loss: 3.0154 (3.0689)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [600/780]  eta: 0:00:18  lr: 0.000033  loss: 2.9704 (3.0649)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [610/780]  eta: 0:00:17  lr: 0.000033  loss: 2.8757 (3.0623)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [620/780]  eta: 0:00:16  lr: 0.000033  loss: 2.8757 (3.0616)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [630/780]  eta: 0:00:15  lr: 0.000033  loss: 2.8045 (3.0594)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [640/780]  eta: 0:00:14  lr: 0.000033  loss: 2.9620 (3.0586)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [650/780]  eta: 0:00:13  lr: 0.000033  loss: 3.1110 (3.0599)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [660/780]  eta: 0:00:12  lr: 0.000033  loss: 3.0861 (3.0571)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [670/780]  eta: 0:00:11  lr: 0.000033  loss: 2.8091 (3.0536)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [680/780]  eta: 0:00:10  lr: 0.000033  loss: 2.8758 (3.0533)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [690/780]  eta: 0:00:09  lr: 0.000033  loss: 2.9997 (3.0524)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [700/780]  eta: 0:00:07  lr: 0.000033  loss: 2.8485 (3.0499)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [710/780]  eta: 0:00:07  lr: 0.000033  loss: 2.8081 (3.0487)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [720/780]  eta: 0:00:06  lr: 0.000033  loss: 3.0113 (3.0477)  time: 0.1018  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [730/780]  eta: 0:00:05  lr: 0.000033  loss: 3.0113 (3.0468)  time: 0.1009  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [740/780]  eta: 0:00:04  lr: 0.000033  loss: 3.1290 (3.0472)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [750/780]  eta: 0:00:03  lr: 0.000033  loss: 3.1206 (3.0461)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [760/780]  eta: 0:00:02  lr: 0.000033  loss: 3.0693 (3.0443)  time: 0.1013  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [770/780]  eta: 0:00:01  lr: 0.000033  loss: 2.8752 (3.0416)  time: 0.1009  data: 0.0002  max mem: 1920\n",
            "Epoch: [8]  [779/780]  eta: 0:00:00  lr: 0.000033  loss: 3.0309 (3.0413)  time: 0.0999  data: 0.0001  max mem: 1920\n",
            "Epoch: [8] Total time: 0:01:18 (0.1002 s / it)\n",
            "Averaged stats: lr: 0.000033  loss: 3.0309 (3.0413)\n",
            "Test:  [  0/105]  eta: 0:01:19  loss: 1.0406 (1.0406)  acc1: 73.9583 (73.9583)  acc5: 95.8333 (95.8333)  time: 0.7570  data: 0.7189  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:13  loss: 1.1699 (1.1591)  acc1: 73.9583 (73.6742)  acc5: 93.7500 (93.8447)  time: 0.1401  data: 0.0935  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 1.1154 (1.1289)  acc1: 73.9583 (73.6607)  acc5: 93.7500 (94.1964)  time: 0.0659  data: 0.0174  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 1.0983 (1.1363)  acc1: 73.9583 (72.6479)  acc5: 94.7917 (94.5565)  time: 0.0528  data: 0.0021  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:05  loss: 1.1512 (1.1271)  acc1: 70.8333 (73.0437)  acc5: 95.8333 (94.7409)  time: 0.0545  data: 0.0027  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:03  loss: 1.1252 (1.1306)  acc1: 73.9583 (73.0801)  acc5: 94.7917 (94.8734)  time: 0.0551  data: 0.0041  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 1.1093 (1.1189)  acc1: 73.9583 (73.3948)  acc5: 94.7917 (94.9454)  time: 0.0501  data: 0.0027  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 1.1039 (1.1261)  acc1: 72.9167 (73.1074)  acc5: 94.7917 (94.9531)  time: 0.0470  data: 0.0031  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 1.1495 (1.1282)  acc1: 72.9167 (73.1224)  acc5: 94.7917 (94.9203)  time: 0.0524  data: 0.0034  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 1.1036 (1.1256)  acc1: 73.9583 (73.2486)  acc5: 94.7917 (94.9977)  time: 0.0512  data: 0.0022  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 1.1036 (1.1240)  acc1: 73.9583 (73.2054)  acc5: 95.8333 (95.0598)  time: 0.0406  data: 0.0009  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 1.0728 (1.1219)  acc1: 73.9583 (73.2200)  acc5: 95.8333 (95.0800)  time: 0.0369  data: 0.0009  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0596 s / it)\n",
            "* Acc@1 73.220 Acc@5 95.080 loss 1.122\n",
            "Accuracy of the network on the 10000 test images: 73.2%\n",
            "Max accuracy: 73.22%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [9]  [  0/780]  eta: 0:14:51  lr: 0.000027  loss: 2.4727 (2.4727)  time: 1.1430  data: 0.9885  max mem: 1920\n",
            "Epoch: [9]  [ 10/780]  eta: 0:02:33  lr: 0.000027  loss: 3.2095 (3.0837)  time: 0.1992  data: 0.0900  max mem: 1920\n",
            "Epoch: [9]  [ 20/780]  eta: 0:01:54  lr: 0.000027  loss: 2.9929 (3.0039)  time: 0.1012  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [ 30/780]  eta: 0:01:40  lr: 0.000027  loss: 2.9753 (3.0285)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [ 40/780]  eta: 0:01:32  lr: 0.000027  loss: 3.2171 (3.0137)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [ 50/780]  eta: 0:01:27  lr: 0.000027  loss: 3.1697 (3.0432)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [ 60/780]  eta: 0:01:24  lr: 0.000027  loss: 2.8878 (2.9874)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [ 70/780]  eta: 0:01:21  lr: 0.000027  loss: 2.8483 (3.0113)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [ 80/780]  eta: 0:01:18  lr: 0.000027  loss: 3.1415 (3.0287)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [ 90/780]  eta: 0:01:16  lr: 0.000027  loss: 2.7997 (2.9979)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [100/780]  eta: 0:01:14  lr: 0.000027  loss: 2.7997 (2.9965)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [110/780]  eta: 0:01:12  lr: 0.000027  loss: 3.0506 (2.9905)  time: 0.0969  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [120/780]  eta: 0:01:10  lr: 0.000027  loss: 2.8864 (2.9833)  time: 0.0947  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [130/780]  eta: 0:01:09  lr: 0.000027  loss: 3.0428 (2.9933)  time: 0.0960  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [140/780]  eta: 0:01:08  lr: 0.000027  loss: 3.2601 (3.0194)  time: 0.1011  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [150/780]  eta: 0:01:06  lr: 0.000027  loss: 3.2601 (3.0301)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [160/780]  eta: 0:01:05  lr: 0.000027  loss: 3.0477 (3.0141)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [170/780]  eta: 0:01:03  lr: 0.000027  loss: 2.8446 (3.0071)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [180/780]  eta: 0:01:02  lr: 0.000027  loss: 2.9775 (3.0089)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [190/780]  eta: 0:01:01  lr: 0.000027  loss: 2.9421 (3.0013)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [200/780]  eta: 0:01:00  lr: 0.000027  loss: 2.8707 (3.0067)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [210/780]  eta: 0:00:58  lr: 0.000027  loss: 2.9511 (3.0085)  time: 0.0967  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [220/780]  eta: 0:00:57  lr: 0.000027  loss: 3.0560 (3.0156)  time: 0.0946  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [230/780]  eta: 0:00:56  lr: 0.000027  loss: 3.2091 (3.0191)  time: 0.0950  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [240/780]  eta: 0:00:55  lr: 0.000027  loss: 3.1761 (3.0200)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [250/780]  eta: 0:00:54  lr: 0.000027  loss: 3.1608 (3.0222)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [260/780]  eta: 0:00:53  lr: 0.000027  loss: 3.1476 (3.0184)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [270/780]  eta: 0:00:52  lr: 0.000027  loss: 3.0904 (3.0160)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [280/780]  eta: 0:00:51  lr: 0.000027  loss: 3.0051 (3.0143)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [290/780]  eta: 0:00:49  lr: 0.000027  loss: 3.0051 (3.0190)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [300/780]  eta: 0:00:48  lr: 0.000027  loss: 2.8903 (3.0157)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [310/780]  eta: 0:00:47  lr: 0.000027  loss: 2.8752 (3.0135)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [320/780]  eta: 0:00:46  lr: 0.000027  loss: 3.0075 (3.0124)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [330/780]  eta: 0:00:45  lr: 0.000027  loss: 3.1483 (3.0149)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [340/780]  eta: 0:00:44  lr: 0.000027  loss: 3.1527 (3.0162)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [350/780]  eta: 0:00:43  lr: 0.000027  loss: 2.9924 (3.0122)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [360/780]  eta: 0:00:42  lr: 0.000027  loss: 2.8918 (3.0131)  time: 0.0963  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [370/780]  eta: 0:00:41  lr: 0.000027  loss: 3.1285 (3.0133)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [380/780]  eta: 0:00:40  lr: 0.000027  loss: 2.8812 (3.0057)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [390/780]  eta: 0:00:39  lr: 0.000027  loss: 2.8621 (3.0026)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [400/780]  eta: 0:00:38  lr: 0.000027  loss: 3.1771 (3.0065)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [410/780]  eta: 0:00:37  lr: 0.000027  loss: 3.2151 (3.0060)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [420/780]  eta: 0:00:36  lr: 0.000027  loss: 2.9867 (3.0031)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [430/780]  eta: 0:00:35  lr: 0.000027  loss: 2.8810 (2.9976)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [440/780]  eta: 0:00:34  lr: 0.000027  loss: 2.8854 (3.0013)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [450/780]  eta: 0:00:33  lr: 0.000027  loss: 3.0738 (3.0060)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [460/780]  eta: 0:00:32  lr: 0.000027  loss: 3.0858 (3.0059)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [470/780]  eta: 0:00:31  lr: 0.000027  loss: 2.8890 (3.0022)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [480/780]  eta: 0:00:30  lr: 0.000027  loss: 2.8857 (3.0002)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [490/780]  eta: 0:00:29  lr: 0.000027  loss: 2.9722 (2.9982)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [500/780]  eta: 0:00:28  lr: 0.000027  loss: 2.9231 (2.9958)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [510/780]  eta: 0:00:27  lr: 0.000027  loss: 3.0094 (2.9965)  time: 0.0967  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [520/780]  eta: 0:00:26  lr: 0.000027  loss: 2.9307 (2.9929)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [530/780]  eta: 0:00:25  lr: 0.000027  loss: 2.9307 (2.9960)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [540/780]  eta: 0:00:24  lr: 0.000027  loss: 2.8674 (2.9913)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [550/780]  eta: 0:00:23  lr: 0.000027  loss: 2.7602 (2.9881)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [560/780]  eta: 0:00:22  lr: 0.000027  loss: 2.9312 (2.9892)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [570/780]  eta: 0:00:21  lr: 0.000027  loss: 3.1876 (2.9916)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [580/780]  eta: 0:00:20  lr: 0.000027  loss: 3.0678 (2.9907)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [590/780]  eta: 0:00:19  lr: 0.000027  loss: 3.0031 (2.9910)  time: 0.1015  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [600/780]  eta: 0:00:18  lr: 0.000027  loss: 2.9644 (2.9878)  time: 0.1019  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [610/780]  eta: 0:00:17  lr: 0.000027  loss: 2.9877 (2.9896)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [620/780]  eta: 0:00:16  lr: 0.000027  loss: 3.0561 (2.9885)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [630/780]  eta: 0:00:15  lr: 0.000027  loss: 3.0055 (2.9895)  time: 0.0965  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [640/780]  eta: 0:00:14  lr: 0.000027  loss: 2.9958 (2.9887)  time: 0.0963  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [650/780]  eta: 0:00:13  lr: 0.000027  loss: 3.2113 (2.9916)  time: 0.0961  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [660/780]  eta: 0:00:12  lr: 0.000027  loss: 2.8743 (2.9881)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [670/780]  eta: 0:00:11  lr: 0.000027  loss: 2.8743 (2.9894)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [680/780]  eta: 0:00:10  lr: 0.000027  loss: 3.0113 (2.9913)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [690/780]  eta: 0:00:09  lr: 0.000027  loss: 3.0317 (2.9920)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [700/780]  eta: 0:00:08  lr: 0.000027  loss: 2.5516 (2.9844)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [710/780]  eta: 0:00:07  lr: 0.000027  loss: 2.5670 (2.9846)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [720/780]  eta: 0:00:06  lr: 0.000027  loss: 2.9354 (2.9828)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [730/780]  eta: 0:00:05  lr: 0.000027  loss: 2.8845 (2.9807)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [740/780]  eta: 0:00:03  lr: 0.000027  loss: 2.9149 (2.9822)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [750/780]  eta: 0:00:02  lr: 0.000027  loss: 2.9042 (2.9797)  time: 0.0959  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [760/780]  eta: 0:00:01  lr: 0.000027  loss: 2.8415 (2.9794)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [770/780]  eta: 0:00:00  lr: 0.000027  loss: 3.0405 (2.9801)  time: 0.0954  data: 0.0002  max mem: 1920\n",
            "Epoch: [9]  [779/780]  eta: 0:00:00  lr: 0.000027  loss: 2.8264 (2.9776)  time: 0.0910  data: 0.0001  max mem: 1920\n",
            "Epoch: [9] Total time: 0:01:17 (0.0999 s / it)\n",
            "Averaged stats: lr: 0.000027  loss: 2.8264 (2.9776)\n",
            "Test:  [  0/105]  eta: 0:01:31  loss: 0.9711 (0.9711)  acc1: 73.9583 (73.9583)  acc5: 96.8750 (96.8750)  time: 0.8707  data: 0.8300  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:12  loss: 1.1230 (1.1157)  acc1: 73.9583 (73.2955)  acc5: 94.7917 (94.3182)  time: 0.1288  data: 0.0846  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 1.1131 (1.0922)  acc1: 73.9583 (73.5119)  acc5: 94.7917 (94.8413)  time: 0.0589  data: 0.0127  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 1.0991 (1.1018)  acc1: 72.9167 (72.7823)  acc5: 94.7917 (94.9933)  time: 0.0607  data: 0.0156  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:04  loss: 1.0991 (1.0935)  acc1: 72.9167 (73.1707)  acc5: 94.7917 (95.0965)  time: 0.0539  data: 0.0088  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:03  loss: 1.0920 (1.0991)  acc1: 73.9583 (73.2639)  acc5: 94.7917 (95.0572)  time: 0.0480  data: 0.0031  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 1.0674 (1.0856)  acc1: 73.9583 (73.8217)  acc5: 95.8333 (95.2015)  time: 0.0496  data: 0.0039  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 1.0615 (1.0939)  acc1: 73.9583 (73.5916)  acc5: 95.8333 (95.1878)  time: 0.0544  data: 0.0038  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 1.0615 (1.0939)  acc1: 71.8750 (73.5468)  acc5: 95.8333 (95.1646)  time: 0.0554  data: 0.0046  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 1.0664 (1.0910)  acc1: 73.9583 (73.6493)  acc5: 95.8333 (95.2953)  time: 0.0514  data: 0.0029  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 1.0771 (1.0892)  acc1: 73.9583 (73.6799)  acc5: 96.8750 (95.3383)  time: 0.0421  data: 0.0004  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 1.0743 (1.0866)  acc1: 73.9583 (73.7100)  acc5: 96.8750 (95.3900)  time: 0.0372  data: 0.0001  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0601 s / it)\n",
            "* Acc@1 73.710 Acc@5 95.390 loss 1.087\n",
            "Accuracy of the network on the 10000 test images: 73.7%\n",
            "Max accuracy: 73.71%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [10]  [  0/780]  eta: 0:15:07  lr: 0.000021  loss: 3.1650 (3.1650)  time: 1.1635  data: 0.9753  max mem: 1920\n",
            "Epoch: [10]  [ 10/780]  eta: 0:02:40  lr: 0.000021  loss: 3.0016 (3.0509)  time: 0.2080  data: 0.0888  max mem: 1920\n",
            "Epoch: [10]  [ 20/780]  eta: 0:01:58  lr: 0.000021  loss: 3.0016 (3.0892)  time: 0.1061  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [ 30/780]  eta: 0:01:43  lr: 0.000021  loss: 2.9850 (3.0456)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [ 40/780]  eta: 0:01:34  lr: 0.000021  loss: 2.9356 (3.0014)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [ 50/780]  eta: 0:01:29  lr: 0.000021  loss: 3.2095 (3.0270)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [ 60/780]  eta: 0:01:25  lr: 0.000021  loss: 3.1901 (2.9853)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [ 70/780]  eta: 0:01:21  lr: 0.000021  loss: 3.0113 (3.0219)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [ 80/780]  eta: 0:01:19  lr: 0.000021  loss: 3.2284 (3.0197)  time: 0.0969  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [ 90/780]  eta: 0:01:16  lr: 0.000021  loss: 3.1281 (3.0130)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [100/780]  eta: 0:01:15  lr: 0.000021  loss: 3.1368 (3.0225)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [110/780]  eta: 0:01:13  lr: 0.000021  loss: 3.1450 (3.0277)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [120/780]  eta: 0:01:11  lr: 0.000021  loss: 2.8688 (2.9986)  time: 0.0966  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [130/780]  eta: 0:01:09  lr: 0.000021  loss: 2.8411 (2.9890)  time: 0.0976  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [140/780]  eta: 0:01:08  lr: 0.000021  loss: 2.9129 (2.9911)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [150/780]  eta: 0:01:07  lr: 0.000021  loss: 3.1189 (2.9970)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [160/780]  eta: 0:01:05  lr: 0.000021  loss: 3.1131 (2.9951)  time: 0.1018  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [170/780]  eta: 0:01:04  lr: 0.000021  loss: 2.9267 (2.9827)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [180/780]  eta: 0:01:03  lr: 0.000021  loss: 2.7251 (2.9792)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [190/780]  eta: 0:01:01  lr: 0.000021  loss: 2.9797 (2.9745)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [200/780]  eta: 0:01:00  lr: 0.000021  loss: 2.9325 (2.9614)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [210/780]  eta: 0:00:59  lr: 0.000021  loss: 2.8843 (2.9593)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [220/780]  eta: 0:00:58  lr: 0.000021  loss: 2.9486 (2.9653)  time: 0.0953  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [230/780]  eta: 0:00:56  lr: 0.000021  loss: 2.8839 (2.9586)  time: 0.0958  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [240/780]  eta: 0:00:55  lr: 0.000021  loss: 2.8382 (2.9552)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [250/780]  eta: 0:00:54  lr: 0.000021  loss: 2.9025 (2.9437)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [260/780]  eta: 0:00:53  lr: 0.000021  loss: 2.9370 (2.9414)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [270/780]  eta: 0:00:52  lr: 0.000021  loss: 3.2177 (2.9501)  time: 0.1034  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [280/780]  eta: 0:00:51  lr: 0.000021  loss: 3.2432 (2.9517)  time: 0.1030  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [290/780]  eta: 0:00:50  lr: 0.000021  loss: 3.1111 (2.9573)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [300/780]  eta: 0:00:49  lr: 0.000021  loss: 3.0373 (2.9562)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [310/780]  eta: 0:00:48  lr: 0.000021  loss: 2.9612 (2.9553)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [320/780]  eta: 0:00:47  lr: 0.000021  loss: 2.9649 (2.9542)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [330/780]  eta: 0:00:46  lr: 0.000021  loss: 2.9649 (2.9551)  time: 0.1020  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [340/780]  eta: 0:00:45  lr: 0.000021  loss: 2.8115 (2.9486)  time: 0.1039  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [350/780]  eta: 0:00:44  lr: 0.000021  loss: 2.8673 (2.9489)  time: 0.1012  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [360/780]  eta: 0:00:43  lr: 0.000021  loss: 2.8722 (2.9473)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [370/780]  eta: 0:00:41  lr: 0.000021  loss: 2.9382 (2.9468)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [380/780]  eta: 0:00:40  lr: 0.000021  loss: 2.8844 (2.9449)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [390/780]  eta: 0:00:39  lr: 0.000021  loss: 2.8844 (2.9428)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [400/780]  eta: 0:00:38  lr: 0.000021  loss: 2.8878 (2.9413)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [410/780]  eta: 0:00:37  lr: 0.000021  loss: 2.9890 (2.9449)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [420/780]  eta: 0:00:36  lr: 0.000021  loss: 3.0106 (2.9404)  time: 0.0977  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [430/780]  eta: 0:00:35  lr: 0.000021  loss: 2.9901 (2.9397)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [440/780]  eta: 0:00:34  lr: 0.000021  loss: 2.9078 (2.9370)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [450/780]  eta: 0:00:33  lr: 0.000021  loss: 2.9078 (2.9350)  time: 0.1009  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [460/780]  eta: 0:00:32  lr: 0.000021  loss: 3.0493 (2.9384)  time: 0.1013  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [470/780]  eta: 0:00:31  lr: 0.000021  loss: 3.1596 (2.9418)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [480/780]  eta: 0:00:30  lr: 0.000021  loss: 3.1317 (2.9403)  time: 0.0953  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [490/780]  eta: 0:00:29  lr: 0.000021  loss: 2.9297 (2.9419)  time: 0.0958  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [500/780]  eta: 0:00:28  lr: 0.000021  loss: 2.8408 (2.9388)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [510/780]  eta: 0:00:27  lr: 0.000021  loss: 2.7434 (2.9358)  time: 0.1009  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [520/780]  eta: 0:00:26  lr: 0.000021  loss: 2.7768 (2.9341)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [530/780]  eta: 0:00:25  lr: 0.000021  loss: 2.8884 (2.9348)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [540/780]  eta: 0:00:24  lr: 0.000021  loss: 2.9230 (2.9345)  time: 0.0967  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [550/780]  eta: 0:00:23  lr: 0.000021  loss: 2.9230 (2.9335)  time: 0.0956  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [560/780]  eta: 0:00:22  lr: 0.000021  loss: 3.0642 (2.9352)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [570/780]  eta: 0:00:21  lr: 0.000021  loss: 3.1114 (2.9345)  time: 0.1015  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [580/780]  eta: 0:00:20  lr: 0.000021  loss: 2.9558 (2.9359)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [590/780]  eta: 0:00:19  lr: 0.000021  loss: 3.0380 (2.9389)  time: 0.1011  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [600/780]  eta: 0:00:18  lr: 0.000021  loss: 2.9496 (2.9384)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [610/780]  eta: 0:00:17  lr: 0.000021  loss: 2.9797 (2.9404)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [620/780]  eta: 0:00:16  lr: 0.000021  loss: 2.8994 (2.9368)  time: 0.0963  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [630/780]  eta: 0:00:15  lr: 0.000021  loss: 2.8120 (2.9382)  time: 0.0960  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [640/780]  eta: 0:00:14  lr: 0.000021  loss: 3.0306 (2.9366)  time: 0.0963  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [650/780]  eta: 0:00:13  lr: 0.000021  loss: 3.1035 (2.9415)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [660/780]  eta: 0:00:12  lr: 0.000021  loss: 3.1035 (2.9419)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [670/780]  eta: 0:00:11  lr: 0.000021  loss: 2.7984 (2.9398)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [680/780]  eta: 0:00:10  lr: 0.000021  loss: 2.7324 (2.9383)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [690/780]  eta: 0:00:09  lr: 0.000021  loss: 2.8272 (2.9361)  time: 0.0965  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [700/780]  eta: 0:00:08  lr: 0.000021  loss: 2.9166 (2.9386)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [710/780]  eta: 0:00:07  lr: 0.000021  loss: 3.1359 (2.9411)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [720/780]  eta: 0:00:06  lr: 0.000021  loss: 3.0818 (2.9399)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [730/780]  eta: 0:00:05  lr: 0.000021  loss: 3.0362 (2.9405)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [740/780]  eta: 0:00:04  lr: 0.000021  loss: 3.1391 (2.9434)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [750/780]  eta: 0:00:03  lr: 0.000021  loss: 3.1649 (2.9438)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [760/780]  eta: 0:00:02  lr: 0.000021  loss: 3.0877 (2.9453)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [770/780]  eta: 0:00:01  lr: 0.000021  loss: 3.0310 (2.9442)  time: 0.0965  data: 0.0002  max mem: 1920\n",
            "Epoch: [10]  [779/780]  eta: 0:00:00  lr: 0.000021  loss: 2.9967 (2.9425)  time: 0.0936  data: 0.0001  max mem: 1920\n",
            "Epoch: [10] Total time: 0:01:18 (0.1005 s / it)\n",
            "Averaged stats: lr: 0.000021  loss: 2.9967 (2.9425)\n",
            "Test:  [  0/105]  eta: 0:01:59  loss: 0.9415 (0.9415)  acc1: 79.1667 (79.1667)  acc5: 96.8750 (96.8750)  time: 1.1339  data: 1.0679  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:14  loss: 1.1195 (1.0851)  acc1: 73.9583 (74.6212)  acc5: 95.8333 (94.7917)  time: 0.1482  data: 0.1012  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 1.0629 (1.0523)  acc1: 73.9583 (74.8016)  acc5: 94.7917 (94.8909)  time: 0.0490  data: 0.0042  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 1.0521 (1.0657)  acc1: 73.9583 (74.0591)  acc5: 95.8333 (94.9261)  time: 0.0496  data: 0.0026  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:05  loss: 1.0624 (1.0544)  acc1: 72.9167 (74.3902)  acc5: 95.8333 (95.1728)  time: 0.0546  data: 0.0035  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:04  loss: 1.0624 (1.0602)  acc1: 75.0000 (74.3873)  acc5: 95.8333 (95.2002)  time: 0.0556  data: 0.0057  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 1.0521 (1.0478)  acc1: 75.0000 (74.7268)  acc5: 95.8333 (95.3723)  time: 0.0524  data: 0.0065  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 1.0154 (1.0559)  acc1: 75.0000 (74.5012)  acc5: 95.8333 (95.3492)  time: 0.0508  data: 0.0066  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 1.0533 (1.0564)  acc1: 72.9167 (74.5499)  acc5: 95.8333 (95.3447)  time: 0.0498  data: 0.0050  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 1.0429 (1.0534)  acc1: 75.0000 (74.6451)  acc5: 95.8333 (95.4327)  time: 0.0470  data: 0.0027  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 1.0367 (1.0520)  acc1: 76.0417 (74.7525)  acc5: 95.8333 (95.4517)  time: 0.0404  data: 0.0007  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.9837 (1.0502)  acc1: 76.0417 (74.7600)  acc5: 95.8333 (95.4700)  time: 0.0363  data: 0.0001  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0599 s / it)\n",
            "* Acc@1 74.760 Acc@5 95.470 loss 1.050\n",
            "Accuracy of the network on the 10000 test images: 74.8%\n",
            "Max accuracy: 74.76%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [11]  [  0/780]  eta: 0:16:18  lr: 0.000017  loss: 2.8529 (2.8529)  time: 1.2544  data: 1.1078  max mem: 1920\n",
            "Epoch: [11]  [ 10/780]  eta: 0:02:43  lr: 0.000017  loss: 3.1874 (2.8947)  time: 0.2121  data: 0.1024  max mem: 1920\n",
            "Epoch: [11]  [ 20/780]  eta: 0:02:02  lr: 0.000017  loss: 3.1874 (2.9991)  time: 0.1060  data: 0.0010  max mem: 1920\n",
            "Epoch: [11]  [ 30/780]  eta: 0:01:45  lr: 0.000017  loss: 3.1158 (3.0534)  time: 0.1012  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [ 40/780]  eta: 0:01:36  lr: 0.000017  loss: 3.1158 (3.0470)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [ 50/780]  eta: 0:01:30  lr: 0.000017  loss: 3.0107 (3.0328)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [ 60/780]  eta: 0:01:26  lr: 0.000017  loss: 2.8790 (2.9933)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [ 70/780]  eta: 0:01:22  lr: 0.000017  loss: 3.0426 (2.9955)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [ 80/780]  eta: 0:01:19  lr: 0.000017  loss: 2.9581 (2.9724)  time: 0.0958  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [ 90/780]  eta: 0:01:17  lr: 0.000017  loss: 2.8814 (2.9627)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [100/780]  eta: 0:01:15  lr: 0.000017  loss: 3.0042 (2.9485)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [110/780]  eta: 0:01:13  lr: 0.000017  loss: 2.8005 (2.9299)  time: 0.0958  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [120/780]  eta: 0:01:11  lr: 0.000017  loss: 2.8005 (2.9234)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [130/780]  eta: 0:01:10  lr: 0.000017  loss: 2.9283 (2.9234)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [140/780]  eta: 0:01:08  lr: 0.000017  loss: 2.9237 (2.9198)  time: 0.0973  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [150/780]  eta: 0:01:07  lr: 0.000017  loss: 2.9572 (2.9230)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [160/780]  eta: 0:01:05  lr: 0.000017  loss: 3.1486 (2.9396)  time: 0.0985  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [170/780]  eta: 0:01:04  lr: 0.000017  loss: 3.1403 (2.9429)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [180/780]  eta: 0:01:03  lr: 0.000017  loss: 2.9858 (2.9458)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [190/780]  eta: 0:01:01  lr: 0.000017  loss: 2.9858 (2.9452)  time: 0.0976  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [200/780]  eta: 0:01:00  lr: 0.000017  loss: 2.9272 (2.9457)  time: 0.0967  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [210/780]  eta: 0:00:59  lr: 0.000017  loss: 3.0589 (2.9528)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [220/780]  eta: 0:00:58  lr: 0.000017  loss: 3.0589 (2.9522)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [230/780]  eta: 0:00:57  lr: 0.000017  loss: 2.8984 (2.9477)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [240/780]  eta: 0:00:55  lr: 0.000017  loss: 2.7641 (2.9436)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [250/780]  eta: 0:00:54  lr: 0.000017  loss: 2.8582 (2.9411)  time: 0.1011  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [260/780]  eta: 0:00:53  lr: 0.000017  loss: 2.9556 (2.9413)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [270/780]  eta: 0:00:52  lr: 0.000017  loss: 2.9554 (2.9371)  time: 0.0979  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [280/780]  eta: 0:00:51  lr: 0.000017  loss: 2.9624 (2.9401)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [290/780]  eta: 0:00:50  lr: 0.000017  loss: 3.1325 (2.9370)  time: 0.1007  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [300/780]  eta: 0:00:49  lr: 0.000017  loss: 2.8481 (2.9320)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [310/780]  eta: 0:00:48  lr: 0.000017  loss: 2.8481 (2.9299)  time: 0.0976  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [320/780]  eta: 0:00:47  lr: 0.000017  loss: 2.8754 (2.9249)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [330/780]  eta: 0:00:46  lr: 0.000017  loss: 2.8756 (2.9264)  time: 0.0964  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [340/780]  eta: 0:00:44  lr: 0.000017  loss: 2.8756 (2.9253)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [350/780]  eta: 0:00:43  lr: 0.000017  loss: 2.9858 (2.9283)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [360/780]  eta: 0:00:42  lr: 0.000017  loss: 3.0234 (2.9305)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [370/780]  eta: 0:00:41  lr: 0.000017  loss: 2.9640 (2.9261)  time: 0.0978  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [380/780]  eta: 0:00:40  lr: 0.000017  loss: 2.8124 (2.9261)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [390/780]  eta: 0:00:39  lr: 0.000017  loss: 3.1445 (2.9343)  time: 0.1027  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [400/780]  eta: 0:00:38  lr: 0.000017  loss: 3.1024 (2.9288)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [410/780]  eta: 0:00:37  lr: 0.000017  loss: 2.9949 (2.9374)  time: 0.0982  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [420/780]  eta: 0:00:36  lr: 0.000017  loss: 2.9556 (2.9284)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [430/780]  eta: 0:00:35  lr: 0.000017  loss: 2.5056 (2.9226)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [440/780]  eta: 0:00:34  lr: 0.000017  loss: 2.9941 (2.9276)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [450/780]  eta: 0:00:33  lr: 0.000017  loss: 3.0035 (2.9252)  time: 0.1018  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [460/780]  eta: 0:00:32  lr: 0.000017  loss: 2.9111 (2.9252)  time: 0.1021  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [470/780]  eta: 0:00:31  lr: 0.000017  loss: 2.7479 (2.9174)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [480/780]  eta: 0:00:30  lr: 0.000017  loss: 2.5242 (2.9174)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [490/780]  eta: 0:00:29  lr: 0.000017  loss: 3.0374 (2.9154)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [500/780]  eta: 0:00:28  lr: 0.000017  loss: 2.9547 (2.9147)  time: 0.1013  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [510/780]  eta: 0:00:27  lr: 0.000017  loss: 2.9355 (2.9147)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [520/780]  eta: 0:00:26  lr: 0.000017  loss: 2.8712 (2.9110)  time: 0.0974  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [530/780]  eta: 0:00:25  lr: 0.000017  loss: 2.9127 (2.9166)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [540/780]  eta: 0:00:24  lr: 0.000017  loss: 2.9127 (2.9128)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [550/780]  eta: 0:00:23  lr: 0.000017  loss: 2.8739 (2.9126)  time: 0.0961  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [560/780]  eta: 0:00:22  lr: 0.000017  loss: 2.9638 (2.9110)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [570/780]  eta: 0:00:21  lr: 0.000017  loss: 2.8234 (2.9094)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [580/780]  eta: 0:00:20  lr: 0.000017  loss: 2.7255 (2.9070)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [590/780]  eta: 0:00:19  lr: 0.000017  loss: 2.9926 (2.9092)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [600/780]  eta: 0:00:18  lr: 0.000017  loss: 2.9284 (2.9035)  time: 0.1026  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [610/780]  eta: 0:00:17  lr: 0.000017  loss: 2.7008 (2.9025)  time: 0.1040  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [620/780]  eta: 0:00:16  lr: 0.000017  loss: 2.8774 (2.9043)  time: 0.1016  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [630/780]  eta: 0:00:15  lr: 0.000017  loss: 2.9326 (2.9044)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [640/780]  eta: 0:00:14  lr: 0.000017  loss: 3.0331 (2.9057)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [650/780]  eta: 0:00:13  lr: 0.000017  loss: 3.0653 (2.9050)  time: 0.0957  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [660/780]  eta: 0:00:12  lr: 0.000017  loss: 2.6095 (2.9018)  time: 0.0955  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [670/780]  eta: 0:00:11  lr: 0.000017  loss: 2.6439 (2.9002)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [680/780]  eta: 0:00:10  lr: 0.000017  loss: 2.8219 (2.9004)  time: 0.0970  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [690/780]  eta: 0:00:09  lr: 0.000017  loss: 2.9888 (2.9004)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [700/780]  eta: 0:00:08  lr: 0.000017  loss: 3.1485 (2.9027)  time: 0.1043  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [710/780]  eta: 0:00:07  lr: 0.000017  loss: 2.9304 (2.8999)  time: 0.1034  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [720/780]  eta: 0:00:06  lr: 0.000017  loss: 2.8204 (2.9008)  time: 0.1010  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [730/780]  eta: 0:00:05  lr: 0.000017  loss: 2.9281 (2.9002)  time: 0.1021  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [740/780]  eta: 0:00:04  lr: 0.000017  loss: 2.9792 (2.9002)  time: 0.1013  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [750/780]  eta: 0:00:03  lr: 0.000017  loss: 2.8462 (2.8973)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [760/780]  eta: 0:00:02  lr: 0.000017  loss: 2.7819 (2.8983)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [770/780]  eta: 0:00:01  lr: 0.000017  loss: 2.6685 (2.8948)  time: 0.0951  data: 0.0002  max mem: 1920\n",
            "Epoch: [11]  [779/780]  eta: 0:00:00  lr: 0.000017  loss: 2.8651 (2.8948)  time: 0.0949  data: 0.0001  max mem: 1920\n",
            "Epoch: [11] Total time: 0:01:18 (0.1009 s / it)\n",
            "Averaged stats: lr: 0.000017  loss: 2.8651 (2.8948)\n",
            "Test:  [  0/105]  eta: 0:01:19  loss: 0.8659 (0.8659)  acc1: 79.1667 (79.1667)  acc5: 96.8750 (96.8750)  time: 0.7529  data: 0.7054  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:13  loss: 1.0786 (1.0540)  acc1: 75.0000 (75.3788)  acc5: 94.7917 (94.5076)  time: 0.1391  data: 0.0959  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 1.0224 (1.0176)  acc1: 75.0000 (75.3472)  acc5: 95.8333 (94.9901)  time: 0.0705  data: 0.0262  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 1.0184 (1.0302)  acc1: 75.0000 (74.6976)  acc5: 95.8333 (95.0941)  time: 0.0569  data: 0.0099  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:05  loss: 1.0185 (1.0197)  acc1: 75.0000 (74.8984)  acc5: 95.8333 (95.4522)  time: 0.0514  data: 0.0045  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:04  loss: 1.0595 (1.0257)  acc1: 76.0417 (75.0000)  acc5: 95.8333 (95.4453)  time: 0.0528  data: 0.0067  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 1.0050 (1.0137)  acc1: 76.0417 (75.3245)  acc5: 95.8333 (95.5601)  time: 0.0546  data: 0.0058  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 0.9662 (1.0218)  acc1: 75.0000 (75.1761)  acc5: 95.8333 (95.5986)  time: 0.0540  data: 0.0072  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 1.0436 (1.0223)  acc1: 73.9583 (75.1415)  acc5: 95.8333 (95.6147)  time: 0.0517  data: 0.0060  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 1.0250 (1.0197)  acc1: 75.0000 (75.2404)  acc5: 96.8750 (95.7189)  time: 0.0479  data: 0.0014  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 1.0069 (1.0181)  acc1: 76.0417 (75.3919)  acc5: 96.8750 (95.7611)  time: 0.0401  data: 0.0002  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.9521 (1.0164)  acc1: 76.0417 (75.3800)  acc5: 95.8333 (95.8000)  time: 0.0361  data: 0.0001  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0604 s / it)\n",
            "* Acc@1 75.380 Acc@5 95.800 loss 1.016\n",
            "Accuracy of the network on the 10000 test images: 75.4%\n",
            "Max accuracy: 75.38%\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "Epoch: [12]  [  0/780]  eta: 0:15:09  lr: 0.000013  loss: 3.2094 (3.2094)  time: 1.1658  data: 0.9533  max mem: 1920\n",
            "Epoch: [12]  [ 10/780]  eta: 0:02:41  lr: 0.000013  loss: 3.1615 (3.0404)  time: 0.2091  data: 0.0894  max mem: 1920\n",
            "Epoch: [12]  [ 20/780]  eta: 0:01:59  lr: 0.000013  loss: 2.8709 (2.9477)  time: 0.1074  data: 0.0016  max mem: 1920\n",
            "Epoch: [12]  [ 30/780]  eta: 0:01:43  lr: 0.000013  loss: 2.8709 (2.9716)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [ 40/780]  eta: 0:01:35  lr: 0.000013  loss: 2.9970 (2.9508)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [ 50/780]  eta: 0:01:30  lr: 0.000013  loss: 3.0447 (2.9630)  time: 0.1002  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [ 60/780]  eta: 0:01:25  lr: 0.000013  loss: 3.2014 (2.9705)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [ 70/780]  eta: 0:01:22  lr: 0.000013  loss: 3.0942 (2.9872)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [ 80/780]  eta: 0:01:19  lr: 0.000013  loss: 3.0513 (2.9836)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [ 90/780]  eta: 0:01:17  lr: 0.000013  loss: 3.0513 (2.9852)  time: 0.0981  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [100/780]  eta: 0:01:15  lr: 0.000013  loss: 2.8979 (2.9644)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [110/780]  eta: 0:01:13  lr: 0.000013  loss: 2.9983 (2.9741)  time: 0.1006  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [120/780]  eta: 0:01:11  lr: 0.000013  loss: 3.0506 (2.9756)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [130/780]  eta: 0:01:10  lr: 0.000013  loss: 2.9762 (2.9690)  time: 0.0984  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [140/780]  eta: 0:01:08  lr: 0.000013  loss: 2.8736 (2.9625)  time: 0.0989  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [150/780]  eta: 0:01:07  lr: 0.000013  loss: 2.6299 (2.9290)  time: 0.0994  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [160/780]  eta: 0:01:06  lr: 0.000013  loss: 2.4855 (2.9254)  time: 0.0983  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [170/780]  eta: 0:01:04  lr: 0.000013  loss: 3.0873 (2.9327)  time: 0.0980  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [180/780]  eta: 0:01:03  lr: 0.000013  loss: 3.0640 (2.9344)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [190/780]  eta: 0:01:02  lr: 0.000013  loss: 2.8415 (2.9225)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [200/780]  eta: 0:01:00  lr: 0.000013  loss: 2.6712 (2.9185)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [210/780]  eta: 0:00:59  lr: 0.000013  loss: 2.9319 (2.9205)  time: 0.0966  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [220/780]  eta: 0:00:58  lr: 0.000013  loss: 2.8820 (2.9154)  time: 0.0968  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [230/780]  eta: 0:00:57  lr: 0.000013  loss: 2.8820 (2.9132)  time: 0.1007  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [240/780]  eta: 0:00:56  lr: 0.000013  loss: 2.8009 (2.9013)  time: 0.1009  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [250/780]  eta: 0:00:54  lr: 0.000013  loss: 2.8009 (2.9003)  time: 0.0992  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [260/780]  eta: 0:00:53  lr: 0.000013  loss: 2.7528 (2.8936)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [270/780]  eta: 0:00:52  lr: 0.000013  loss: 2.7664 (2.8965)  time: 0.1010  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [280/780]  eta: 0:00:51  lr: 0.000013  loss: 2.7409 (2.8867)  time: 0.1021  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [290/780]  eta: 0:00:50  lr: 0.000013  loss: 2.7409 (2.8893)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [300/780]  eta: 0:00:49  lr: 0.000013  loss: 2.9745 (2.8902)  time: 0.0997  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [310/780]  eta: 0:00:48  lr: 0.000013  loss: 3.0261 (2.8988)  time: 0.0999  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [320/780]  eta: 0:00:47  lr: 0.000013  loss: 3.0469 (2.9014)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [330/780]  eta: 0:00:46  lr: 0.000013  loss: 2.9253 (2.9007)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [340/780]  eta: 0:00:45  lr: 0.000013  loss: 2.8559 (2.8997)  time: 0.1024  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [350/780]  eta: 0:00:44  lr: 0.000013  loss: 2.9180 (2.9032)  time: 0.1014  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [360/780]  eta: 0:00:43  lr: 0.000013  loss: 2.9180 (2.9023)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [370/780]  eta: 0:00:42  lr: 0.000013  loss: 2.9342 (2.9019)  time: 0.1011  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [380/780]  eta: 0:00:41  lr: 0.000013  loss: 2.9373 (2.9010)  time: 0.1003  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [390/780]  eta: 0:00:39  lr: 0.000013  loss: 2.8484 (2.8989)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [400/780]  eta: 0:00:38  lr: 0.000013  loss: 2.7268 (2.8968)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [410/780]  eta: 0:00:37  lr: 0.000013  loss: 2.7268 (2.8931)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [420/780]  eta: 0:00:36  lr: 0.000013  loss: 2.7365 (2.8905)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [430/780]  eta: 0:00:35  lr: 0.000013  loss: 2.9953 (2.8950)  time: 0.0996  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [440/780]  eta: 0:00:34  lr: 0.000013  loss: 3.0778 (2.8967)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [450/780]  eta: 0:00:33  lr: 0.000013  loss: 2.9421 (2.8942)  time: 0.0959  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [460/780]  eta: 0:00:32  lr: 0.000013  loss: 2.8014 (2.8937)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [470/780]  eta: 0:00:31  lr: 0.000013  loss: 2.9074 (2.8911)  time: 0.1017  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [480/780]  eta: 0:00:30  lr: 0.000013  loss: 2.9074 (2.8897)  time: 0.1028  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [490/780]  eta: 0:00:29  lr: 0.000013  loss: 2.9427 (2.8880)  time: 0.1020  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [500/780]  eta: 0:00:28  lr: 0.000013  loss: 2.7732 (2.8845)  time: 0.1017  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [510/780]  eta: 0:00:27  lr: 0.000013  loss: 2.7101 (2.8851)  time: 0.1010  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [520/780]  eta: 0:00:26  lr: 0.000013  loss: 2.7450 (2.8837)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [530/780]  eta: 0:00:25  lr: 0.000013  loss: 3.0185 (2.8875)  time: 0.0971  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [540/780]  eta: 0:00:24  lr: 0.000013  loss: 2.9786 (2.8840)  time: 0.0986  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [550/780]  eta: 0:00:23  lr: 0.000013  loss: 2.7801 (2.8832)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [560/780]  eta: 0:00:22  lr: 0.000013  loss: 2.9968 (2.8843)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [570/780]  eta: 0:00:21  lr: 0.000013  loss: 3.0179 (2.8861)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [580/780]  eta: 0:00:20  lr: 0.000013  loss: 3.0140 (2.8857)  time: 0.0975  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [590/780]  eta: 0:00:19  lr: 0.000013  loss: 2.9896 (2.8863)  time: 0.1000  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [600/780]  eta: 0:00:18  lr: 0.000013  loss: 2.6857 (2.8802)  time: 0.0988  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [610/780]  eta: 0:00:17  lr: 0.000013  loss: 2.6676 (2.8815)  time: 0.0961  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [620/780]  eta: 0:00:16  lr: 0.000013  loss: 2.9888 (2.8825)  time: 0.0972  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [630/780]  eta: 0:00:15  lr: 0.000013  loss: 2.9762 (2.8828)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [640/780]  eta: 0:00:14  lr: 0.000013  loss: 2.7760 (2.8804)  time: 0.0995  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [650/780]  eta: 0:00:13  lr: 0.000013  loss: 2.8495 (2.8822)  time: 0.1004  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [660/780]  eta: 0:00:12  lr: 0.000013  loss: 2.9253 (2.8829)  time: 0.1005  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [670/780]  eta: 0:00:11  lr: 0.000013  loss: 2.9782 (2.8828)  time: 0.0991  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [680/780]  eta: 0:00:10  lr: 0.000013  loss: 3.1173 (2.8866)  time: 0.0993  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [690/780]  eta: 0:00:09  lr: 0.000013  loss: 2.8225 (2.8823)  time: 0.1007  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [700/780]  eta: 0:00:08  lr: 0.000013  loss: 2.9053 (2.8865)  time: 0.1001  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [710/780]  eta: 0:00:07  lr: 0.000013  loss: 3.0738 (2.8863)  time: 0.0987  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [720/780]  eta: 0:00:06  lr: 0.000013  loss: 3.0738 (2.8878)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [730/780]  eta: 0:00:05  lr: 0.000013  loss: 2.8412 (2.8856)  time: 0.0990  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [740/780]  eta: 0:00:04  lr: 0.000013  loss: 2.9020 (2.8870)  time: 0.1008  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [750/780]  eta: 0:00:03  lr: 0.000013  loss: 3.0621 (2.8870)  time: 0.1024  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [760/780]  eta: 0:00:02  lr: 0.000013  loss: 2.9042 (2.8883)  time: 0.0998  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [770/780]  eta: 0:00:01  lr: 0.000013  loss: 2.8810 (2.8863)  time: 0.0962  data: 0.0002  max mem: 1920\n",
            "Epoch: [12]  [779/780]  eta: 0:00:00  lr: 0.000013  loss: 2.8762 (2.8855)  time: 0.0952  data: 0.0001  max mem: 1920\n",
            "Epoch: [12] Total time: 0:01:18 (0.1010 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 2.8762 (2.8855)\n",
            "Test:  [  0/105]  eta: 0:01:33  loss: 0.8569 (0.8569)  acc1: 77.0833 (77.0833)  acc5: 97.9167 (97.9167)  time: 0.8910  data: 0.8273  max mem: 1920\n",
            "Test:  [ 10/105]  eta: 0:00:13  loss: 1.0124 (1.0258)  acc1: 77.0833 (76.0417)  acc5: 95.8333 (95.5492)  time: 0.1420  data: 0.0853  max mem: 1920\n",
            "Test:  [ 20/105]  eta: 0:00:08  loss: 1.0008 (0.9940)  acc1: 77.0833 (76.3889)  acc5: 95.8333 (95.7341)  time: 0.0592  data: 0.0070  max mem: 1920\n",
            "Test:  [ 30/105]  eta: 0:00:06  loss: 0.9810 (1.0075)  acc1: 75.0000 (75.2688)  acc5: 95.8333 (95.6989)  time: 0.0530  data: 0.0016  max mem: 1920\n",
            "Test:  [ 40/105]  eta: 0:00:05  loss: 0.9881 (0.9971)  acc1: 73.9583 (75.4319)  acc5: 95.8333 (95.8841)  time: 0.0571  data: 0.0010  max mem: 1920\n",
            "Test:  [ 50/105]  eta: 0:00:04  loss: 1.0140 (1.0028)  acc1: 73.9583 (75.3472)  acc5: 95.8333 (95.8946)  time: 0.0622  data: 0.0083  max mem: 1920\n",
            "Test:  [ 60/105]  eta: 0:00:03  loss: 0.9717 (0.9918)  acc1: 76.0417 (75.6831)  acc5: 95.8333 (95.9870)  time: 0.0604  data: 0.0090  max mem: 1920\n",
            "Test:  [ 70/105]  eta: 0:00:02  loss: 0.9717 (1.0004)  acc1: 76.0417 (75.5282)  acc5: 96.8750 (95.8920)  time: 0.0529  data: 0.0028  max mem: 1920\n",
            "Test:  [ 80/105]  eta: 0:00:01  loss: 1.0243 (1.0004)  acc1: 75.0000 (75.6301)  acc5: 96.8750 (95.8848)  time: 0.0506  data: 0.0024  max mem: 1920\n",
            "Test:  [ 90/105]  eta: 0:00:00  loss: 0.9799 (0.9979)  acc1: 76.0417 (75.7555)  acc5: 96.8750 (95.9707)  time: 0.0494  data: 0.0011  max mem: 1920\n",
            "Test:  [100/105]  eta: 0:00:00  loss: 0.9799 (0.9960)  acc1: 77.0833 (75.8663)  acc5: 96.8750 (96.0087)  time: 0.0418  data: 0.0002  max mem: 1920\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.9334 (0.9937)  acc1: 77.0833 (75.9000)  acc5: 96.8750 (96.0400)  time: 0.0381  data: 0.0001  max mem: 1920\n",
            "Test: Total time: 0:00:06 (0.0619 s / it)\n",
            "* Acc@1 75.900 Acc@5 96.040 loss 0.994\n",
            "Accuracy of the network on the 10000 test images: 75.9%\n",
            "Max accuracy: 75.90%\n",
            "Training time 0:18:24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQLh6hE2Zj0e",
        "outputId": "ed90622d-2970-4a42-a51e-06ab312acc40"
      },
      "source": [
        "#if running on a local divice, comment these lines\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deit_small_cifar100.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1143428bd6084a8ebc1d2286eb04a225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6f8d94e22304d3b854c380971b3b0f3",
              "IPY_MODEL_547eecc9ed414835ae48df973d6cbc33",
              "IPY_MODEL_7ab25c75239e4f5f9cac2a9fcb04065e"
            ],
            "layout": "IPY_MODEL_df09fb5b5bf8451dae9f8b2d25b49a14"
          }
        },
        "c6f8d94e22304d3b854c380971b3b0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aacce6d789d406584cad9f9b7b4638e",
            "placeholder": "​",
            "style": "IPY_MODEL_1eaa359b92414d1792d8fc4e200ec5f2",
            "value": ""
          }
        },
        "547eecc9ed414835ae48df973d6cbc33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a961d7df708347148d8c045008d249d1",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ffa08b48ba5477a97a77c9fd3354796",
            "value": 170498071
          }
        },
        "7ab25c75239e4f5f9cac2a9fcb04065e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daf2c417fc6c4ff8ace803bc0a0cdbf2",
            "placeholder": "​",
            "style": "IPY_MODEL_175ef95433344baa931043e604446edb",
            "value": " 170499072/? [00:05&lt;00:00, 34156250.59it/s]"
          }
        },
        "df09fb5b5bf8451dae9f8b2d25b49a14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aacce6d789d406584cad9f9b7b4638e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eaa359b92414d1792d8fc4e200ec5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a961d7df708347148d8c045008d249d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ffa08b48ba5477a97a77c9fd3354796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "daf2c417fc6c4ff8ace803bc0a0cdbf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "175ef95433344baa931043e604446edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}