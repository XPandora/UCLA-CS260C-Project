{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70Vxlv9lMRZ",
        "outputId": "1beb0112-e98c-4859-fc3f-630a37354b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ViT-pytorch'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 170 (delta 32), reused 27 (delta 27), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (170/170), 21.31 MiB | 18.92 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "/content/ViT-pytorch/ViT-pytorch/ViT-pytorch\n",
            "Requirement already satisfied: ml-collections in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from ml-collections) (1.0.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from ml-collections) (0.5.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ml-collections) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ml-collections) (3.13)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jeonsworld/ViT-pytorch.git\n",
        "%cd ViT-pytorch/\n",
        "!pip install ml-collections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQLh6hE2Zj0e",
        "outputId": "57bea308-4c47-438b-e14a-d3570ad189ae"
      },
      "source": [
        "#if running on a local divice, comment these lines\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2t5UUX-qR_lO"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from datetime import timedelta\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule\n",
        "from utils.data_utils import get_loader\n",
        "from utils.dist_util import get_world_size\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "# Required parameters\n",
        "parser.add_argument(\"--name\", default=\"cifar100\",\n",
        "                    help=\"Name of this run. Used for monitoring.\")\n",
        "parser.add_argument(\"--dataset\", choices=[\"cifar10\", \"cifar100\"], default=\"cifar100\",\n",
        "                    help=\"Which downstream task.\")\n",
        "parser.add_argument(\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\",\n",
        "                                              \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"],\n",
        "                    default=\"ViT-B_16\",\n",
        "                    help=\"Which variant to use.\")\n",
        "parser.add_argument(\"--pretrained_dir\", type=str, default=\"checkpoint/ViT-B_16.npz\",\n",
        "                    help=\"Where to search for pretrained ViT models.\")\n",
        "parser.add_argument(\"--pretrained_model\", type=str, default=\"cifar100_checkpoint.bin\")\n",
        "parser.add_argument(\"--output_dir\", default=\"output\", type=str,\n",
        "                    help=\"The output directory where checkpoints will be written.\")\n",
        "\n",
        "parser.add_argument(\"--img_size\", default=224, type=int,\n",
        "                    help=\"Resolution size\")\n",
        "parser.add_argument(\"--train_batch_size\", default=512, type=int,\n",
        "                    help=\"Total batch size for training.\")\n",
        "parser.add_argument(\"--eval_batch_size\", default=64, type=int,\n",
        "                    help=\"Total batch size for eval.\")\n",
        "parser.add_argument(\"--eval_every\", default=100, type=int,\n",
        "                    help=\"Run prediction on validation set every so many steps.\"\n",
        "                          \"Will always run one evaluation at the end of training.\")\n",
        "\n",
        "parser.add_argument(\"--learning_rate\", default=3e-2, type=float,\n",
        "                    help=\"The initial learning rate for SGD.\")\n",
        "parser.add_argument(\"--weight_decay\", default=0, type=float,\n",
        "                    help=\"Weight deay if we apply some.\")\n",
        "parser.add_argument(\"--num_steps\", default=10000, type=int,\n",
        "                    help=\"Total number of training epochs to perform.\")\n",
        "parser.add_argument(\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"cosine\",\n",
        "                    help=\"How to decay the learning rate.\")\n",
        "parser.add_argument(\"--warmup_steps\", default=500, type=int,\n",
        "                    help=\"Step of training to perform learning rate warmup for.\")\n",
        "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                    help=\"Max gradient norm.\")\n",
        "\n",
        "parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                    help=\"local_rank for distributed training on gpus\")\n",
        "parser.add_argument('--seed', type=int, default=42,\n",
        "                    help=\"random seed for initialization\")\n",
        "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "parser.add_argument('--fp16', action='store_true',\n",
        "                    help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
        "parser.add_argument('--fp16_opt_level', type=str, default='O2',\n",
        "                    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                          \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "parser.add_argument('--loss_scale', type=float, default=0,\n",
        "                    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
        "                          \"0 (default value): dynamic loss scaling.\\n\"\n",
        "                          \"Positive power of 2: static loss scaling value.\\n\")\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl',\n",
        "                                          timeout=timedelta(minutes=60))\n",
        "    args.n_gpu = 1\n",
        "\n",
        "args.name = 'cifar100'\n",
        "args.device = device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A8p_iyr3XJQR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "from torch.nn.modules.utils import _pair\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "from scipy import ndimage\n",
        "\n",
        "import models.configs as configs\n",
        "from models.modeling import *\n",
        "\n",
        "# coding=utf-8\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import math\n",
        "\n",
        "from os.path import join as pjoin\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "from torch.nn.modules.utils import _pair\n",
        "from scipy import ndimage\n",
        "\n",
        "import models.configs as configs\n",
        "\n",
        "from models.modeling_resnet import ResNetV2\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\"\n",
        "\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, config, vis, is_quant=False):\n",
        "        super(Attention, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
        "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "\n",
        "        self.is_quant = is_quant\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        if self.is_quant:\n",
        "          query_layer = self.dequant(query_layer)\n",
        "          key_layer = self.dequant(key_layer)\n",
        "          value_layer = self.dequant(value_layer)\n",
        "        \n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        weights = attention_probs if self.vis else None\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        if self.is_quant:\n",
        "          context_layer = self.quant(context_layer)\n",
        "        attention_output = self.out(context_layer)\n",
        "        attention_output = self.proj_dropout(attention_output)\n",
        "        return attention_output, weights\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, config, is_quant=False):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
        "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
        "        self.act_fn = ACT2FN[\"gelu\"]\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "        self._init_weights()\n",
        "        self.is_quant = is_quant\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        if self.is_quant:\n",
        "          x = self.dequant(x)\n",
        "        x = self.act_fn(x)\n",
        "        if self.is_quant:\n",
        "          x = self.quant(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, img_size, in_channels=3, is_quant=False):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.hybrid = None\n",
        "        self.is_quant = is_quant\n",
        "        img_size = _pair(img_size)\n",
        "\n",
        "        if config.patches.get(\"grid\") is not None:\n",
        "            grid_size = config.patches[\"grid\"]\n",
        "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
        "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
        "            self.hybrid = True\n",
        "        else:\n",
        "            patch_size = _pair(config.patches[\"size\"])\n",
        "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "            self.hybrid = False\n",
        "\n",
        "        if self.hybrid:\n",
        "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
        "                                         width_factor=config.resnet.width_factor)\n",
        "            in_channels = self.hybrid_model.width * 16\n",
        "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
        "                                       out_channels=config.hidden_size,\n",
        "                                       kernel_size=patch_size,\n",
        "                                       stride=patch_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.hybrid:\n",
        "            x = self.hybrid_model(x)\n",
        "        x = self.patch_embeddings(x)\n",
        "        if self.is_quant:\n",
        "          x = self.dequant(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(-1, -2)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        embeddings = x + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        if self.is_quant:\n",
        "          embeddings = self.quant(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config, vis, is_quant=False):\n",
        "        super(Block, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn = Mlp(config, is_quant=is_quant)\n",
        "        self.attn = Attention(config, vis, is_quant=is_quant)\n",
        "\n",
        "        self.is_quant = is_quant\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.attention_norm(x)\n",
        "        x, weights = self.attn(x)\n",
        "        if self.is_quant:\n",
        "          x = self.dequant(x)\n",
        "          h = self.dequant(h)\n",
        "        x = x + h\n",
        "        if self.is_quant:\n",
        "          x = self.quant(x)\n",
        "\n",
        "        h = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = self.ffn(x)\n",
        "        if self.is_quant:\n",
        "          x = self.dequant(x)\n",
        "          h = self.dequant(h)\n",
        "        x = x + h\n",
        "        if self.is_quant:\n",
        "          x = self.quant(x)\n",
        "        return x, weights\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config, vis, is_quant=False):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        for _ in range(config.transformer[\"num_layers\"]):\n",
        "            layer = Block(config, vis, is_quant=is_quant)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        attn_weights = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states, weights = layer_block(hidden_states)\n",
        "            if self.vis:\n",
        "                attn_weights.append(weights)\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config, img_size, vis, is_quant=False):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embeddings = Embeddings(config, img_size=img_size, is_quant=is_quant)\n",
        "        self.encoder = Encoder(config, vis, is_quant=is_quant)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoded, attn_weights = self.encoder(embedding_output)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False, is_quant=False):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.zero_head = zero_head\n",
        "        self.classifier = config.classifier\n",
        "\n",
        "        self.transformer = Transformer(config, img_size, vis, is_quant=is_quant)\n",
        "        self.head = Linear(config.hidden_size, num_classes)\n",
        "\n",
        "        self.is_quant = is_quant\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        if self.is_quant:\n",
        "          x = self.quant(x)\n",
        "        x, attn_weights = self.transformer(x)\n",
        "        logits = self.head(x[:, 0])\n",
        "\n",
        "        if self.is_quant:\n",
        "          logits = self.dequant(logits)\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return logits, attn_weights\n",
        "\n",
        "\n",
        "CONFIGS = {\n",
        "    'ViT-B_16': configs.get_b16_config(),\n",
        "    'ViT-B_32': configs.get_b32_config(),\n",
        "    'ViT-L_16': configs.get_l16_config(),\n",
        "    'ViT-L_32': configs.get_l32_config(),\n",
        "    'ViT-H_14': configs.get_h14_config(),\n",
        "    'R50-ViT-B_16': configs.get_r50_b16_config(),\n",
        "    'testing': configs.get_testing(),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3208CvASUeR3"
      },
      "outputs": [],
      "source": [
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def test(model, test_loader):\n",
        "  model.eval()\n",
        "  all_preds, all_label = [], []\n",
        "  epoch_iterator = tqdm(test_loader,\n",
        "                        desc=\"Validating... (loss=X.X)\",\n",
        "                        bar_format=\"{l_bar}{r_bar}\",\n",
        "                        dynamic_ncols=True,\n",
        "                        disable=args.local_rank not in [-1, 0])\n",
        "  loss_fct = torch.nn.CrossEntropyLoss()\n",
        "  for step, batch in enumerate(epoch_iterator):\n",
        "      batch = tuple(t.to(args.device) for t in batch)\n",
        "      x, y = batch\n",
        "      with torch.no_grad():\n",
        "          logits = model(x)[0]\n",
        "\n",
        "          eval_loss = loss_fct(logits, y)\n",
        "          # eval_losses.update(eval_loss.item())\n",
        "\n",
        "          preds = torch.argmax(logits, dim=-1)\n",
        "      # assert False\n",
        "      if len(all_preds) == 0:\n",
        "          all_preds.append(preds.detach().cpu().numpy())\n",
        "          all_label.append(y.detach().cpu().numpy())\n",
        "      else:\n",
        "          all_preds[0] = np.append(\n",
        "              all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
        "          )\n",
        "          all_label[0] = np.append(\n",
        "              all_label[0], y.detach().cpu().numpy(), axis=0\n",
        "          )\n",
        "      # epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
        "\n",
        "  all_preds, all_label = all_preds[0], all_label[0]\n",
        "  accuracy = simple_accuracy(all_preds, all_label)\n",
        "  print(\"Valid Accuracy: %2.5f\" % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original Model"
      ],
      "metadata": {
        "id": "Q3UCpd8Y9Awb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "80c45887334747ac9b10a7af02417725",
            "a2860a17d74b45f3a833325e79aa6009",
            "4d3d0ef9e4c444dcb9064b28dad58f6a",
            "ddba09e4ad114b32908398d80c873f16",
            "546aa80fe46f42a9a85bfa5daab6bea1",
            "ae079e491d234c05afddd92b4d937159",
            "7dfb32e14cf240459363ccedcdedbbf7",
            "40619ffe1ac1445eb57c4f642f90f63d",
            "fb3b5666914647ae88b82f9de76fc6ad",
            "733a373b157d456fae6a426360a467bc",
            "e96489eedc1e42a8badf878de7af062b"
          ]
        },
        "id": "w9chZMfcSYaW",
        "outputId": "1176d126-7839-401e-fac9-0b3cefe7f148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80c45887334747ac9b10a7af02417725"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "config = CONFIGS[args.model_type]\n",
        "num_classes = 10 if args.dataset == \"cifar10\" else 100\n",
        "model = VisionTransformer(config, args.img_size, zero_head=True, num_classes=num_classes)\n",
        "checkpoint = torch.load(args.pretrained_model, map_location=args.device)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.to(args.device)\n",
        "train_loader, test_loader = get_loader(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbUXkRNxVjls",
        "outputId": "582e16c6-f61a-4877-e2c7-9a2269b7f6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating... (loss=X.X): 100%|| 157/157 [00:33<00:00,  4.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.92870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post Training Dynamic Quantization"
      ],
      "metadata": {
        "id": "jL-CeVc783n4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXf8gGV7GbcH",
        "outputId": "672e4819-e116-4797-9f10-8357ba2448d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating... (loss=X.X): 100%|| 157/157 [21:22<00:00,  8.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.90870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "test(quantized_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post Training Static Quantization"
      ],
      "metadata": {
        "id": "e2dkQPvH8vey"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kxODdiLXjAv",
        "outputId": "e1c8e226-8bc7-49ee-e031-2e7f3eb24659"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (transformer): Transformer(\n",
              "    (embeddings): Embeddings(\n",
              "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (encoder): Encoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (1): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (2): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (3): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (4): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (5): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (6): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (7): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (8): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (9): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (10): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "        (11): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (quant): QuantStub()\n",
              "          (dequant): DeQuantStub()\n",
              "        )\n",
              "      )\n",
              "      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (head): Linear(in_features=768, out_features=100, bias=True)\n",
              "  (quant): QuantStub()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model_quant = VisionTransformer(config, args.img_size, zero_head=True, num_classes=num_classes, is_quant=True)\n",
        "checkpoint = torch.load(args.pretrained_model, map_location=args.device)\n",
        "model_quant.load_state_dict(checkpoint)\n",
        "model_quant.to(args.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYZM_cG_AHZu",
        "outputId": "621ff9b3-7a90-4305-c201-694db4e7f306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n",
            "100%|| 157/157 [32:12<00:00, 12.31s/it]\n"
          ]
        }
      ],
      "source": [
        "static_quantized_model = model_quant\n",
        "static_quantized_model.qconfig = torch.quantization.default_qconfig\n",
        "\n",
        "static_quantized_model_prepared = torch.quantization.prepare(static_quantized_model)\n",
        "static_quantized_model_prepared.eval()\n",
        "epoch_iterator = tqdm(test_loader,\n",
        "                      bar_format=\"{l_bar}{r_bar}\",\n",
        "                      dynamic_ncols=True,\n",
        "                      disable=args.local_rank not in [-1, 0])\n",
        "\n",
        "for step, batch in enumerate(epoch_iterator):\n",
        "    batch = tuple(t.to(args.device) for t in batch)\n",
        "    x, y = batch\n",
        "    static_quantized_model_prepared(x)\n",
        "\n",
        "\n",
        "model_prepared_int8 = torch.quantization.convert(static_quantized_model_prepared)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6Gmvdoa5XvAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc74c75f-8c3f-41dd-cc75-b41ad13527b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating... (loss=X.X): 100%|| 157/157 [23:44<00:00,  9.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.00980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test(model_prepared_int8, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Model Param"
      ],
      "metadata": {
        "id": "6rEpmIHC8lrn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "C3ib1WPWZHNB"
      },
      "outputs": [],
      "source": [
        "new_checkpoint = {}\n",
        "qkv_weight = None\n",
        "qkv_bias = None\n",
        "\n",
        "new_checkpoint['cls_token'] = checkpoint['transformer.embeddings.cls_token']\n",
        "new_checkpoint['pos_embed'] = checkpoint['transformer.embeddings.position_embeddings']\n",
        "new_checkpoint['patch_embed.proj.weight'] = checkpoint['transformer.embeddings.patch_embeddings.weight']\n",
        "new_checkpoint['patch_embed.proj.bias'] = checkpoint['transformer.embeddings.patch_embeddings.bias']\n",
        "\n",
        "for key in checkpoint.keys():\n",
        "  if 'transformer.encoder.layer' in key:\n",
        "    new_key = key.replace('transformer.encoder.layer', 'blocks')\n",
        "    if 'attn.query' in new_key:\n",
        "      if 'weight' in new_key:\n",
        "        qkv_weight = checkpoint[key]\n",
        "      else:\n",
        "        qkv_bias = checkpoint[key]\n",
        "    elif 'attn.key' in new_key:\n",
        "      if 'weight' in new_key:\n",
        "        qkv_weight = torch.cat((qkv_weight, checkpoint[key]), dim=0)\n",
        "      else:\n",
        "        qkv_bias = torch.cat((qkv_bias, checkpoint[key]), dim=0)\n",
        "    elif 'attn.value' in new_key:\n",
        "      if 'weight' in new_key:\n",
        "        qkv_weight = torch.cat((qkv_weight, checkpoint[key]), dim=0)\n",
        "        new_key = new_key.replace('attn.value', 'attn.qkv')\n",
        "        new_checkpoint[new_key] = qkv_weight\n",
        "        qkv_weight = None\n",
        "      else:\n",
        "        qkv_bias = torch.cat((qkv_bias, checkpoint[key]), dim=0)\n",
        "        new_key = new_key.replace('attn.value', 'attn.qkv')\n",
        "        new_checkpoint[new_key] = qkv_bias\n",
        "        qkv_bias = None\n",
        "    else:\n",
        "      new_key = new_key.replace('attn.out.', 'attn.proj.')\n",
        "      new_key = new_key.replace('attention_norm', 'norm1')\n",
        "      new_key = new_key.replace('ffn_norm', 'norm2')\n",
        "      new_key = new_key.replace('ffn', 'mlp')\n",
        "      new_checkpoint[new_key] = checkpoint[key]\n",
        "\n",
        "new_checkpoint['norm.weight'] = checkpoint['transformer.encoder.encoder_norm.weight']\n",
        "new_checkpoint['norm.bias'] = checkpoint['transformer.encoder.encoder_norm.bias']\n",
        "new_checkpoint['head.weight'] = checkpoint['head.weight']\n",
        "new_checkpoint['head.bias'] = checkpoint['head.bias']\n",
        "\n",
        "model_to_save = new_checkpoint\n",
        "checkpoint_dir = os.path.join(args.output_dir, \"new_checkpoint.bin\")\n",
        "torch.save(model_to_save, checkpoint_dir)\n",
        "logger.info(\"Saved model checkpoint to [DIR: %s]\", args.output_dir)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ViT_cifar100_quantization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "80c45887334747ac9b10a7af02417725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2860a17d74b45f3a833325e79aa6009",
              "IPY_MODEL_4d3d0ef9e4c444dcb9064b28dad58f6a",
              "IPY_MODEL_ddba09e4ad114b32908398d80c873f16"
            ],
            "layout": "IPY_MODEL_546aa80fe46f42a9a85bfa5daab6bea1"
          }
        },
        "a2860a17d74b45f3a833325e79aa6009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae079e491d234c05afddd92b4d937159",
            "placeholder": "​",
            "style": "IPY_MODEL_7dfb32e14cf240459363ccedcdedbbf7",
            "value": ""
          }
        },
        "4d3d0ef9e4c444dcb9064b28dad58f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40619ffe1ac1445eb57c4f642f90f63d",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb3b5666914647ae88b82f9de76fc6ad",
            "value": 169001437
          }
        },
        "ddba09e4ad114b32908398d80c873f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_733a373b157d456fae6a426360a467bc",
            "placeholder": "​",
            "style": "IPY_MODEL_e96489eedc1e42a8badf878de7af062b",
            "value": " 169001984/? [00:01&lt;00:00, 95039426.99it/s]"
          }
        },
        "546aa80fe46f42a9a85bfa5daab6bea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae079e491d234c05afddd92b4d937159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dfb32e14cf240459363ccedcdedbbf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40619ffe1ac1445eb57c4f642f90f63d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb3b5666914647ae88b82f9de76fc6ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "733a373b157d456fae6a426360a467bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e96489eedc1e42a8badf878de7af062b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}