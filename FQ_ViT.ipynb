{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FQ-ViT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "604b98926d2a4554a725f20e519c7b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e1b26c037fe14d30bfdbbdfbaaf19d3e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a91af755346248988a36cc4151dc3ee7",
              "IPY_MODEL_96c1e51be8b3420f87b29251190d0fc7",
              "IPY_MODEL_3c7efe380bf34010b8d519db71e2cb2a"
            ]
          }
        },
        "e1b26c037fe14d30bfdbbdfbaaf19d3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a91af755346248988a36cc4151dc3ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d92a92ee4eb047aca2ce61eeae9d80c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5eb7467b41ba4e989ecb34004a2d6e00"
          }
        },
        "96c1e51be8b3420f87b29251190d0fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ca88ef101f0b431ea391c66cfdccc28f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f0731c489354b8587f9d902df683938"
          }
        },
        "3c7efe380bf34010b8d519db71e2cb2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_99b2fce56e1247e88149d1b740589f34",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:11&lt;00:00, 16995178.54it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6aa3a0e43a84de7937de3d76bf2570f"
          }
        },
        "d92a92ee4eb047aca2ce61eeae9d80c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5eb7467b41ba4e989ecb34004a2d6e00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca88ef101f0b431ea391c66cfdccc28f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f0731c489354b8587f9d902df683938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99b2fce56e1247e88149d1b740589f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6aa3a0e43a84de7937de3d76bf2570f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIjX43sJni0_",
        "outputId": "05057f39-8009-4b6a-b497-094b59d62d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'FQ-ViT' already exists and is not an empty directory.\n",
            "/content/FQ-ViT\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/linyang-zhh/FQ-ViT.git\n",
        "%cd FQ-ViT\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ozBb5ynqQfchkNWccbO1qvGD-NRNK9SH' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ozBb5ynqQfchkNWccbO1qvGD-NRNK9SH\" -O converted_cifar10-100_500_checkpoint.bin && rm -rf /tmp/cookies.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "### Before running this code cell:\n",
        "### Add parameter num_classes=10 to the line 552 of models/vit_quant.py file\n",
        "###\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n",
        "\n",
        "from models import *\n",
        "from config import Config\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"FQ-ViT\")\n",
        "\n",
        "# parser.add_argument(\"model\",\n",
        "#                     choices=['deit_tiny', 'deit_small', 'deit_base', 'vit_base',\n",
        "#                              'vit_large', 'swin_tiny', 'swin_small', 'swin_base'],\n",
        "#                     help=\"model\")\n",
        "# parser.add_argument('data', metavar='DIR',\n",
        "#                     help='path to dataset')\n",
        "parser.add_argument(\"--quant\", default=False, action=\"store_true\")\n",
        "parser.add_argument(\"--pts\", default=False, action=\"store_true\")\n",
        "parser.add_argument(\"--lis\", default=False, action=\"store_true\")\n",
        "parser.add_argument(\"--quant-method\", default=\"minmax\",\n",
        "                    choices=[\"minmax\", \"ema\", \"omse\", \"percentile\"])\n",
        "parser.add_argument(\"--calib-batchsize\", default=100,\n",
        "                    type=int, help=\"batchsize of calibration set\")\n",
        "parser.add_argument(\"--calib-iter\", default=10, type=int)\n",
        "parser.add_argument(\"--val-batchsize\", default=100,\n",
        "                    type=int, help=\"batchsize of validation set\")\n",
        "parser.add_argument(\"--num-workers\", default=16, type=int,\n",
        "                    help=\"number of data loading workers (default: 16)\")\n",
        "parser.add_argument(\"--device\", default=\"cuda\", type=str, help=\"device\")\n",
        "parser.add_argument(\"--print-freq\", default=100,\n",
        "                    type=int, help=\"print frequency\")\n",
        "parser.add_argument(\"--seed\", default=0, type=int, help=\"seed\")\n",
        "\n",
        "\n",
        "def str2model(name):\n",
        "    d = {'deit_tiny': deit_tiny_patch16_224,\n",
        "         'deit_small': deit_small_patch16_224,\n",
        "         'deit_base': deit_base_patch16_224,\n",
        "         'vit_base': vit_base_patch16_224,\n",
        "         'vit_large': vit_large_patch16_224,\n",
        "         'swin_tiny': swin_tiny_patch4_window7_224,\n",
        "         'swin_small': swin_small_patch4_window7_224,\n",
        "         'swin_base': swin_base_patch4_window7_224,\n",
        "         }\n",
        "    print('Model: %s' % d[name].__name__)\n",
        "    return d[name]\n",
        "\n",
        "\n",
        "def seed(seed=0):\n",
        "    import os\n",
        "    import sys\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    import random\n",
        "    sys.setrecursionlimit(100000)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parser.parse_args('')\n",
        "    args.model = 'vit_base'\n",
        "    args.pretrained_model = 'converted_cifar10-100_500_checkpoint.bin'\n",
        "    seed(args.seed)\n",
        "    args.quant = True\n",
        "    args.pts = True\n",
        "    args.lis = True\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "    cfg = Config(args.pts, args.lis, args.quant_method)\n",
        "    model = str2model(args.model)(pretrained=False, cfg=cfg)\n",
        "    checkpoint = torch.load(args.pretrained_model, map_location=args.device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Note: Different models have different strategies of data preprocessing.\n",
        "    # model_type = args.model.split(\"_\")[0]\n",
        "    # if model_type == \"deit\":\n",
        "    #     mean = (0.485, 0.456, 0.406)\n",
        "    #     std = (0.229, 0.224, 0.225)\n",
        "    #     crop_pct = 0.875\n",
        "    # elif model_type == 'vit':\n",
        "    #     mean = (0.5, 0.5, 0.5)\n",
        "    #     std = (0.5, 0.5, 0.5)\n",
        "    #     crop_pct = 0.9\n",
        "    # elif model_type == 'swin':\n",
        "    #     mean = (0.485, 0.456, 0.406)\n",
        "    #     std = (0.229, 0.224, 0.225)\n",
        "    #     crop_pct = 0.9\n",
        "    # else:\n",
        "    #     raise NotImplementedError\n",
        "\n",
        "    # train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
        "    # val_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
        "\n",
        "    # # Data\n",
        "    # traindir = os.path.join(args.data, 'train')\n",
        "    # valdir = os.path.join(args.data, 'val')\n",
        "\n",
        "    # val_dataset = datasets.ImageFolder(valdir, val_transform)\n",
        "    # val_loader = torch.utils.data.DataLoader(\n",
        "    #     val_dataset,\n",
        "    #     batch_size=args.val_batchsize,\n",
        "    #     shuffle=False,\n",
        "    #     num_workers=args.num_workers,\n",
        "    #     pin_memory=True,\n",
        "    # )\n",
        "    \n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomResizedCrop((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR10(root=\"./data\",\n",
        "                                train=True,\n",
        "                                download=True,\n",
        "                                transform=transform_train)\n",
        "    testset = datasets.CIFAR10(root=\"./data\",\n",
        "                                train=False,\n",
        "                                download=True,\n",
        "                                transform=transform_test)\n",
        "    \n",
        "    train_sampler = RandomSampler(trainset)\n",
        "    test_sampler = SequentialSampler(testset)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=128,\n",
        "                              num_workers=4,\n",
        "                              pin_memory=True)\n",
        "    \n",
        "    test_loader = DataLoader(testset,\n",
        "                             sampler=test_sampler,\n",
        "                             batch_size=args.val_batchsize,\n",
        "                             num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    # define loss function (criterion)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    if args.quant:\n",
        "        # train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
        "        # train_loader = torch.utils.data.DataLoader(\n",
        "        #     train_dataset,\n",
        "        #     batch_size=args.calib_batchsize,\n",
        "        #     shuffle=True,\n",
        "        #     num_workers=args.num_workers,\n",
        "        #     pin_memory=True,\n",
        "        #     drop_last=True,\n",
        "        # )\n",
        "        # Get calibration set.\n",
        "        image_list = []\n",
        "        for i, (data, target) in enumerate(train_loader):\n",
        "            if i == args.calib_iter:\n",
        "                break\n",
        "            data = data.to(device)\n",
        "            image_list.append(data)\n",
        "\n",
        "        print(\"Calibrating...\")\n",
        "        model.model_open_calibrate()\n",
        "        with torch.no_grad():\n",
        "            for i, image in enumerate(image_list):\n",
        "                if i == len(image_list)-1:\n",
        "                    # This is used for OMSE method to\n",
        "                    # calculate minimum quantization error\n",
        "                    model.model_open_last_calibrate()\n",
        "                output = model(image)\n",
        "        model.model_close_calibrate()\n",
        "        model.model_quant()\n",
        "\n",
        "    print(\"Validating...\")\n",
        "    val_loss, val_prec1 = validate(\n",
        "        args, test_loader, model, criterion, device\n",
        "    )\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, criterion, device):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    val_start_time = end = time.time()\n",
        "    for i, (data, target) in enumerate(val_loader):\n",
        "        target = target.to(device)\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1,_ = accuracy(output.data, target, topk=(1,5))\n",
        "        losses.update(loss.data.item(), data.size(0))\n",
        "        top1.update(prec1.data.item(), data.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            print(\n",
        "                \"Test: [{0}/{1}]\\t\"\n",
        "                \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
        "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
        "                \"Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\".format(\n",
        "                    i,\n",
        "                    len(val_loader),\n",
        "                    batch_time=batch_time,\n",
        "                    loss=losses,\n",
        "                    top1=top1,\n",
        "                )\n",
        "            )\n",
        "    val_end_time = time.time()\n",
        "    print(\" * Prec@1 {top1.avg:.3f} Time {time:.3f}\".format(\n",
        "        top1=top1, time=val_end_time - val_start_time))\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "def build_transform(input_size=224, interpolation=\"bicubic\",\n",
        "                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225),\n",
        "                    crop_pct=0.875):\n",
        "    def _pil_interp(method):\n",
        "        if method == \"bicubic\":\n",
        "            return Image.BICUBIC\n",
        "        elif method == \"lanczos\":\n",
        "            return Image.LANCZOS\n",
        "        elif method == \"hamming\":\n",
        "            return Image.HAMMING\n",
        "        else:\n",
        "            return Image.BILINEAR\n",
        "    resize_im = input_size > 32\n",
        "    t = []\n",
        "    if resize_im:\n",
        "        size = int(math.floor(input_size / crop_pct))\n",
        "        ip = _pil_interp(interpolation)\n",
        "        t.append(\n",
        "            transforms.Resize(\n",
        "                size, interpolation=ip\n",
        "            ),  # to maintain same ratio w.r.t. 224 images\n",
        "        )\n",
        "        t.append(transforms.CenterCrop(input_size))\n",
        "\n",
        "    t.append(transforms.ToTensor())\n",
        "    t.append(transforms.Normalize(mean, std))\n",
        "    return transforms.Compose(t)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "604b98926d2a4554a725f20e519c7b92",
            "e1b26c037fe14d30bfdbbdfbaaf19d3e",
            "a91af755346248988a36cc4151dc3ee7",
            "96c1e51be8b3420f87b29251190d0fc7",
            "3c7efe380bf34010b8d519db71e2cb2a",
            "d92a92ee4eb047aca2ce61eeae9d80c1",
            "5eb7467b41ba4e989ecb34004a2d6e00",
            "ca88ef101f0b431ea391c66cfdccc28f",
            "5f0731c489354b8587f9d902df683938",
            "99b2fce56e1247e88149d1b740589f34",
            "b6aa3a0e43a84de7937de3d76bf2570f"
          ]
        },
        "id": "r1tFG4wVkgNi",
        "outputId": "7931e1f6-edfb-4900-aec5-f28a01119ad3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: vit_base_patch16_224\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "604b98926d2a4554a725f20e519c7b92",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibrating...\n",
            "Validating...\n",
            "Test: [0/100]\tTime 3.039 (3.039)\tLoss 0.0590 (0.0590)\tPrec@1 98.000 (98.000)\t\n",
            " * Prec@1 97.310 Time 168.019\n"
          ]
        }
      ]
    }
  ]
}