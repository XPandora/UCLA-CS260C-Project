{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vit-pruning.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f455bc5aa714c8c97c6fbbe1359c616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_23b1976fa15543249e0bde1009044774",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_626c7e9951764ea59ce29fc42b5f98c0",
              "IPY_MODEL_a8a8f336aad14ff2aa433b972eb8187b",
              "IPY_MODEL_25b66ac2ed0a45b6bab3aa230499e925"
            ]
          }
        },
        "23b1976fa15543249e0bde1009044774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "626c7e9951764ea59ce29fc42b5f98c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ca7cbfe09760431289f9c49e36427ea3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f908533ad3ab4530b812505e9d4ef3d9"
          }
        },
        "a8a8f336aad14ff2aa433b972eb8187b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5325b1468fb34b33a92453f8b6c166af",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b83ef03666e46d499858b46449c0471"
          }
        },
        "25b66ac2ed0a45b6bab3aa230499e925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_babfad687502434dadd06728add75595",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:05&lt;00:00, 32278301.91it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e74024814b5047efae737a7f585bd4a4"
          }
        },
        "ca7cbfe09760431289f9c49e36427ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f908533ad3ab4530b812505e9d4ef3d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5325b1468fb34b33a92453f8b6c166af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b83ef03666e46d499858b46449c0471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "babfad687502434dadd06728add75595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e74024814b5047efae737a7f585bd4a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr2mb1gVuVK4",
        "outputId": "ba557c54-ef98-4ed3-8737-9880ab2b8809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ViT-pytorch'...\n",
            "remote: Enumerating objects: 187, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 187 (delta 42), reused 27 (delta 27), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (187/187), 21.31 MiB | 16.26 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n",
            "/content/ViT-pytorch\n",
            "Collecting ml-collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from ml-collections) (1.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ml-collections) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ml-collections) (1.15.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from ml-collections) (0.5.5)\n",
            "Building wheels for collected packages: ml-collections\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94524 sha256=7a474a1b174c7b6366fad57f71a271bf02c93cfc8817d06d7ca2fef9228ffc11\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/da/64/33c926a1b10ff19791081b705879561b715a8341a856a3bbd2\n",
            "Successfully built ml-collections\n",
            "Installing collected packages: ml-collections\n",
            "Successfully installed ml-collections-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/franklinnwren/ViT-pytorch.git\n",
        "%cd ViT-pytorch/\n",
        "!pip install ml-collections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNZSNAXBx84H",
        "outputId": "1e3518d0-e90c-44fe-d917-c40920ec8e49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if running on a local divice, comment these lines\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFwSv0P7wpHw",
        "outputId": "c50206a5-d0a3-4aa5-c60e-6237f2173882"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "6f455bc5aa714c8c97c6fbbe1359c616",
            "23b1976fa15543249e0bde1009044774",
            "626c7e9951764ea59ce29fc42b5f98c0",
            "a8a8f336aad14ff2aa433b972eb8187b",
            "25b66ac2ed0a45b6bab3aa230499e925",
            "ca7cbfe09760431289f9c49e36427ea3",
            "f908533ad3ab4530b812505e9d4ef3d9",
            "5325b1468fb34b33a92453f8b6c166af",
            "8b83ef03666e46d499858b46449c0471",
            "babfad687502434dadd06728add75595",
            "e74024814b5047efae737a7f585bd4a4"
          ]
        },
        "id": "6V5pwDAnSdP8",
        "outputId": "57a99ae9-7b43-4814-ab1c-a621bc08d1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f455bc5aa714c8c97c6fbbe1359c616",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from datetime import timedelta\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from models.modeling import VisionTransformer, CONFIGS\n",
        "from utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule\n",
        "from utils.data_utils import get_loader\n",
        "from utils.dist_util import get_world_size\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "# Required parameters\n",
        "parser.add_argument(\"--name\", default=\"cifar10\",\n",
        "                    help=\"Name of this run. Used for monitoring.\")\n",
        "parser.add_argument(\"--dataset\", choices=[\"cifar10\", \"cifar100\"], default=\"cifar10\",\n",
        "                    help=\"Which downstream task.\")\n",
        "parser.add_argument(\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\",\n",
        "                                              \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"],\n",
        "                    default=\"ViT-B_16\",\n",
        "                    help=\"Which variant to use.\")\n",
        "parser.add_argument(\"--pretrained_dir\", type=str, default=\"ViT-B_16.npz\",\n",
        "                    help=\"Where to search for pretrained ViT models.\")\n",
        "parser.add_argument(\"--pretrained_model\", type=str, default=\"../drive/MyDrive/cifar10-100_500_checkpoint_prune.bin\")\n",
        "parser.add_argument(\"--output_dir\", default=\"output\", type=str,\n",
        "                    help=\"The output directory where checkpoints will be written.\")\n",
        "\n",
        "parser.add_argument(\"--img_size\", default=224, type=int,\n",
        "                    help=\"Resolution size\")\n",
        "parser.add_argument(\"--train_batch_size\", default=512, type=int,\n",
        "                    help=\"Total batch size for training.\")\n",
        "parser.add_argument(\"--eval_batch_size\", default=64, type=int,\n",
        "                    help=\"Total batch size for eval.\")\n",
        "parser.add_argument(\"--eval_every\", default=100, type=int,\n",
        "                    help=\"Run prediction on validation set every so many steps.\"\n",
        "                          \"Will always run one evaluation at the end of training.\")\n",
        "\n",
        "parser.add_argument(\"--learning_rate\", default=3e-2, type=float,\n",
        "                    help=\"The initial learning rate for SGD.\")\n",
        "parser.add_argument(\"--weight_decay\", default=0, type=float,\n",
        "                    help=\"Weight deay if we apply some.\")\n",
        "parser.add_argument(\"--num_steps\", default=10000, type=int,\n",
        "                    help=\"Total number of training epochs to perform.\")\n",
        "parser.add_argument(\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"cosine\",\n",
        "                    help=\"How to decay the learning rate.\")\n",
        "parser.add_argument(\"--warmup_steps\", default=500, type=int,\n",
        "                    help=\"Step of training to perform learning rate warmup for.\")\n",
        "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                    help=\"Max gradient norm.\")\n",
        "\n",
        "parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                    help=\"local_rank for distributed training on gpus\")\n",
        "parser.add_argument('--seed', type=int, default=42,\n",
        "                    help=\"random seed for initialization\")\n",
        "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "parser.add_argument('--fp16', action='store_true',\n",
        "                    help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
        "parser.add_argument('--fp16_opt_level', type=str, default='O2',\n",
        "                    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                          \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "parser.add_argument('--loss_scale', type=float, default=0,\n",
        "                    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
        "                          \"0 (default value): dynamic loss scaling.\\n\"\n",
        "                          \"Positive power of 2: static loss scaling value.\\n\")\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl',\n",
        "                                          timeout=timedelta(minutes=60))\n",
        "    args.n_gpu = 1\n",
        "\n",
        "args.name = 'cifar10'\n",
        "args.device = device\n",
        "\n",
        "config = CONFIGS[args.model_type]\n",
        "num_classes = 10 if args.dataset == \"cifar10\" else 100\n",
        "model = VisionTransformer(config, args.img_size, zero_head=True, num_classes=num_classes)\n",
        "model.load_state_dict(torch.load(args.pretrained_model))\n",
        "model.to(args.device)\n",
        "train_loader, test_loader = get_loader(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from torch import nn\n",
        "\n",
        "total = 0\n",
        "for name, m in model.named_modules():\n",
        "    if 'select' in name:\n",
        "        print(m)\n",
        "        total += m.indexes.data.shape[0]\n",
        "\n",
        "print(\"pause\")\n",
        "\n",
        "bn = torch.zeros(total)\n",
        "index = 0\n",
        "for name, m in model.named_modules():\n",
        "    if 'select' in name:\n",
        "        print(m)\n",
        "        size = m.indexes.data.shape[0]\n",
        "        bn[index:(index+size)] = m.indexes.data.abs().clone()\n",
        "        index += size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WiYAEKJbwwe",
        "outputId": "2f204cd5-ebef-495c-c1a8-d48cd9ddef3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "pause\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n",
            "channel_selection()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percent = 0.1\n",
        "y, i = torch.sort(bn)\n",
        "thre_index = int(total * percent)\n",
        "thre = y[thre_index]\n",
        "\n",
        "# print(thre)\n",
        "\n",
        "pruned = 0\n",
        "cfg = []\n",
        "cfg_mask = []\n",
        "for k, (name, m) in enumerate(model.named_modules()):\n",
        "    if 'select' in name:\n",
        "        # print(k)\n",
        "        # print(m)\n",
        "        if k not in [14,31,48,65,82,99,116,133,150,167,184,201]:\n",
        "            weight_copy = m.indexes.data.abs().clone()\n",
        "            mask = weight_copy.gt(thre).float().cuda()\n",
        "            thre_ = thre.clone()\n",
        "            while (torch.sum(mask) % 8 != 0): # heads\n",
        "                thre_ = thre_ - 0.001\n",
        "                mask = weight_copy.gt(thre_).float().cuda()\n",
        "        else:\n",
        "            weight_copy = m.indexes.data.abs().clone()\n",
        "            mask = weight_copy.gt(thre).float().cuda()\n",
        "        pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
        "        m.indexes.data.mul_(mask)\n",
        "        cfg.append(int(torch.sum(mask)))\n",
        "        cfg_mask.append(mask.clone())\n",
        "        print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.\n",
        "            format(k, mask.shape[0], int(torch.sum(mask))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVCz-jFJfymW",
        "outputId": "9ed93cfc-2c7f-49e0-a274-a16a3011d388"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer index: 14 \t total channel: 3072 \t remaining channel: 2699\n",
            "layer index: 19 \t total channel: 768 \t remaining channel: 448\n",
            "layer index: 31 \t total channel: 3072 \t remaining channel: 2807\n",
            "layer index: 36 \t total channel: 768 \t remaining channel: 664\n",
            "layer index: 48 \t total channel: 3072 \t remaining channel: 2872\n",
            "layer index: 53 \t total channel: 768 \t remaining channel: 432\n",
            "layer index: 65 \t total channel: 3072 \t remaining channel: 2925\n",
            "layer index: 70 \t total channel: 768 \t remaining channel: 488\n",
            "layer index: 82 \t total channel: 3072 \t remaining channel: 2937\n",
            "layer index: 87 \t total channel: 768 \t remaining channel: 568\n",
            "layer index: 99 \t total channel: 3072 \t remaining channel: 2966\n",
            "layer index: 104 \t total channel: 768 \t remaining channel: 528\n",
            "layer index: 116 \t total channel: 3072 \t remaining channel: 2991\n",
            "layer index: 121 \t total channel: 768 \t remaining channel: 672\n",
            "layer index: 133 \t total channel: 3072 \t remaining channel: 3017\n",
            "layer index: 138 \t total channel: 768 \t remaining channel: 696\n",
            "layer index: 150 \t total channel: 3072 \t remaining channel: 3017\n",
            "layer index: 155 \t total channel: 768 \t remaining channel: 624\n",
            "layer index: 167 \t total channel: 3072 \t remaining channel: 3007\n",
            "layer index: 172 \t total channel: 768 \t remaining channel: 680\n",
            "layer index: 184 \t total channel: 3072 \t remaining channel: 3017\n",
            "layer index: 189 \t total channel: 768 \t remaining channel: 648\n",
            "layer index: 201 \t total channel: 3072 \t remaining channel: 3043\n",
            "layer index: 206 \t total channel: 768 \t remaining channel: 752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def test(model, test_loader):\n",
        "  model.eval()\n",
        "  all_preds, all_label = [], []\n",
        "  epoch_iterator = tqdm(test_loader,\n",
        "                        desc=\"Validating... (loss=X.X)\",\n",
        "                        bar_format=\"{l_bar}{r_bar}\",\n",
        "                        dynamic_ncols=True,\n",
        "                        disable=args.local_rank not in [-1, 0])\n",
        "  loss_fct = torch.nn.CrossEntropyLoss()\n",
        "  for step, batch in enumerate(epoch_iterator):\n",
        "      batch = tuple(t.to(args.device) for t in batch)\n",
        "      x, y = batch\n",
        "      with torch.no_grad():\n",
        "          logits = model(x)[0]\n",
        "\n",
        "          eval_loss = loss_fct(logits, y)\n",
        "          # eval_losses.update(eval_loss.item())\n",
        "\n",
        "          preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "      if len(all_preds) == 0:\n",
        "          all_preds.append(preds.detach().cpu().numpy())\n",
        "          all_label.append(y.detach().cpu().numpy())\n",
        "      else:\n",
        "          all_preds[0] = np.append(\n",
        "              all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
        "          )\n",
        "          all_label[0] = np.append(\n",
        "              all_label[0], y.detach().cpu().numpy(), axis=0\n",
        "          )\n",
        "      # epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
        "\n",
        "  all_preds, all_label = all_preds[0], all_label[0]\n",
        "  accuracy = simple_accuracy(all_preds, all_label)\n",
        "  print(\"Valid Accuracy: %2.5f\" % accuracy)"
      ],
      "metadata": {
        "id": "79v6p4Uyg0kS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nQOhv1_g7jY",
        "outputId": "2bd11954-66a2-466e-eecc-a8fa2f754f6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating... (loss=X.X): 100%|| 157/157 [00:35<00:00,  4.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.88360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}